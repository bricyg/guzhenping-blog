<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>谷震平的博客</title>
  
  <subtitle>写点一路的风景，都很普通，主要还是留给自己。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://guzhenping.com/"/>
  <updated>2018-11-30T05:21:57.452Z</updated>
  <id>http://guzhenping.com/</id>
  
  <author>
    <name>谷震平[guzhenping, hehzyx@gmail.com]</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据开发学习（Redis）</title>
    <link href="http://guzhenping.com/2018/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Redis%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/11/02/大数据开发学习（Redis）/</id>
    <published>2018-11-02T09:04:49.000Z</published>
    <updated>2018-11-30T05:21:57.452Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-前言"><a href="#一-前言" class="headerlink" title="一 前言"></a>一 前言</h2><p>经常使用redis, 特地进行总结。</p><a id="more"></a><h2 id="二-基础"><a href="#二-基础" class="headerlink" title="二 基础"></a>二 基础</h2><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>下载安装包，或者：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://download.redis.io/releases/redis-3.2.0.tar.gz</span><br><span class="line">$ tar xzf redis-3.2.0.tar.gz</span><br><span class="line">$ cd redis-3.2.0</span><br><span class="line">$ make</span><br></pre></td></tr></table></figure><p>就是make命令~中间过程，报什么错搞定什么即可。</p><p>Ubuntu下比较简单，就是apt-get install redis-server</p><p>默认安装位置： 利用whereis redis 去找redis.conf ，需要修改是否后台运行daemonize （为yes）等属性</p><p>完成后，自动启动，可用命令：ps -aux| grep redis</p><p>redis是依赖，因为我们用python，不得不装一个wrapper——redis-py:<br><code>sudo pip install redis</code></p><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><p>要让Redis-server在后台运行！</p><p>1.简单的启动：</p><p>进到redis目录下的src文件夹下，输入：<br>redis-server</p><p>2.后台启动：</p><p>先去找redis.conf文件，修改daemonize属性，从no变为yes<br>进到redis目录下的src文件夹下，<br>输入：<br>redis-server &amp;</p><p>3.通过配置文件启动：</p><p>需要配置启动文件，在Redis工程目录下有个redis.conf文件，修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#修改daemonize为yes，即默认以后台程序方式运行（还记得前面手动使用&amp;号强制后台运行吗）。</span><br><span class="line">daemonize yes</span><br><span class="line"></span><br><span class="line">#可修改默认监听端口，别改了，万一你忘了</span><br><span class="line">port 6379</span><br><span class="line"></span><br><span class="line">#修改生成默认日志文件位置</span><br><span class="line">logfile  &quot;/home/futeng/logs/redis.log&quot;</span><br><span class="line"></span><br><span class="line">#配置持久化文件存放位置</span><br><span class="line">dir /home/futeng/data/redisData</span><br></pre></td></tr></table></figure><p>配置完以后，启动：<br>还是 src目录下，<code>redis-server ./redis.conf</code></p><p>if 改了端口，使用redis-cli命令连接时，需要带上端口，比如：<br>redis-cli -p xxxx  [再次强调：仍在src目录下]</p><p>4.使用redis启动脚本，设置开机自启<br>在生产环境中使用这种方式。</p><p>5.启动无密码验证的</p><p><code>redis-server --protected-mode no</code></p><p>设置连接数</p><p><code>redis-server --protected-mode no --maxclients 100000</code></p><h4 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h4><p>还是进到redis目录下的src文件夹下，输入：<br>redis-cli</p><p>在弹出的交互界面测试下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">redis&gt; set foo bar</span><br><span class="line">OK</span><br><span class="line">redis&gt; get foo</span><br><span class="line">&quot;bar&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-前言&quot;&gt;&lt;a href=&quot;#一-前言&quot; class=&quot;headerlink&quot; title=&quot;一 前言&quot;&gt;&lt;/a&gt;一 前言&lt;/h2&gt;&lt;p&gt;经常使用redis, 特地进行总结。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Redis" scheme="http://guzhenping.com/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redash开发指南</title>
    <link href="http://guzhenping.com/2018/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Redash%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/11/02/大数据开发学习（Redash）/</id>
    <published>2018-11-02T09:04:48.000Z</published>
    <updated>2018-11-30T02:48:21.177Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-Redash介绍"><a href="#一-Redash介绍" class="headerlink" title="一 Redash介绍"></a>一 Redash介绍</h2><p>Redash是一款融合多数据源的可视化查询工具，用于Ad-hoc查询再好不过。除了官方支持的数据源，还可以通过复用代码开发支持Kylin、Clickhouse、TiDB、Palo、Druid等。<br><a id="more"></a></p><h2 id="二-测试环境"><a href="#二-测试环境" class="headerlink" title="二 测试环境"></a>二 测试环境</h2><h4 id="官方提供的测试环境启动方式"><a href="#官方提供的测试环境启动方式" class="headerlink" title="官方提供的测试环境启动方式"></a>官方提供的测试环境启动方式</h4><p>redash开发文档 ： <a href="https://redash.io/help-onpremise/" target="_blank" rel="noopener">https://redash.io/help-onpremise/</a></p><h4 id="自主搭建"><a href="#自主搭建" class="headerlink" title="自主搭建"></a>自主搭建</h4><p>安装虚拟环境管理工具anaconda, 创建虚拟环境redash:<code>conda create -n redash python=2.7</code>,激活该环境:<code>source activate redash</code>。</p><p>创建虚拟环境（也可以不创建），激活后。利用pip安装redash所需要的包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt      # 程序基础依赖</span><br><span class="line">pip install -r requirements_all_ds.txt    # 所有数据库的依赖</span><br></pre></td></tr></table></figure><p>在上述安装过程中大概率会出现安装错误(遇到过10多回)。请将出错的包单独安装。</p><p>安装npm，在主目录执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install    # 安装node模块</span><br><span class="line">npm run build    # 编译前端的东西</span><br></pre></td></tr></table></figure><p>启动服务器:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/run ./manage.py runserver --debugger --reload</span><br></pre></td></tr></table></figure></p><p>启动celery的woker和调度器:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/run celery worker --app=redash.worker --beat -Qscheduled_queries,queries,celery -c2</span><br></pre></td></tr></table></figure></p><p>也可以分开启动woker和调度器，调度器启动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/run celery worker --app=redash.worker -c4 -Qscheduled_queries --maxtasksperchild=10 -Ofair</span><br></pre></td></tr></table></figure><p>worker启动:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/run celery worker --app=redash.worker --beat -c8 -Qqueries,celery --maxtasksperchild=10 -Ofair</span><br></pre></td></tr></table></figure></p><p><strong>根据机器机器实际情况，调节-c后的参数，用来指定启动多少个进程数量。</strong></p><h4 id="启动方式"><a href="#启动方式" class="headerlink" title="启动方式"></a>启动方式</h4><p>除了上面的启动方式，还推荐使用guncoin的启动方式。</p><p>启动服务器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 前台启动</span><br><span class="line">gunicorn -b 127.0.0.1:5000 --name redash -w 4 --max-requests 1000 redash.wsgi:app</span><br><span class="line"></span><br><span class="line"># 后台启动</span><br><span class="line">nohup /home/hadoop/anaconda3/envs/redash/bin/python /home/hadoop/anaconda3/envs/redash/bin/gunicorn -b 0.0.0.0:5000 --name redash -w 4 --max-requests 1000 redash.wsgi:app &gt;&gt; redash.log &amp;</span><br></pre></td></tr></table></figure><p>没有gunicorn命令的，需要装Python包。</p><p>启动调度器进程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 前台启动</span><br><span class="line">bin/run celery worker --app=redash.worker -c4 -Qscheduled_queries --maxtasksperchild=10 -Ofair</span><br><span class="line"></span><br><span class="line"># 后台启动</span><br><span class="line">nohup /home/hadoop/anaconda3/envs/redash/bin/celery worker --app=redash.worker -c4 -Qscheduled_queries --maxtasksperchild=10 -Ofair &gt;&gt; schedular.log &amp;</span><br></pre></td></tr></table></figure><p>启动worker进程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 前台启动</span><br><span class="line">bin/run celery worker --app=redash.worker --beat -c8 -Qqueries,celery --maxtasksperchild=10 -Ofair  </span><br><span class="line"></span><br><span class="line"># 后台启动</span><br><span class="line">nohup /home/hadoop/anaconda3/envs/redash/bin/celery worker --app=redash.worker --beat -c8 -Qqueries,celery --maxtasksperchild=10 -Ofair  &gt;&gt; worker.log &amp;</span><br></pre></td></tr></table></figure><p>也可以用nginx做下代理，配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">worker_processes  4;</span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile        on;</span><br><span class="line">    keepalive_timeout  3600;</span><br><span class="line"></span><br><span class="line">    upstream rd_servers &#123;</span><br><span class="line">         server 127.0.0.1:5000;</span><br><span class="line">    &#125;</span><br><span class="line">    server &#123;</span><br><span class="line">          server_tokens off;</span><br><span class="line">          listen 5123 default;</span><br><span class="line">          access_log /var/log/nginx/rd.access.log;</span><br><span class="line">          gzip on;</span><br><span class="line">          gzip_types *;</span><br><span class="line">          gzip_proxied any;</span><br><span class="line">          location / &#123;</span><br><span class="line">              proxy_set_header Host $http_host;</span><br><span class="line">              proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">              proxy_set_header X-Forwarded-Proto $scheme;</span><br><span class="line">              proxy_pass       http://rd_servers;</span><br><span class="line">          &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动NGINX:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/nginx/sbin</span><br><span class="line">sudo ./nginx</span><br></pre></td></tr></table></figure><h2 id="三-Redash-VS-Superset"><a href="#三-Redash-VS-Superset" class="headerlink" title="三 Redash VS Superset"></a>三 Redash VS Superset</h2><p>关于Redash：<a href="https://redash.io/；" target="_blank" rel="noopener">https://redash.io/；</a></p><p>与Superset的区别与联系：<a href="https://www.zhihu.com/question/60369195/answer/258298127。" target="_blank" rel="noopener">https://www.zhihu.com/question/60369195/answer/258298127。</a></p><h2 id="Bug"><a href="#Bug" class="headerlink" title="Bug"></a>Bug</h2><p>hive 读 schema<br>show columns in table , 无法处理中文comment,<br>desc table , 无法处理没有字段的表，</p><p>报错：Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.ClassNotFoundException Class org.apache.hive.hcatalog.data.JsonSerDe not found</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-Redash介绍&quot;&gt;&lt;a href=&quot;#一-Redash介绍&quot; class=&quot;headerlink&quot; title=&quot;一 Redash介绍&quot;&gt;&lt;/a&gt;一 Redash介绍&lt;/h2&gt;&lt;p&gt;Redash是一款融合多数据源的可视化查询工具，用于Ad-hoc查询再好不过。除了官方支持的数据源，还可以通过复用代码开发支持Kylin、Clickhouse、TiDB、Palo、Druid等。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Redash" scheme="http://guzhenping.com/tags/Redash/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发学习（Flink）</title>
    <link href="http://guzhenping.com/2018/10/29/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Flink%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/10/29/大数据开发学习（Flink）/</id>
    <published>2018-10-29T09:04:48.000Z</published>
    <updated>2018-11-30T02:46:37.477Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Flink在实时流处理领域越来越热，以阿里为首的企业正在投入更多的资源。在实际工作中也遇到了流处理的场景，特此学习一下。</p><a id="more"></a><h2 id="Demo制作"><a href="#Demo制作" class="headerlink" title="Demo制作"></a>Demo制作</h2><p>先看一个demo:</p><p>WordCount.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gzp.batch;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.DataSet;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line">        <span class="keyword">final</span> ExecutionEnvironment env = ExecutionEnvironment.createCollectionsEnvironment();</span><br><span class="line">        DataSet&lt;String&gt; text = WordCountData.getDefaultTextLineDataSet(env);</span><br><span class="line">        DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; counts =</span><br><span class="line">                <span class="comment">// split up the lines in pairs (2-tuples) containing: (word,1)</span></span><br><span class="line">                text.flatMap(<span class="keyword">new</span> Tokenizer())</span><br><span class="line">                    .groupBy(<span class="number">0</span>)</span><br><span class="line">                    .sum(<span class="number">1</span>);</span><br><span class="line">        counts.print();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Tokenizer</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// emit the pairs</span></span><br><span class="line">            <span class="keyword">for</span> (String token : value.toLowerCase().split(<span class="string">"\\W+"</span>)) &#123;</span><br><span class="line">                <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(token, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>WordCountData.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">package com.gzp.batch;</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.java.DataSet;</span><br><span class="line">import org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"></span><br><span class="line">public class WordCountData &#123;</span><br><span class="line">    public static final String[] WORDS = new String[] &#123;</span><br><span class="line">            &quot;To be, or not to be,--that is the question:--&quot;,</span><br><span class="line">            &quot;Whether &apos;tis nobler in the mind to suffer&quot;,</span><br><span class="line">            &quot;The slings and arrows of outrageous fortune&quot;,</span><br><span class="line">            &quot;Or to take arms against a sea of troubles,&quot;,</span><br><span class="line">            &quot;And by opposing end them?--To die,--to sleep,--&quot;,</span><br><span class="line">            &quot;No more; and by a sleep to say we end&quot;,</span><br><span class="line">            &quot;The heartache, and the thousand natural shocks&quot;,</span><br><span class="line">            &quot;That flesh is heir to,--&apos;tis a consummation&quot;,</span><br><span class="line">            &quot;Devoutly to be wish&apos;d. To die,--to sleep;--&quot;,</span><br><span class="line">            &quot;To sleep! perchance to dream:--ay, there&apos;s the rub;&quot;,</span><br><span class="line">            &quot;For in that sleep of death what dreams may come,&quot;,</span><br><span class="line">            &quot;When we have shuffled off this mortal coil,&quot;,</span><br><span class="line">            &quot;Must give us pause: there&apos;s the respect&quot;,</span><br><span class="line">            &quot;That makes calamity of so long life;&quot;,</span><br><span class="line">            &quot;For who would bear the whips and scorns of time,&quot;,</span><br><span class="line">            &quot;The oppressor&apos;s wrong, the proud man&apos;s contumely,&quot;,</span><br><span class="line">            &quot;The pangs of despis&apos;d love, the law&apos;s delay,&quot;,</span><br><span class="line">            &quot;The insolence of office, and the spurns&quot;,</span><br><span class="line">            &quot;That patient merit of the unworthy takes,&quot;,</span><br><span class="line">            &quot;When he himself might his quietus make&quot;,</span><br><span class="line">            &quot;With a bare bodkin? who would these fardels bear,&quot;,</span><br><span class="line">            &quot;To grunt and sweat under a weary life,&quot;,</span><br><span class="line">            &quot;But that the dread of something after death,--&quot;,</span><br><span class="line">            &quot;The undiscover&apos;d country, from whose bourn&quot;,</span><br><span class="line">            &quot;No traveller returns,--puzzles the will,&quot;,</span><br><span class="line">            &quot;And makes us rather bear those ills we have&quot;,</span><br><span class="line">            &quot;Than fly to others that we know not of?&quot;,</span><br><span class="line">            &quot;Thus conscience does make cowards of us all;&quot;,</span><br><span class="line">            &quot;And thus the native hue of resolution&quot;,</span><br><span class="line">            &quot;Is sicklied o&apos;er with the pale cast of thought;&quot;,</span><br><span class="line">            &quot;And enterprises of great pith and moment,&quot;,</span><br><span class="line">            &quot;With this regard, their currents turn awry,&quot;,</span><br><span class="line">            &quot;And lose the name of action.--Soft you now!&quot;,</span><br><span class="line">            &quot;The fair Ophelia!--Nymph, in thy orisons&quot;,</span><br><span class="line">            &quot;Be all my sins remember&apos;d.&quot;</span><br><span class="line">    &#125;;</span><br><span class="line">    public static DataSet&lt;String&gt; getDefaultTextLineDataSet(ExecutionEnvironment env) &#123;</span><br><span class="line">        return env.fromElements(WORDS);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述代码核心是将一段文本按单词切割，统计词频。flatMap()调用udf将文本切割并生成结构化数据，按单词分组后再sum。</p><p>不得不说，java写的flink task代码太长….</p><h2 id="Core-Concepts"><a href="#Core-Concepts" class="headerlink" title="Core Concepts"></a>Core Concepts</h2><p>####</p><h2 id="Flink-Case"><a href="#Flink-Case" class="headerlink" title="Flink Case"></a>Flink Case</h2><h4 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h4><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/cli.html" target="_blank" rel="noopener">使用简介</a></p><h2 id="参考资料、"><a href="#参考资料、" class="headerlink" title="参考资料、"></a>参考资料、</h2><ul><li>核心概念翻译</li></ul><p><a href="https://blog.csdn.net/xiaoping2017/article/details/79846158" target="_blank" rel="noopener">Flink架构、原理与部署测试详解</a></p><ul><li>简单的Scala Demo应用： </li></ul><p><a href="https://github.com/liguohua-bigdata/simple-flink/blob/master/book/stream/customSource/customSourceScala.md" target="_blank" rel="noopener">flink demo</a></p><ul><li>关于动态表: </li></ul><p><a href="http://www.10tiao.com/html/157/201707/2653162664/1.html" target="_blank" rel="noopener">在数据流中使用SQL查询：Apache Flink中的动态表的持续查询</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Flink在实时流处理领域越来越热，以阿里为首的企业正在投入更多的资源。在实际工作中也遇到了流处理的场景，特此学习一下。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="实时流" scheme="http://guzhenping.com/tags/%E5%AE%9E%E6%97%B6%E6%B5%81/"/>
    
      <category term="Flink" scheme="http://guzhenping.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Kylin应用篇</title>
    <link href="http://guzhenping.com/2018/10/26/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Kylin%E5%BA%94%E7%94%A8%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/10/26/大数据开发学习（Kylin应用）/</id>
    <published>2018-10-26T09:04:48.000Z</published>
    <updated>2018-11-30T02:56:21.818Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Kylin是一款处理海量数据，提供SQL和多维度分析的OLAP工具。Kylin用于处理hadoop/spark场景下大量数据的预聚合，用户可自定义数据模型用于解决超过100亿+条记录的查询。</p><a id="more"></a><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="Not-Support"><a href="#Not-Support" class="headerlink" title="Not Support"></a>Not Support</h3><ul><li><p>建表或构建模型时，请勿使用中文列名。</p></li><li><p>不支持临时创建新列的统计</p></li></ul><p>原表有两个字段a和b，通过concat进行拼接，然后做count()或者count(distinct *)。Kylin并不支持上述做法，因为无法命中相关的cube。</p><p>不能统计下述例子：</p><p>count(distinct concat(cast(a as varchar),b))</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select count(distinct concat(cast(a as varchar),b))</span><br><span class="line">from table_a</span><br><span class="line">where dt = &apos;2018-05-01&apos;</span><br></pre></td></tr></table></figure><h3 id="Cube命中"><a href="#Cube命中" class="headerlink" title="Cube命中"></a>Cube命中</h3><p>所有的SQL需要命中相关Cube才可以使用。如果不是使用姿势问题，请联系管理员构建新的Cube。</p><ul><li>在join时，需要用事实表join维度表，负责容易出现：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">No realization found for OLAPContext, CUBE_NOT_READY, CUBE_NOT_READY, CUBE_NOT_READY, MODEL_UNMATCHED_JOIN, MODEL_UNMATCHED_JOIN</span><br></pre></td></tr></table></figure><ul><li>在写子查询时，不能将事实表写在查询中，Cube可能无法命中。</li></ul><h2 id="调度脚本"><a href="#调度脚本" class="headerlink" title="调度脚本"></a>调度脚本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime, requests</span><br><span class="line"></span><br><span class="line">auth_str = <span class="string">"Basic YWRtaW46S1lMSU6="</span></span><br><span class="line">url_str = <span class="string">"http://xxxxx.com:7070/kylin/api"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">auth</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用户认证</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    url = <span class="string">"&#123;url_str&#125;/user/authentication"</span>.format(url_str=url_str)</span><br><span class="line">    payload = <span class="string">"="</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Content-Type'</span>: <span class="string">"application/x-www-form-urlencoded"</span>,</span><br><span class="line">        <span class="string">'Authorization'</span>: auth_str,</span><br><span class="line">        <span class="string">'Cache-Control'</span>: <span class="string">"no-cache"</span></span><br><span class="line">        &#125;</span><br><span class="line">    response = requests.request(<span class="string">"POST"</span>, url, data=payload, headers=headers)</span><br><span class="line">    print(response.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cube</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    获取cube信息</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    url = <span class="string">"&#123;url_str&#125;/cubes"</span>.format(url_str=url_str)</span><br><span class="line">    querystring = &#123;<span class="string">"cubeName"</span>: <span class="string">"test_join"</span>&#125;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Cache-Control'</span>: <span class="string">"no-cache"</span>,</span><br><span class="line">        <span class="string">'Authorization'</span>: auth_str</span><br><span class="line">        &#125;</span><br><span class="line">    response = requests.request(<span class="string">"GET"</span>, url, headers=headers, params=querystring)</span><br><span class="line">    print(response.json())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_cube</span><span class="params">(cube_name, start_date, end_date, build_type)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    构建指定cube</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    startTime 和 endTime 应该是utc时间。</span></span><br><span class="line"><span class="string">    buildType 可以是 BUILD 、 MERGE 或 REFRESH。</span></span><br><span class="line"><span class="string">        - BUILD 用于构建一个新的segment，</span></span><br><span class="line"><span class="string">        - REFRESH 用于刷新一个已有的segment，</span></span><br><span class="line"><span class="string">        - MERGE 用于合并多个已有的segment生成一个较大的segment</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    url = <span class="string">"&#123;url_str&#125;/cubes/&#123;cube_name&#125;/rebuild"</span>.format(cube_name=cube_name, url_str=url_str)</span><br><span class="line">    start_stamp = int(datetime.datetime.strptime(start_date, <span class="string">'%Y-%m-%d %H:%M:%S'</span>).timestamp() * <span class="number">1000</span>)</span><br><span class="line">    end_stamp = int(datetime.datetime.strptime(end_date, <span class="string">'%Y-%m-%d %H:%M:%S'</span>).timestamp() * <span class="number">1000</span>)</span><br><span class="line">    payload = <span class="string">"&#123;\"startTime\": %d, \"endTime\": %d, \"buildType\": \"%s\"&#125;"</span> % (start_stamp, end_stamp, build_type)</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Content-Type'</span>: <span class="string">"application/json"</span>,</span><br><span class="line">        <span class="string">'Authorization'</span>: auth_str,</span><br><span class="line">        <span class="string">'Cache-Control'</span>: <span class="string">"no-cache"</span></span><br><span class="line">        &#125;</span><br><span class="line">    response = requests.request(<span class="string">"PUT"</span>, url, data=payload, headers=headers)</span><br><span class="line">    print(response.json())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    cube_name = <span class="string">"test_api"</span></span><br><span class="line">    start_date = <span class="string">'2018-05-01 04:00:00'</span></span><br><span class="line">    end_date = <span class="string">'2018-05-01 08:00:00'</span></span><br><span class="line">    build_type = <span class="string">'BUILD'</span></span><br><span class="line">    build_cube(cube_name, start_date, end_date, build_type)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;Kylin是一款处理海量数据，提供SQL和多维度分析的OLAP工具。Kylin用于处理hadoop/spark场景下大量数据的预聚合，用户可自定义数据模型用于解决超过100亿+条记录的查询。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Kylin" scheme="http://guzhenping.com/tags/Kylin/"/>
    
  </entry>
  
  <entry>
    <title>Clickhouse 高级SQL</title>
    <link href="http://guzhenping.com/2018/07/27/Clickhouse%20%E9%AB%98%E7%BA%A7SQL/"/>
    <id>http://guzhenping.com/2018/07/27/Clickhouse 高级SQL/</id>
    <published>2018-07-27T09:04:48.000Z</published>
    <updated>2018-11-30T02:43:37.103Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>测试数据集：</p><table><thead><tr><th>ca</th><th>cb</th><th>cc</th></tr></thead><tbody><tr><td>A</td><td>W</td><td>1</td></tr><tr><td>A</td><td>W</td><td>2</td></tr><tr><td>B</td><td>X</td><td>1</td></tr><tr><td>B</td><td>Z</td><td>2</td></tr><tr><td>B</td><td>Z</td><td>4</td></tr></tbody></table><h3 id="按最大-最小值-TOP1去重"><a href="#按最大-最小值-TOP1去重" class="headerlink" title="按最大/最小值/TOP1去重"></a>按最大/最小值/TOP1去重</h3><p>按ca和cb取cc最小值取值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">ca, </span><br><span class="line">cb, </span><br><span class="line">min(cc)</span><br><span class="line">from table</span><br><span class="line">group by ca, </span><br><span class="line">cb</span><br></pre></td></tr></table></figure><table><thead><tr><th>column_A</th><th>column_B</th><th>column_C</th></tr></thead><tbody><tr><td>A</td><td>W</td><td>1</td></tr><tr><td>B</td><td>X</td><td>1</td></tr><tr><td>B</td><td>Z</td><td>2</td></tr></tbody></table><p>取最大，则使用max()函数。取一组值的TOP1也是同理。</p><h3 id="按列合并多行（多-gt-少）"><a href="#按列合并多行（多-gt-少）" class="headerlink" title="按列合并多行（多-&gt;少）"></a>按列合并多行（多-&gt;少）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">ca, </span><br><span class="line">cb, </span><br><span class="line">groupUniqArray(cc)</span><br><span class="line">from table</span><br><span class="line">group by ca, cb</span><br></pre></td></tr></table></figure><table><thead><tr><th>column_A</th><th>column_B</th><th>column_C</th></tr></thead><tbody><tr><td>A</td><td>W</td><td>[2,1]</td></tr><tr><td>B</td><td>X</td><td>[1]</td></tr><tr><td>B</td><td>Z</td><td>[4,2]</td></tr></tbody></table><p>这是一种分组的概念，将相同的数据放在一起，需要被统计的数据放在array数据类型中。统计array，可以使用length()，获取数组长度，相当于分组count</p><p>此外，groupArray也可以满足需求。</p><h3 id="分组排序后取TopN"><a href="#分组排序后取TopN" class="headerlink" title="分组排序后取TopN"></a>分组排序后取TopN</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">SELECT ca,</span><br><span class="line">       groupArray(1)(cc) </span><br><span class="line">FROM</span><br><span class="line">  ( SELECT *</span><br><span class="line">   FROM table</span><br><span class="line">   ORDER BY ca,</span><br><span class="line">            cb,</span><br><span class="line">            cc )</span><br><span class="line">GROUP BY ca</span><br></pre></td></tr></table></figure><table><thead><tr><th>column_A</th><th>column_B</th><th>column_C</th></tr></thead><tbody><tr><td>A</td><td>W</td><td>[1]</td></tr><tr><td>B</td><td>X</td><td>[1]</td></tr><tr><td>B</td><td>Z</td><td>[2]</td></tr></tbody></table><p>以上是按cc列的值升序去top1。通过order by x改变顺序，再用groupArry(N)()函数处理获取top值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;测试数据集：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ca&lt;/th&gt;
&lt;th&gt;cb&lt;/th&gt;
&lt;th&gt;cc&lt;/th&gt;
&lt;/
      
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Clickhouse" scheme="http://guzhenping.com/tags/Clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>Redash Model源码分析</title>
    <link href="http://guzhenping.com/2018/07/26/Redash%20Model%E6%95%B4%E7%90%86%E5%88%86%E6%9E%90/"/>
    <id>http://guzhenping.com/2018/07/26/Redash Model整理分析/</id>
    <published>2018-07-26T09:04:48.000Z</published>
    <updated>2018-11-30T02:43:09.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>掌握Redash执行原理，对于深度的二次开发至关重要。</p><a id="more"></a><h2 id="query功能"><a href="#query功能" class="headerlink" title="query功能"></a>query功能</h2><p>SQL查询是Redash的核心功能之一。通常情况下，用户在前端会生成如下参数：</p><ul><li>query.data_source：下列列表中的数据源，必须选</li><li>parameter_values：用户自定义的参数值，可无</li><li>query.query_text：用户自定义的SQL文本，必须写</li><li>query.id：前端自动生成的查询id，系统生成</li></ul><p>在上述条件具备后，将会调用Redash API： <code>/api/queries/</code>进行任务提交。</p><p>所有任务将会被redash.handlers.query_result.run_query()方法接受处理。此处将进行参数校验，语法解析，任务传输等方面的处理。</p><p>多数的查询场景，先将SQL文本hash后，去缓存队列和后台数据库（Redash的Postgre）进行寻找，找不到再发给celery让worker执行该SQL。</p><p>负责具体执行：redash.models.QueryResult()类实例 和redash.tasks.queries.enqueue_query()方法。前者：负责查现有的结果；后者：立即执行相关SQL语句。</p><p>在enqueue_query()方法里，先连Redis,将任务加入队列，持续监听查询结果。</p><p>核心代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">while try_count &lt; 5:</span><br><span class="line">    try_count += 1</span><br><span class="line"></span><br><span class="line">    pipe = redis_connection.pipeline()</span><br><span class="line">    try:</span><br><span class="line">        pipe.watch(_job_lock_id(query_hash, data_source.id))</span><br><span class="line">        job_id = pipe.get(_job_lock_id(query_hash, data_source.id))</span><br><span class="line">        if job_id:</span><br><span class="line">            logging.info(&quot;[%s] Found existing job: %s&quot;, query_hash, job_id)</span><br><span class="line"></span><br><span class="line">            job = QueryTask(job_id=job_id)</span><br><span class="line"></span><br><span class="line">            if job.ready():</span><br><span class="line">                logging.info(&quot;[%s] job found is ready (%s), removing lock&quot;, query_hash, job.celery_status)</span><br><span class="line">                redis_connection.delete(_job_lock_id(query_hash, data_source.id))</span><br><span class="line">                job = None</span><br><span class="line"></span><br><span class="line">        if not job:</span><br><span class="line">            pipe.multi()</span><br><span class="line"></span><br><span class="line">            time_limit = None</span><br><span class="line"></span><br><span class="line">            if scheduled_query:</span><br><span class="line">                queue_name = data_source.scheduled_queue_name</span><br><span class="line">                scheduled_query_id = scheduled_query.id</span><br><span class="line">            else:</span><br><span class="line">                queue_name = data_source.queue_name</span><br><span class="line">                scheduled_query_id = None</span><br><span class="line">                time_limit = settings.ADHOC_QUERY_TIME_LIMIT</span><br><span class="line"></span><br><span class="line">            result = execute_query.apply_async(args=(query, data_source.id, metadata, user_id, scheduled_query_id),</span><br><span class="line">                                               queue=queue_name,</span><br><span class="line">                                               time_limit=time_limit)</span><br><span class="line">            job = QueryTask(async_result=result)</span><br><span class="line">            tracker = QueryTaskTracker.create(</span><br><span class="line">                result.id, &apos;created&apos;, query_hash, data_source.id,</span><br><span class="line">                scheduled_query is not None, metadata)</span><br><span class="line">            tracker.save(connection=pipe)</span><br><span class="line"></span><br><span class="line">            logging.info(&quot;[%s] Created new job: %s&quot;, query_hash, job.id)</span><br><span class="line">            pipe.set(_job_lock_id(query_hash, data_source.id), job.id, settings.JOB_EXPIRY_TIME)</span><br><span class="line">            pipe.execute()</span><br><span class="line">        break</span><br><span class="line"></span><br><span class="line">    except redis.WatchError:</span><br><span class="line">        continue</span><br></pre></td></tr></table></figure><p>以上是Query功能的分析。主要组件：</p><ul><li>celery分布式框架</li><li>redis缓存和任务队列</li><li>postgre后台DB。</li></ul><p>相关功能的设计比较中规中矩，报错异常的检测较少，后续值得优化。</p><h2 id="Model层"><a href="#Model层" class="headerlink" title="Model层"></a>Model层</h2><p>model层是Redash的设计核心，每个类对应后台数据库一张表和一个功能。</p><ul><li>access_permission, 权限表</li><li>alembic_version,版本号         </li><li>alert_subscriptions，报警描述</li><li>alerts，报警列表         </li><li>api_keys, api key管理</li><li>changes，升级改动</li><li>dashboards，报表存储</li><li>data_source_groups,每个组对应的数据源</li><li>data_sources，所有数据源</li><li>events，后台日志</li><li>groups，所有分组列表</li><li>notification_destinations，报警的模板和目的地</li><li>organizations， 组织         </li><li>queries，所有queris         </li><li>query_results，query的结果，另一种缓存</li><li>query_snippets，SQL的评论         </li><li>users, 用户已列表</li><li>visualizations，可视化图表存储</li><li>widgets，可视化控件</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;掌握Redash执行原理，对于深度的二次开发至关重要。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="可视化" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="Redash" scheme="http://guzhenping.com/tags/Redash/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发学习（Kudu）</title>
    <link href="http://guzhenping.com/2018/05/25/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Kudu%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/05/25/大数据开发学习（Kudu）/</id>
    <published>2018-05-25T09:04:48.000Z</published>
    <updated>2018-11-30T02:57:14.186Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>kudu，作为OLAP工具十分强劲。本文记录了笔者对其学习和使用的过程。</p><p>注明：这篇文章是研究Kudu在OLAP的场景，不准备讨论其他Case。</p><h2 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h2><p>最新版安装：<a href="https://kudu.apache.org/docs/installation.html" target="_blank" rel="noopener">Installing Apache Kudu</a></p><p>推荐使用VM进行快速体验，<a href="https://kudu.apache.org/docs/quickstart.html" target="_blank" rel="noopener">Apache Kudu Quickstart</a>。</p><p>当需要使用kudu client功能时，需要安装：kudu-client、kudu-client-devel这两个C++的库。</p><h2 id="常规使用"><a href="#常规使用" class="headerlink" title="常规使用"></a>常规使用</h2><p>教学Demo参见：<a href="https://kudu.apache.org/docs/quickstart.html" target="_blank" rel="noopener">Apache Kudu Quickstart</a></p><p>以上内容是对静态数据的使用，如果是一条实时的数据流，则采用kudu API的方式。需要去做定制开发。</p><h2 id="架构体系"><a href="#架构体系" class="headerlink" title="架构体系"></a>架构体系</h2><p>kudu架构：<br><img src="static/kudu/kudu.png" alt=""></p><p>关于运行原理的文章，推荐：<a href="http://www.nosqlnotes.com/technotes/kudu-design/" target="_blank" rel="noopener">Kudu设计原理初探</a></p><h2 id="OLAP服务"><a href="#OLAP服务" class="headerlink" title="OLAP服务"></a>OLAP服务</h2><p>作为OLAP服务，ETL环节的处理至关重要。以下列举了小米公司的场景：<br><img src="static/kudu/kudu-etl1.jpg" alt=""></p><p>对比：<br><img src="static/kudu/kudu-etl2.jpg" alt=""></p><p>可以看出，除了日志数据外，线上业务数据都可以实时同步到kudu里。基于kudu对外提供OLAP服务，数据的实时性非常可观。</p><p>在小米，采用impala作为查询（计算）引擎，但是网上也有presto on kudu的组件可供选型（<a href="https://github.com/MartinWeindel/presto-kudu" target="_blank" rel="noopener">传送门</a>）。</p><h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>微店：</p><ul><li><a href="https://datascience.weidian-inc.com/kudu_impala/" target="_blank" rel="noopener">Kudu+Impala介绍</a><ul><li><a href="https://datascience.weidian-inc.com/kudu_schema_design/" target="_blank" rel="noopener">Kudu的Schema表结构设计</a></li></ul></li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>运行原理：<a href="https://blog.csdn.net/cdxxx5708/article/details/79074763" target="_blank" rel="noopener">kudu内部机制</a></p><p>关于Kudu的介绍： <a href="https://kudu.apache.org/docs/#_kudu_impala_integration_features" target="_blank" rel="noopener">Introducing Apache Kudu</a> </p><p>基于Kudu搭建OLAP工具：<a href="https://myslide.cn/slides/3584?vertical=1" target="_blank" rel="noopener">小米：使用Kudu搭建OLAP服务</a></p><p>基于Kudu的实际应用：<a href="http://www.infoq.com/cn/articles/spark-streaming-kudu-impala" target="_blank" rel="noopener">使用Spark Streaming + Kudu + Impala构建一个预测引擎</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;kudu，作为OLAP工具十分强劲。本文记录了笔者对其学习和使用的过程。&lt;/p&gt;
&lt;p&gt;注明：这篇文章是研究Kudu在OLAP的场景，不准备
      
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Kudu" scheme="http://guzhenping.com/tags/Kudu/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发学习（Clickhouse）</title>
    <link href="http://guzhenping.com/2018/05/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Clickhouse%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/05/24/大数据开发学习（Clickhouse）/</id>
    <published>2018-05-24T09:04:48.000Z</published>
    <updated>2018-11-30T02:10:54.811Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>1年前，用过Greenplum，这是第一次接触MPP结构的OLAP系统。今天市面上常见的MPP架构工具，还有Clickhouse和Palo等。(当然，SQL on Hadoop体系的Presto和Impala也算是MPP结构，只是数据存储方面没有自己的东西，都是依赖hdfs,mysql等。)</p><p>工作需要，对Clickhouse进行学习。<br><a id="more"></a></p><h2 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h2><p>Ubuntu上比较好安装，但是一般公司用的服务器都是Centos。这里只讨论在Centos 7的安装方式，centos 6请在Altinity公司提供的下载界面中自行寻找。</p><h3 id="安装命令"><a href="#安装命令" class="headerlink" title="安装命令"></a>安装命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">如果是装54362版本的包，其余所有依赖都需要是一致的。</span><br><span class="line"></span><br><span class="line">准备源依赖，由Altinity公司提供：</span><br><span class="line">curl -s https://packagecloud.io/install/repositories/Altinity/clickhouse/script.rpm.sh | sudo bash</span><br><span class="line"></span><br><span class="line">server-common:</span><br><span class="line">sudo yum install clickhouse-server-common-1.1.54362-1.el7.x86_64</span><br><span class="line">sudo yum install clickhouse-server-common-1.1.54380-1.el7.x86_64</span><br><span class="line"></span><br><span class="line">server:</span><br><span class="line">sudo yum install clickhouse-server-1.1.54362-1.el7.x86_64</span><br><span class="line">sudo yum install clickhouse-server-1.1.54380-1.el7.x86_64</span><br><span class="line"></span><br><span class="line">client：</span><br><span class="line">sudo yum install clickhouse-client-1.1.54362-1.el7.x86_64</span><br><span class="line">sudo yum install clickhouse-client-1.1.54380-1.el7.x86_64</span><br></pre></td></tr></table></figure><p>关于Altinity公司的其他版本，可访问<a href="https://packagecloud.io/Altinity/clickhouse" target="_blank" rel="noopener">这里下载</a>。</p><p>以上安装如有疑问，可以使用下方安装方式：</p><ul><li><a href="https://github.com/red-soft-ru/clickhouse-rpm" target="_blank" rel="noopener">https://github.com/red-soft-ru/clickhouse-rpm</a></li></ul><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server端：</span><br><span class="line">sudo /etc/rc.d/init.d/clickhouse-server start</span><br><span class="line"></span><br><span class="line">client端：</span><br><span class="line">clickhouse-client</span><br></pre></td></tr></table></figure><h2 id="使用经验"><a href="#使用经验" class="headerlink" title="使用经验"></a>使用经验</h2><h3 id="常用SQL"><a href="#常用SQL" class="headerlink" title="常用SQL"></a>常用SQL</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-- 查看集群情况</span><br><span class="line">select * from system.clusters;</span><br><span class="line"></span><br><span class="line">-- 查看分区情况</span><br><span class="line">select </span><br><span class="line">partition,</span><br><span class="line">name,</span><br><span class="line">rows</span><br><span class="line">from system.parts;</span><br></pre></td></tr></table></figure><h3 id="MergeTree"><a href="#MergeTree" class="headerlink" title="MergeTree"></a>MergeTree</h3><p>选择engine时，尽量用merge tree.MergeTree引擎支持以主键和日期作为索引，提供实时更新数据的可能性。这是Clickhouse中最先进的表引擎。</p><p>数据表将数据分割为小的索引块进行处理。每个索引块之间依照主键排序。每个索引块记录了指定的开始日期和结束日期。插入数据时，MergeTree会对数据进行排序，以保证存储在索引块中的数据有序。当写入新数据时，会放在新的文件夹下，索引块之间的合并过程会在系统后台定期自动执行。MergeTree引擎会选择几个相邻的索引块进行合并，然后对二者合并排序。</p><p><img src="static/clickhouse/mergetree1.png" alt=""></p><p><img src="static/clickhouse/mergetree2.png" alt=""></p><p><img src="static/clickhouse/mergetree3.png" alt=""></p><p><img src="static/clickhouse/mergetree4.png" alt=""></p><p><img src="static/clickhouse/mergetree5.png" alt=""></p><p><img src="static/clickhouse/mergetree6.png" alt=""></p><p><img src="static/clickhouse/mergetree7.png" alt=""></p><h3 id="Distributed"><a href="#Distributed" class="headerlink" title="Distributed"></a>Distributed</h3><p>相当于数据库的视图，并不存储数据，而是用来做分布式的写入和查询，与其他引擎结合使用。</p><p>一种集群拓扑结构：<br><img src="static/clickhouse/distributed.jpg" alt=""></p><h2 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题"></a>实际问题</h2><ol><li>关键字大小写敏感，对强转支持不太好。</li><li>特性上不支持事务，不支持update/delete。</li><li>关于分布式<br><img src="static/clickhouse/question_clickhouse1.png" alt=""></li><li>在分布式表上执行count()时，发现结果不一致。</li></ol><p>暂无解决办法。问题:<a href="https://github.com/yandex/ClickHouse/issues/1443" target="_blank" rel="noopener">https://github.com/yandex/ClickHouse/issues/1443</a></p><ol start="5"><li>查询时，内存超出限制：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Progress: 4.84 million rows, 42.70 MB (45.34 million rows/s., 399.59 MB/s.)  87%</span><br><span class="line"></span><br><span class="line">Received exception from server:</span><br><span class="line">Code: 241. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Memory limit (for query) exceeded: would use 12.15 GiB (attempt to allocate chunk of 4294967296 bytes), maximum: 9.31 GiB.</span><br><span class="line"></span><br><span class="line">0 rows in set. Elapsed: 48.023 sec. Processed 4.84 million rows, 42.70 MB (100.89 thousand rows/s., 889.21 KB/s.)</span><br><span class="line"></span><br><span class="line">Progress: 4.89 million rows, 43.81 MB (23.23 thousand rows/s., 208.09 KB/s.)  88%</span><br><span class="line"></span><br><span class="line">Received exception from server:</span><br><span class="line">Code: 241. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Memory limit (for query) exceeded: would use 80.15 GiB (attempt to allocate chunk of 17179869184 bytes), maximum: 74.51 GiB.</span><br><span class="line"></span><br><span class="line">3668208133 rows in set. Elapsed: 235.536 sec. Processed 4.89 million rows, 43.81 MB (20.77 thousand rows/s., 186.00 KB/s.)</span><br><span class="line"></span><br><span class="line">Received exception from server (version 1.1.54362):</span><br><span class="line">Code: 241. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Memory limit (for query) exceeded: would use 96.14 GiB (attempt to allocate chunk of 17179869184 bytes), maximum: 93.13 GiB.</span><br><span class="line"></span><br><span class="line">8816716667 rows in set. Elapsed: 428.543 sec. Processed 4.89 million rows, 43.81 MB (11.41 thousand rows/s., 102.23 KB/s.)</span><br></pre></td></tr></table></figure><p>当处理的数据集超过设定的阈值以后，会触发限制，并返回已经处理好的结果。</p><p>解决办法：<br>修改max_memory_usage的值（user.xml）</p><p>不同的节点可以处理设置不同。在高并发分流的时候，尽量把query分摊到各个机器上，否则会将某一节点资源耗尽。</p><p>这个问题导致Clickhouse的高并发特性很差。在前端暴露端口时，不能单独暴露一个host,需要做成有命名空间的方式，或者有个query平衡器的机制。</p><p>解决办法：<br>就上述问题，发现一个公司采用如下架构：</p><p><img src="static/clickhouse/clickhouse_rebalance.jpg" alt=""></p><p>即：域名轮询。</p><h2 id="SQL-实战"><a href="#SQL-实战" class="headerlink" title="SQL 实战"></a>SQL 实战</h2><p>第一步建表：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table test_analysis (created_at DateTime, dt Date,  user String,  page_id String ) ENGINE=MergeTree(dt, (user, dt), 8192);</span><br></pre></td></tr></table></figure></p><p>插入测试数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert into table test_analysis Format Values</span><br><span class="line">(&apos;2018-4-24 18:45&apos;,&apos;2018-4-24&apos;,&apos;A&apos;,&apos;首页&apos;),</span><br><span class="line">(&apos;2018-4-24 18:46&apos;,&apos;2018-4-24&apos;,&apos;A&apos;,&apos;购物车&apos;),</span><br><span class="line">(&apos;2018-4-24 18:45&apos;,&apos;2018-4-24,&apos;B&apos;,&apos;首页&apos;),</span><br><span class="line">(&apos;2018-4-24 18:48&apos;,&apos;2018-4-24&apos;,&apos;B&apos;,&apos;商品详情&apos;),</span><br><span class="line">(&apos;2018-4-24 18:49&apos;,&apos;2018-4-24&apos;,&apos;B&apos;,&apos;购物车&apos;),</span><br><span class="line">(&apos;2018-4-24 18:46&apos;,&apos;2018-4-24&apos;,&apos;C&apos;,&apos;商品详情&apos;);</span><br></pre></td></tr></table></figure><p>第二步，建立模型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">SELECT `user`,</span><br><span class="line">       created_at,</span><br><span class="line">       page_id,</span><br><span class="line">       gap1/60 AS &quot;与第一个动作的间隔时间&quot;,</span><br><span class="line">       if(gap1 == 0, 0, runningDifference(gap1)/60) AS &quot;与上一个动作的间隔时间&quot;</span><br><span class="line">FROM</span><br><span class="line">  (SELECT `user`,</span><br><span class="line">          created_at,</span><br><span class="line">          fist_created_at,</span><br><span class="line">          page_id,</span><br><span class="line">          created_at-fist_created_at AS gap1</span><br><span class="line">   FROM test_analysis ANY</span><br><span class="line">   LEFT JOIN</span><br><span class="line">     (SELECT `user` ,</span><br><span class="line">             min(created_at) AS fist_created_at</span><br><span class="line">      FROM test_analysis</span><br><span class="line">      GROUP BY `user`) using(`user`)) AS t</span><br></pre></td></tr></table></figure><p>获得的结果：</p><p><a href="http://www.clickhouse.com.cn/topic/5adf0d0a9d28dfde2ddc5fb2" target="_blank" rel="noopener">clickhouse 实战</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><h3 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h3><p><a href="https://clickhouse.yandex/docs/en/single/#introduction" target="_blank" rel="noopener">What is ClickHouse</a></p><p>官方写的体系结构文章：<a href="https://clickhouse.yandex/docs/en/development/architecture/" target="_blank" rel="noopener">Overview of ClickHouse architecture</a></p><p>翻译:<a href="http://www.clickhouse.com.cn/topic/5a3df7b02141c2917483557c" target="_blank" rel="noopener">ClickHouse 内部架构介绍</a></p><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p><a href="https://kuaibao.qq.com/s/20180409G06IIM00?refer=spider" target="_blank" rel="noopener">ClickHouse相关配置剖析</a></p><p><a href="http://note.abeffect.com/note/articles/2017/12/18/1513590469620.html" target="_blank" rel="noopener">ClickHouse的分布式引擎</a></p><h3 id="用户权限"><a href="#用户权限" class="headerlink" title="用户权限"></a>用户权限</h3><p><a href="https://www.jianshu.com/p/e339336e7bb9" target="_blank" rel="noopener">ClickHouse 用户名密码设置</a></p><p><a href="http://www.cnblogs.com/gomysql/p/6708796.html" target="_blank" rel="noopener">ClickHouse之访问权限控制</a></p><h3 id="引擎介绍"><a href="#引擎介绍" class="headerlink" title="引擎介绍"></a>引擎介绍</h3><p><a href="https://www.jianshu.com/p/48dbf2db2765" target="_blank" rel="noopener">ClickHouse MergeTree引擎介绍</a></p><p><a href="http://www.clickhouse.com.cn/topic/5a3e768d2141c2917483557e" target="_blank" rel="noopener">ClickHouse Distribute 引擎深度解读</a></p><p><a href="http://www.linkedkeeper.com/detail/blog.action?bid=1117" target="_blank" rel="noopener">实时大数据分析引擎ClickHouse介绍</a></p><h3 id="数据同步"><a href="#数据同步" class="headerlink" title="数据同步"></a>数据同步</h3><p>kafka-&gt;Clickhouse：<a href="http://jackpgao.github.io/2017/12/27/ClickHouse-with-Hangout/" target="_blank" rel="noopener">Hangout with ClickHouse</a></p><p>mysql-&gt;Clickhouse：<a href="http://jackpgao.github.io/2018/02/04/ClickHouse-Use-MySQL-Data/" target="_blank" rel="noopener">使用ClickHouse一键接管MySQL数据分析</a></p><h3 id="杂文"><a href="#杂文" class="headerlink" title="杂文"></a>杂文</h3><p>奶clickhouse的文章：<a href="https://zhuanlan.zhihu.com/p/33371816" target="_blank" rel="noopener">ClickHouse Beijing Meetup-数据分析领域的黑马-ClickHouse-新浪-高鹏</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;1年前，用过Greenplum，这是第一次接触MPP结构的OLAP系统。今天市面上常见的MPP架构工具，还有Clickhouse和Palo等。(当然，SQL on Hadoop体系的Presto和Impala也算是MPP结构，只是数据存储方面没有自己的东西，都是依赖hdfs,mysql等。)&lt;/p&gt;
&lt;p&gt;工作需要，对Clickhouse进行学习。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Clickhouse" scheme="http://guzhenping.com/tags/Clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库学习（概念篇）</title>
    <link href="http://guzhenping.com/2018/05/24/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%A6%82%E5%BF%B5%E7%AF%87%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/05/24/数据仓库学习（概念篇）/</id>
    <published>2018-05-24T09:04:48.000Z</published>
    <updated>2018-11-30T02:53:03.735Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>写点数据仓库（DW）的一些常用知识,架构等方面。加深基本功，保持进阶的心。</p><a id="more"></a><h2 id="典型架构"><a href="#典型架构" class="headerlink" title="典型架构"></a>典型架构</h2><p><img src="static/ScreenShot2017-01-13at11.09.58.png" alt="数据仓库系统架构"></p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul><li>数据仓库</li></ul><blockquote><p>数据仓库是一个面向主题的、集成的、非易失的（nonvolatile）、随时间变化的（time-variant）用来支持管理人员决策的数据集合。</p></blockquote><blockquote><p>—-William H.Inmon</p></blockquote><p>以上也是数据仓库区别于业务系统的4个特征。</p><ul><li>OLAP</li></ul><p>联机分析处理（On-line Analytical Processing），该概念由数据库创始人E. F. Codd于1993年提出。OLAP理事会（OLAP Council）进一步完善：OLAP是一种软件技术，它使分析人员、经理和执行官能够迅速、一致、交互的从各方面观察信息，以达到深入理解数据的目的。</p><ul><li>OLTP</li></ul><p>联机事务处理（On-line Transaction Processing）</p><ul><li>ODS </li></ul><p>操作数据存储（Operational Data Store, 简称ODS）。在数据仓库系统中，ODS存储了原始数据经过集成统一后的数据。</p><ul><li>DW</li></ul><p>数据仓库（Data Warehouse，简称DW）。在数据仓库系统中，DW数据库存储了整个企业的所有历史数据，是狭义上的数据仓库。DW数据库需要满足企业数据分析的各种需求，以及各个部门建设数据集市的需求，通常存储企业的基础数据和通用数据。</p><ul><li>数据集市(Data Market)</li></ul><p>数据集市是指针对特定部门和主题的小型数据仓库，数据从DW中获取。</p><ul><li>ETL</li></ul><p>ETL（Extract，Transform，Load）表示抽取、转换和装载，数据从多个同构或异构的数据源抽取出来，经过自定义的转换操作，最终装载进入目标表的过程叫做一次ETL。ETL是数据进入ODS、DW、Data Market的主要方式。</p><h2 id="建模方法"><a href="#建模方法" class="headerlink" title="建模方法"></a>建模方法</h2><ul><li>维度建模法（Dimensional Modeling）</li></ul><p>Ralph Kimball主张，</p><ul><li>实体-关系建模（Entity-Relationship Medeling）</li></ul><p>实体-关系建模也叫做第三范式建模（Third Normal Form, 3NF）,是William H.Inmon主张的一种数据仓库建模方法。</p><h2 id="数据与建模"><a href="#数据与建模" class="headerlink" title="数据与建模"></a>数据与建模</h2><p><a href="https://www.zhihu.com/question/19955124" target="_blank" rel="noopener">OLAP中roll-up和drill-down和slicing？</a></p><p><img src="static/olap.jpg" alt=""><br>真正好的建模，应该契合这些功能转换。见过Tableau做的很好，但不算是OLAP系统自带的功能。</p><h2 id="数仓进化史"><a href="#数仓进化史" class="headerlink" title="数仓进化史"></a>数仓进化史</h2><p>比较推荐这篇文章：<a href="http://lxw1234.com/archives/2016/03/624.htm" target="_blank" rel="noopener">从数据仓库到大数据，数据平台这25年是怎样进化的？</a></p><p>在架构方面，偏理论一点的文章：<a href="http://www.clickhouse.com.cn/topic/5ae13a599d28dfde2ddc5fc5#5aea82689d28dfde2ddc6014" target="_blank" rel="noopener">大数据分析的下一代架构–IOTA</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://segmentfault.com/a/1190000002672666" target="_blank" rel="noopener">hadoop HDFS常用文件操作命令</a></li><li><a href="http://book.51cto.com/art/201409/452359.htm" target="_blank" rel="noopener">HDFS 文件操作命令</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;写点数据仓库（DW）的一些常用知识,架构等方面。加深基本功，保持进阶的心。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="数据仓库" scheme="http://guzhenping.com/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Redash权限管理</title>
    <link href="http://guzhenping.com/2018/05/21/Redash%20%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/"/>
    <id>http://guzhenping.com/2018/05/21/Redash 权限管理/</id>
    <published>2018-05-21T09:04:48.000Z</published>
    <updated>2018-11-30T02:42:24.499Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Reash是一个数据查询平台，必定会涉及权限管理。主要由3个概念：组（group）,用户（user）， 数据源（data source）。</p><p>group在最上层，一个group对应多个user 和 data source。反之也可行，但不利于权限的管理。</p><p>权限功能主要基于组（group）和所属数据源(data source)来控制。一个用户（user）必须属于一个或多个组，当新用户进入时，该用户默认在“default组”。在新增一个数据源时，该数据源默认归属于”default组”。</p><a id="more"></a><h2 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h2><p>数据源有两个权限设置：</p><ul><li>Full access，即：该组的用户可操作被保存的查询（可以改SQL源码），创建一个新的查询</li><li>View Only access，即：该组的用户只能阅读被保存的查询（看不到SQL源码）及其结果</li></ul><p>对于不需要写SQL取结果的用户群来说，应该与需要写SQL取结果的用户群体区分在不同组（group）中。比如：基础架构查看报表组，基础架构制作报表组。</p><p>如果想对一个组的用户做table级别的查询限制，官方提供的方案：</p><blockquote><p>The idea is to leverage your database’s security model and hence create a user with access to the tables/columns you want to give access to. Create a data source that’s using this user and then associate it with a group of users who need this level of access.</p></blockquote><p>翻译过来：在建立数据源时，配置一个带有权限控制的数据库用户即可。比如，连接mysql时，用一个只能查询测试db/table的用户名进行连接。把这个数据源赋给某个组，然后该组的所有用户，只能看到这个数据源里的测试db/table。</p><h2 id="组"><a href="#组" class="headerlink" title="组"></a>组</h2><p>组可设置的权限有：</p><ul><li>admin/super_admin,管理员/超级管理员，可用所用功能</li><li>create_dashboard,创建dashboard</li><li>create_query,创建SQL查询</li><li>edit_dashboard,编辑自己/别人的dashboard</li><li>edit_query,编辑自己/别人的SQL查询</li><li>view_query,查看已经存在的SQL</li><li>view_source,查看SQL源码</li><li>execute_query,执行SQL</li><li>list_users,看到所有用户</li><li>schedule_query,设置定时刷新</li><li>list_dashboards,看到所用的dashboard</li><li>list_alerts,看到所有的提醒任务</li><li>list_data_sources,看到所有的数据源</li></ul><p>将以上权限赋给不同的组，每个组的用户就可以实现不同的功能。Redash不强调对用户做太多的权限控制，因为一个用户必须要归属于一个组。所以，对组做现在即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Reash是一个数据查询平台，必定会涉及权限管理。主要由3个概念：组（group）,用户（user）， 数据源（data source）。&lt;/p&gt;
&lt;p&gt;group在最上层，一个group对应多个user 和 data source。反之也可行，但不利于权限的管理。&lt;/p&gt;
&lt;p&gt;权限功能主要基于组（group）和所属数据源(data source)来控制。一个用户（user）必须属于一个或多个组，当新用户进入时，该用户默认在“default组”。在新增一个数据源时，该数据源默认归属于”default组”。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="可视化" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="Redash" scheme="http://guzhenping.com/tags/Redash/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://guzhenping.com/2018/05/16/OLAP%E5%B7%A5%E5%85%B7%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6/"/>
    <id>http://guzhenping.com/2018/05/16/OLAP工具的关键技术研究/</id>
    <published>2018-05-15T16:00:00.000Z</published>
    <updated>2018-11-28T06:32:44.102Z</updated>
    
    <content type="html"><![CDATA[<h1 id="OLAP工具的关键技术研究"><a href="#OLAP工具的关键技术研究" class="headerlink" title="OLAP工具的关键技术研究"></a>OLAP工具的关键技术研究</h1><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>OLAP工具种类繁多，所用技术都有独到之处。这里一一研究。</p><h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><p>这里总结和计算领域相关的技术点。</p><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p><a href="http://mysql.taobao.org/monthly/2017/01/06/" target="_blank" rel="noopener">PgSQL · 引擎介绍 · 向量化执行引擎简介</a></p><h2 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h2><p>这里总结和存储相关的技术点。</p><h3 id="Bitmap"><a href="#Bitmap" class="headerlink" title="Bitmap"></a>Bitmap</h3><p>Bit即比特，是目前计算机系统里边数据的最小单位，8个bit即为一个Byte。一个bit的值，或者是0，或者是1；也就是说一个bit能存储的最多信息是2。</p><p>Bitmap可以理解为通过一个bit数组来存储特定数据的一种数据结构；由于bit是数据的最小单位，所以这种数据结构往往是非常节省存储空间。比如一个公司有8个员工，现在需要记录公司的考勤记录，传统的方案是记录下每天正常考勤的员工的ID列表，比如2012-01-01:[1,2,3,4,5,6,7,8]。假如员工ID采用byte数据类型，则保存每天的考勤记录需要N个byte，其中N是当天考勤的总人数。另一种方案则是构造一个8bit（01110011）的数组，将这8个员工跟员工号分别映射到这8个位置，如果当天正常考勤了，则将对应的这个位置置为1，否则置为0；这样可以每天采用恒定的1个byte即可保存当天的考勤记录。</p><p>综上所述，Bitmap节省大量的存储空间，因此可以被一次性加载到内存中。</p><p><a href="http://www.infoq.com/cn/articles/the-secret-of-bitmap" target="_blank" rel="noopener">Bitmap的秘密</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;OLAP工具的关键技术研究&quot;&gt;&lt;a href=&quot;#OLAP工具的关键技术研究&quot; class=&quot;headerlink&quot; title=&quot;OLAP工具的关键技术研究&quot;&gt;&lt;/a&gt;OLAP工具的关键技术研究&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://guzhenping.com/2018/04/10/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Durid%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/04/10/大数据开发学习（Durid）/</id>
    <published>2018-04-09T16:00:00.000Z</published>
    <updated>2018-11-28T06:32:44.086Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-Druid介绍"><a href="#一-Druid介绍" class="headerlink" title="一 Druid介绍"></a>一 Druid介绍</h2><p>Druid和kylin很像，在工作中先接触了kylin以后，为了调研这个竞品，特地进行学习。</p><blockquote><p>Druid is an open-source data store designed for sub-second queries on real-time and historical data. It is primarily used for business intelligence (OLAP) queries on event data. Druid provides low latency (real-time) data ingestion, flexible data exploration, and fast data aggregation. Existing Druid deployments have scaled to trillions of events and petabytes of data. Druid is most commonly used to power user-facing analytic applications.</p></blockquote><p>看起来Druid</p><h2 id="Druid-安装与启动"><a href="#Druid-安装与启动" class="headerlink" title="Druid 安装与启动"></a>Druid 安装与启动</h2><p>暂时未掌握。</p><h2 id="度量计算"><a href="#度量计算" class="headerlink" title="度量计算"></a>度量计算</h2><p>和Kylin一样提供了常用的计算函数，但是不支持精确去重。</p><p>kylin目前提供9种计算方法。</p><ol><li><p>sum</p></li><li><p>min </p></li></ol><ul><li>max</li><li>count</li><li>count_distinct</li><li>top_n</li><li>raw</li><li>extended_column</li><li>percentile</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://www.jianshu.com/p/4fc1951a7245" target="_blank" rel="noopener">Druid实时大数据分析-快速入门</a> </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一-Druid介绍&quot;&gt;&lt;a href=&quot;#一-Druid介绍&quot; class=&quot;headerlink&quot; title=&quot;一 Druid介绍&quot;&gt;&lt;/a&gt;一 Druid介绍&lt;/h2&gt;&lt;p&gt;Druid和kylin很像，在工作中先接触了kylin以后，为了调研这个竞品，特地进
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://guzhenping.com/2018/03/23/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Spark%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/03/23/大数据开发学习（Spark）/</id>
    <published>2018-03-22T16:00:00.000Z</published>
    <updated>2018-11-28T06:32:44.085Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据开发学习（Spark）"><a href="#大数据开发学习（Spark）" class="headerlink" title="大数据开发学习（Spark）"></a>大数据开发学习（Spark）</h1><hr><h2 id="一-前言"><a href="#一-前言" class="headerlink" title="一 前言"></a>一 前言</h2><p>平时接触了一些spark的应用场景，持续保持学习。</p><h2 id="二-安装"><a href="#二-安装" class="headerlink" title="二 安装"></a>二 安装</h2><p>一般都用on yarn的方式</p><h2 id="三-实际问题"><a href="#三-实际问题" class="headerlink" title="三 实际问题"></a>三 实际问题</h2><p>Kylin在采用spark作为计算引擎时，需要进行参数配置。以下答案给了很好的启发性，特地记录：</p><blockquote><p>Note that yarn.nodemanager.resource.memory-mb is total memory that a single NodeManager can allocate across all containers on one node.<br>In your case, since yarn.nodemanager.resource.memory-mb = 12G, if you add up the memory allocated to all YARN containers on any single node, it cannot exceed 12G.<br>You have requested 11G (-executor-memory 11G) for each Spark Executor container. Though 11G is less than 12G, this still won’t work. Why ?</p><ul><li>Because you have to account for spark.yarn.executor.memoryOverhead, which is min(executorMemory * 0.10, 384) (by default, unless you override it).<br>So, following math must hold true:<br>spark.executor.memory + spark.yarn.executor.memoryOverhead &lt;= yarn.nodemanager.resource.memory-mb<br>See: <a href="https://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-yarn.html</a> for latest documentation on spark.yarn.executor.memoryOverhead<br>Moreover, spark.executor.instances is merely a request. Spark ApplicationMaster for your application will make a request to YARN ResourceManager for number of containers = spark.executor.instances. Request will be granted by ResourceManager on NodeManager node based on:</li><li>Resource availability on the node. YARN scheduling has its own nuances - this is a good primer on how YARN FairScheduler works.</li><li>Whether yarn.nodemanager.resource.memory-mb threshold has not been exceeded on the node:<ul><li>(number of spark containers running on the node <em> (spark.executor.memory + spark.yarn.executor.memoryOverhead)) &lt;= yarn.nodemanager.resource.memory-mb</em><br>If the request is not granted, request will be queued and granted when above conditions are met</li></ul></li></ul></blockquote><blockquote><p>from:<br><a href="https://stackoverflow.com/questions/29940711/apache-spark-setting-executor-instances-does-not-change-the-executors" target="_blank" rel="noopener">https://stackoverflow.com/questions/29940711/apache-spark-setting-executor-instances-does-not-change-the-executors</a></p></blockquote><h2 id="四-参考资料"><a href="#四-参考资料" class="headerlink" title="四 参考资料"></a>四 参考资料</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;大数据开发学习（Spark）&quot;&gt;&lt;a href=&quot;#大数据开发学习（Spark）&quot; class=&quot;headerlink&quot; title=&quot;大数据开发学习（Spark）&quot;&gt;&lt;/a&gt;大数据开发学习（Spark）&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;一-前言&quot;&gt;&lt;a hre
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>大数据开发学习（Kylin）</title>
    <link href="http://guzhenping.com/2018/01/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Kylin%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/01/24/大数据开发学习（Kylin）/</id>
    <published>2018-01-24T09:04:48.000Z</published>
    <updated>2018-11-30T02:55:33.282Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-Kylin介绍"><a href="#一-Kylin介绍" class="headerlink" title="一 Kylin介绍"></a>一 Kylin介绍</h2><p>kylin是一款OLAP工具，背靠Hadoop，HBase，Spark，Kafka等大山，提供神奇体验。</p><a id="more"></a><h2 id="Kylin-安装与启动"><a href="#Kylin-安装与启动" class="headerlink" title="Kylin 安装与启动"></a>Kylin 安装与启动</h2><p>除了常规环境，还需要hadoop的historyjobserver启动。</p><p>具体参见官网，启动较为容易。</p><h2 id="度量计算"><a href="#度量计算" class="headerlink" title="度量计算"></a>度量计算</h2><p>目前提供9种计算方法。</p><ol><li><p>sum</p></li><li><p>min </p></li></ol><ul><li>max</li><li>count</li><li>count_distinct</li><li>top_n</li><li>raw</li><li>extended_column</li><li>percentile</li></ul><h2 id="建模心得"><a href="#建模心得" class="headerlink" title="建模心得"></a>建模心得</h2><p>在通过星形模型建立事实表+维度表的过程中，操作比较复杂。但是，通过view的方式，就比较简单。</p><p>view建模的方式，所有结果都在一张表里，只需要对该表进行维度和度量的划分即可。</p><h3 id="建模结果"><a href="#建模结果" class="headerlink" title="建模结果"></a>建模结果</h3><p>测试结果1：</p><p><img src="static/kylin建模2.png" alt=""></p><p>测试结果2：<br><img src="static/kylin建模.png" alt=""></p><h2 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题"></a>实际问题</h2><p>问题1：在同一个project里的Insight，可以看到所用cube的维度，但是不能共用，会报：No model found for rel</p><p>问题2：kylin不支持复杂的列，比如map,array类型。问题参见：<br><a href="https://mail-archives.apache.org/mod_mbox/kylin-dev/201512.mbox/%3C565D565C.3060205@jd.com%3E" target="_blank" rel="noopener">https://mail-archives.apache.org/mod_mbox/kylin-dev/201512.mbox/%3C565D565C.3060205@jd.com%3E</a></p><p>解决方法：创建该表的视图，剔除复杂列</p><p>问题3：kylin建model时，想做增量构建，在指定某列时间作为partition时，该列内容应该满足‘yyyy-MM-dd HH:mm:ss’的样子，不要使用unix_timestamp的类型。否则，构建后cube大小为零。 </p><p>问题4：kylin建cube所用的字段最好不要采用kylin 关键字，例如:year, month, day, hour等。否则写SQL时，不太友好。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">想查一段 简单的pv和uv SQL，</span><br><span class="line">select platform, year, month, day ,count(*) as pv count(distinct guid) as uv from kylin_tracking_view group by platform, year, month, day </span><br><span class="line"></span><br><span class="line">在Kylin的查询界面中，应当这么写:</span><br><span class="line">select platform,</span><br><span class="line">    &quot;YEAR&quot;,</span><br><span class="line">    &quot;MONTH&quot;,</span><br><span class="line">    &quot;DAY&quot;,</span><br><span class="line">    count(*) as pv,</span><br><span class="line">    count(distinct guid) as uv</span><br><span class="line">from kylin_tracking_view </span><br><span class="line">group by </span><br><span class="line">platform,</span><br><span class="line">    &quot;YEAR&quot;,</span><br><span class="line">    &quot;MONTH&quot;,</span><br><span class="line">    &quot;DAY&quot;</span><br><span class="line"></span><br><span class="line">可以看出，关键词必须全部大写，且被双引号(必须是双引号，单引是自定义常量)包住。</span><br><span class="line"></span><br><span class="line">建议提前规范好数据源，免得造成巨大的返工。</span><br></pre></td></tr></table></figure><p>问题5：build维度过不去<br>具体报错： [BadQueryDetector] service.BadQueryDetector:160 : System free memory less than 100 MB. 0 queries running</p><p>暂无解决办法。</p><p>这个问题可能是，kylin维度较多，把regionserver搞成僵死，进而导致的。</p><p>问题6： kylin 不支持中文列名。kylin在创建中间表时，会使用中文+英文的方式做拼接，这个过程会报错。</p><p>问题7：kylin 2.2.0 版本用户相关比较难搞。默认账户 ADMIN有问题，暂无好的解决办法。<br>一种解决方式：升级到 2.3.0版本，修复了230个bug，对用户更加友好。</p><p>问题8：kylin 2.3以后采用spark 2.1.2版本，需要进行相关的配置。<br>解决方式：<a href="https://kylin.apache.org/docs23/tutorial/cube_spark.html" target="_blank" rel="noopener">https://kylin.apache.org/docs23/tutorial/cube_spark.html</a></p><h2 id="Kylin的一些问题"><a href="#Kylin的一些问题" class="headerlink" title="Kylin的一些问题"></a>Kylin的一些问题</h2><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p>关于Kylin的架构和原理，有图可供参考：<a href="http://blog.csdn.net/lvguichen88/article/details/53054745" target="_blank" rel="noopener">Kylin 的架构和原理</a></p><p>Kylin比较详细的介绍：<a href="http://tech.meiyou.com/?p=97" target="_blank" rel="noopener">Kylin对大数据量的多维分析</a></p><p>关于Kylin Cube构建原理，落地到HBase的过程: <a href="https://blog.bcmeng.com/post/kylin-cube.html" target="_blank" rel="noopener">Apache Kylin Cube 构建原理</a></p><p>关于Kylin SQL 语法：<a href="http://calcite.apache.org/docs/reference.html" target="_blank" rel="noopener">SQL language</a></p><p>关于Kylin的关键字：<br><a href="https://github.com/apache/kylin/blob/4d50b26972bb7bbaff852172990e0f189f987673/core-metadata/src/main/java/org/apache/kylin/source/adhocquery/HivePushDownConverter.java" target="_blank" rel="noopener">关键字源码</a></p><p>关于一次正常查询的运行原理：<a href="https://zhuanlan.zhihu.com/p/30613434" target="_blank" rel="noopener">Kylin进阶之路</a></p><p>Kylin使用calcite做sql解析，可以参考calcite的语法文档：<a href="https://calcite.apache.org/" target="_blank" rel="noopener">https://calcite.apache.org/</a></p><h3 id="维度问题"><a href="#维度问题" class="headerlink" title="维度问题"></a>维度问题</h3><p>关于维度的聚合组中各个含义，请参考<br><a href="https://kylin.apache.org/blog/2016/02/18/new-aggregation-group/" target="_blank" rel="noopener">https://kylin.apache.org/blog/2016/02/18/new-aggregation-group/</a></p><p>Kylin Mandatory Dimension(必要维度)：<a href="https://mp.weixin.qq.com/s?__biz=MzAwODE3ODU5MA==&amp;mid=2653077943&amp;idx=1&amp;sn=007d2ba345d0e25ec12807aa47f9913d&amp;chksm=80a4bf46b7d33650465d33e20dac7edc09a7ad9308d77de6a501685c8ae00cba661c1d612074&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">【技术帖】Apache Kylin高级设置： 必要维度 （Mandatory Dimension）原理解析</a></p><p>Kylin Hierarchy Dimension(层级维度)：<a href="https://mp.weixin.qq.com/s?__biz=MzAwODE3ODU5MA==&amp;mid=2653077929&amp;idx=1&amp;sn=c76ed1fbb745945a077d9ca99f159a4d&amp;chksm=80a4bf58b7d3364e0346ad9c433d4e32c57d45f41b361ae653c64c7fcebab21238793d2f66cb&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">【技术帖】Apache Kylin 高级设置：层级维度（Hierarchy Dimension）原理解析</a></p><p>Kylin Joint Dimension(联合维度)：<a href="https://mp.weixin.qq.com/s?__biz=MzAwODE3ODU5MA==&amp;mid=2653077926&amp;idx=1&amp;sn=a0037628bd102ec8e607d67204cbfa7c&amp;chksm=80a4bf57b7d336419896c9e801a51f08ead2f7727d0d0ec0f9e3b7799ae3c302ebea54f93cc0&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">【技术帖】Apache Kylin 高级设置：联合维度（Joint Dimension）原理解析</a></p><p>Kylin Aggregation Group(聚合组)：<a href="https://mp.weixin.qq.com/s?__biz=MzAwODE3ODU5MA==&amp;mid=2653077921&amp;idx=1&amp;sn=89ae88bc63e71098166b74df7106c7bf&amp;chksm=80a4bf50b7d3364692903aac3e901d09a516a8ff635e690e1e22b1d96abb4b2925c98cdace82&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">【技术帖】Apache Kylin 高级设置：聚合组（Aggregation Group）原理解析</a></p><h3 id="Server端"><a href="#Server端" class="headerlink" title="Server端"></a>Server端</h3><p>配置负载均衡：<a href="https://mp.weixin.qq.com/s?__biz=MzAwODE3ODU5MA==&amp;mid=2653077900&amp;idx=1&amp;sn=5bcadd8e5bf1f65c7226344b2f05320e&amp;chksm=80a4bf7db7d3366ba1901296110d51fccd0f41957003d90771db2c238161c41ee5ec555dc334&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">【技术贴】如何部署Apache Kylin集群实现负载均衡？</a></p><h3 id="进阶优化"><a href="#进阶优化" class="headerlink" title="进阶优化"></a>进阶优化</h3><p>Kylin cube算法：<a href="http://www.infoq.com/cn/articles/apache-kylin-algorithm" target="_blank" rel="noopener">Apache Kylin的快速数据立方体算法——概述</a></p><p>Kylin cube介绍：<a href="http://blog.csdn.net/yu616568/article/details/50570536" target="_blank" rel="noopener">Kylin使用之创建Cube和高级设置</a></p><p>别人的Cube优化案例：<a href="http://www.jianshu.com/p/1e82e5dddae2" target="_blank" rel="noopener">Apache Kylin cube优化指南</a></p><p>优化案例：<a href="https://zhuanlan.zhihu.com/p/29084008" target="_blank" rel="noopener">Apache Kylin 深入Cube和查询优化 提升Cube</a></p><h3 id="API开发"><a href="#API开发" class="headerlink" title="API开发"></a>API开发</h3><p>脚本触发增量更新：<a href="http://blog.csdn.net/aaronhadoop/article/details/52806486" target="_blank" rel="noopener">Kylin定时增量build</a></p><p>Kylin+superset 可视化方案案例：<a href="http://zhuanlan.zhihu.com/p/26628057" target="_blank" rel="noopener">Kylin初体验总结</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>很不错的博文：</p><ul><li><a href="http://www.jianshu.com/p/6eadb77d091c" target="_blank" rel="noopener">Apache Kylin 框架介绍</a></li><li><a href="http://www.infoq.com/cn/articles/kylin-apache-in-meituan-olap-scenarios-practice" target="_blank" rel="noopener">Apache Kylin在美团数十亿数据OLAP场景下的实践</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-Kylin介绍&quot;&gt;&lt;a href=&quot;#一-Kylin介绍&quot; class=&quot;headerlink&quot; title=&quot;一 Kylin介绍&quot;&gt;&lt;/a&gt;一 Kylin介绍&lt;/h2&gt;&lt;p&gt;kylin是一款OLAP工具，背靠Hadoop，HBase，Spark，Kafka等大山，提供神奇体验。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Kylin" scheme="http://guzhenping.com/tags/Kylin/"/>
    
  </entry>
  
  <entry>
    <title>Python学习（迭代器&amp;生成器）</title>
    <link href="http://guzhenping.com/2017/05/22/Python%E5%AD%A6%E4%B9%A0%EF%BC%88%E8%BF%AD%E4%BB%A3%E5%99%A8&amp;%E7%94%9F%E6%88%90%E5%99%A8%EF%BC%89/"/>
    <id>http://guzhenping.com/2017/05/22/Python学习（迭代器&amp;生成器）/</id>
    <published>2017-05-22T09:04:48.000Z</published>
    <updated>2018-11-30T02:42:52.947Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一直对Python的生成器、yield相关的东西比较晕。也终于到了这一天，算算总账，一口气理解这些东西。</p><p>这篇文章的背景是对yield/生成器/协程知识的总结，用于自我提升。请静下心来读，走马观花就不会有自己的思考。</p><p>懂分享的人，一定会快乐。笔者的内心期待着大家一起进步。所以，一起来分享自己的理解，留下你的评论吧。</p><a id="more"></a><h2 id="行文介绍"><a href="#行文介绍" class="headerlink" title="行文介绍"></a>行文介绍</h2><p>因为这是一整块知识，请不要拆散理解（好处是更易融会贯通）。所以，从头到尾的必备知识：</p><ul><li>可迭代对象（iterable）</li><li>迭代器（iterator）</li><li>迭代（iterator）</li><li>yield表达式</li><li>生成器（generators）</li></ul><p>本文会分别介绍上面的知识，最后介绍协程。剧透一下，协程和生成器很相似，所以学习协程，必备生成器相关的知识。</p><h2 id="啥是iterable？"><a href="#啥是iterable？" class="headerlink" title="啥是iterable？"></a>啥是iterable？</h2><p>Python中任意的对象，只要它定义了可以返回一个迭代器的<strong>iter</strong>方法，或者定义了可以支持下标索引的<strong>getitem</strong>方法，那么它就是一个可迭代对象。</p><p>简单说，可迭代对象就是能提供迭代器的任意对象。</p><h2 id="啥是iterator？"><a href="#啥是iterator？" class="headerlink" title="啥是iterator？"></a>啥是iterator？</h2><p>任意对象，只要定义了__iter__方法和next(Python2) 或者<strong>next</strong>（Python3）方法，它就是一个迭代器。__iter__()返回迭代器对象本身；next()或者__next__()返回容器的下一个元素，在结尾时引发StopIteration异常退出。</p><p>对于可迭代对象，可以使用内建函数iter()来获取它的迭代器对象。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># python3中</span><br><span class="line">&gt;&gt;&gt; test = [1,2,3] </span><br><span class="line">&gt;&gt;&gt; item = iter(test)    # 获取迭代器对象</span><br><span class="line">&gt;&gt;&gt; print(item)    # 打印该对象的类型、地址</span><br><span class="line">&lt;list_iterator object at 0x10231ab38&gt;</span><br><span class="line">&gt;&gt;&gt; item.__next__()    # 获取第一个元素</span><br><span class="line">1</span><br><span class="line">&gt;&gt;&gt; item.__next__()    # 获取第二个元素</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; item.__next__()    # 获取第三个元素</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; item.__next__()    # 获取StopIteration</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">StopIteration</span><br></pre></td></tr></table></figure><p>可以对string和dict做上面的事情，看看效果。扩展一点：在for循环中，for语法会自动调用迭代器的<strong>next</strong>()或next()，并能在遇到StopIteration时正常退出循环。</p><p>作为强化，给出一个自定义iterator的实例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">class MyIteratorFab():</span><br><span class="line">    &quot;&quot;&quot;Python3实现生成斐波那契数Fibonacci</span><br><span class="line">       输出最后一个数据不大于max</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, max):</span><br><span class="line">        self.max = max</span><br><span class="line">        self.a = 0</span><br><span class="line">        self.b = 1</span><br><span class="line"></span><br><span class="line">    def __iter__(self):</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def __next__(self):</span><br><span class="line">        &quot;&quot;&quot; 返回容器中下一个元素,没有(符合标准的)元素后, 抛出StopIteration</span><br><span class="line">            python2中请将__next__替换成next</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if self.b &lt;= self.max:</span><br><span class="line">            r = self.b    # 用于记录倒数第二个b的值,该值才是小于self.max的</span><br><span class="line">            self.a, self.b = self.b, self.a + self.b</span><br><span class="line">            return r    # 不可返回self.b, 该值会比self.max大一点点</span><br><span class="line">        else:</span><br><span class="line">            raise StopIteration()    # 这条代码很重要,删除这不能正常退出for循环</span><br><span class="line"></span><br><span class="line"># 测试</span><br><span class="line">test = MyIteratorFab(100)</span><br><span class="line">for item in test:</span><br><span class="line">    print(item)</span><br><span class="line">    </span><br><span class="line">#############################################</span><br><span class="line"># 附一段生成fibonacci数的函数，同上述自定义迭代器是</span><br><span class="line"># 同一功能.</span><br><span class="line">#############################################</span><br><span class="line">def fib_func2(max):</span><br><span class="line">    &quot;&quot;&quot;一个普通的的生产fibonacci数的函数</span><br><span class="line">       输出最后一个数据不大于max</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    a, b = 0, 1</span><br><span class="line">    while b &lt;= max:</span><br><span class="line">        print(b)</span><br><span class="line">        a, b = b, a+b</span><br></pre></td></tr></table></figure><p>最后，在<a href="http://python.jobbole.com/81881/" target="_blank" rel="noopener">《Python迭代器和生成器》</a>一文中提到过：对于一个可迭代对象，如果它本身又是一个迭代器对象，那么没办法支持多次迭代。感兴趣可戳过去阅读。</p><p>文中给出该问题的解法是，对迭代器对象类再包一个可迭代对象，实现多次迭代。拿上述MyIteratorFab()自定义迭代器举例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 问题：</span><br><span class="line">fab = MyIteratorFab(10)</span><br><span class="line">print([item for item in fab])    # output: [1, 1, 2, 3, 5, 8]</span><br><span class="line">print([item for item in fab])    # output: []</span><br><span class="line"></span><br><span class="line"># 解决方法：</span><br><span class="line">class BetterFab():</span><br><span class="line">    &quot;&quot;&quot;实现生成斐波那契数Fibonacci</span><br><span class="line">       输出最后一个数据不大于max</span><br><span class="line">       可多次迭代</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, max):</span><br><span class="line">        self.max = max</span><br><span class="line"></span><br><span class="line">    def __iter__(self):</span><br><span class="line">        return MyIteratorFab(self.max)</span><br><span class="line">        </span><br><span class="line"># 测试</span><br><span class="line">fab2 = BetterFab(10)</span><br><span class="line">print([item for item in fab2])    # output: [1, 1, 2, 3, 5, 8]</span><br><span class="line">print([item for item in fab2])    # output: [1, 1, 2, 3, 5, 8]</span><br><span class="line"></span><br><span class="line"># 暴力的解法，但不推荐：</span><br><span class="line">print([item for item in MyIteratorFab(10)])    # output: [1, 1, 2, 3, 5, 8]</span><br><span class="line">print([item for item in MyIteratorFab(10)])    # output: [1, 1, 2, 3, 5, 8]</span><br></pre></td></tr></table></figure><p>到此，iterator就算是说完了。这是三个以itera*开头的概念中最核心的一个概念。当然，它也是生成器（generator）的基础。</p><h2 id="啥是iteration？"><a href="#啥是iteration？" class="headerlink" title="啥是iteration？"></a>啥是iteration？</h2><p>提醒一下，这是一个名词。用简单的话讲，它就是从某个地方（比如一个列表）取出一个元素的过程。当我们使用一个循环来遍历某个东西时，这个过程本身就叫迭代。</p><h2 id="如何理解yield表达式？"><a href="#如何理解yield表达式？" class="headerlink" title="如何理解yield表达式？"></a>如何理解yield表达式？</h2><p>下面的内容摘自Python3官方文档翻译：</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; yield_atom ::=  &quot;(&quot; yield_expression &quot;)&quot;</span><br><span class="line">yield_expression ::=  &quot;yield&quot; [expression_list | &quot;from&quot; expression]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>yield表达式仅在定义生成器函数时使用，因此只能用在函数定义的主体中。在函数体中使用yield表达式会使该函数成为生成器。</p></blockquote><blockquote><p>当生成器函数被调用时，它返回一个称为生成器的迭代器。然后，生成器控制生成器函数的执行。当生成器的一个方法被调用时，执行开始。此时，执行进行到第一个yield表达式，在那里它被再次挂起，将expression_list的值返回给生成器的调用者。挂起，我们的意思是保留所有局部状态，包括局部变量的当前绑定，指令指针，内部计算栈和任何异常处理的状态。当通过调用其中一个生成器的方法来恢复执行时，函数可以像yield表达式只是另一个外部调用一样继续进行。恢复后的yield表达式的值取决于恢复执行的方法。如果使用__next__()（通常通过for或next()内置函数），则结果为None。否则，如果使用send()，则结果将是传递到该方法的值。</p><p>摘自：<a href="http://python.usyiyi.cn/translate/python_352/reference/expressions.html#yieldexpr" target="_blank" rel="noopener">http://python.usyiyi.cn/translate/python_352/reference/expressions.html#yieldexpr</a></p></blockquote><p>yield 是一个类似 return 的关键字，只是这个函数返回的是个生成器。</p><p>另外，官网也提到yield其实和Coroutine类似：</p><blockquote><p>所有这些使生成器函数与协程非常相似；它们产生多次，它们具有多个入口点并且它们的执行可以被挂起。唯一的区别是生成器函数不能控制在它yield后继续执行的位置；控制总是转移到生成器的调用者。</p></blockquote><h2 id="啥是generators？"><a href="#啥是generators？" class="headerlink" title="啥是generators？"></a>啥是generators？</h2><p>下面的内容摘自Python3官方文档翻译：</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; generator_expression ::=  &quot;(&quot; expression comp_for &quot;)&quot;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>生成器表达式产生一个新的生成器对象。它的语法与推导式的语法相同，除了它被括在括号中而不是括号或花括号中。</p><p>当生成器对象调用__next__()方法时，生成器表达式中使用的变量将被懒惰地计算（以与正常生成器相同的方式）。但是，最左边的for子句会立即被求值，所以它产生的错误可以在生成器表达式代码中的任何其它可能的错误之前发现。后续for子句无法立即计算，因为它们可能取决于之前的for循环。例如：(x*y for x in range(10) for y in bar(x))。</p><p>摘自：<a href="http://python.usyiyi.cn/translate/python_352/reference/expressions.html#yieldexpr" target="_blank" rel="noopener">http://python.usyiyi.cn/translate/python_352/reference/expressions.html#yieldexpr</a></p></blockquote><p>生成器也是一种迭代器，但是你只能对其迭代一次。这是因为它们并没有把所有的值存在内存中，而是在运行时生成值。你通过遍历来使用它们，要么用一个“for”循环，要么将它们传递给任意可以进行迭代的函数和结构。大多数时候生成器是以函数来实现的。然而，它们并不返回一个值，而是yield(暂且译作“生出”)一个值。</p><p>生成器最佳应用场景是：你不想同一时间将所有计算出来的大量结果集分配到内存当中，特别是结果集里还包含循环。</p><h2 id="协程（Coroutine）"><a href="#协程（Coroutine）" class="headerlink" title="协程（Coroutine）"></a>协程（Coroutine）</h2><p>常见于合作式多任务，迭代器，无限列表，管道。</p><p>协程是基于单个线程的，Python的线程是基于单核的并发实现。</p><blockquote><p>并发对应计算机中充分利用单核（一个CPU）实现（看起来）多个任务同时执行。实现并发编程可以用多进程、多线程、异步、协程。</p></blockquote><p>从上面这段引用可以看出：为啥很多人讨论协程与其他并发编程方式的异同优劣？但是今天我们不聊并发，所以关注该话题的童鞋请自己Google文章。</p><p>深挖Coroutine的本质:</p><blockquote><p>allowing multiple entry points for suspending and resuming execution at certain locations.</p></blockquote><p>允许多个入口对程序进行挂起、继续执行等操作。</p><p>这和生成器很相似，但有区别：</p><ul><li>生成器是数据的生产者</li><li>协程则是数据的消费者</li></ul><p>协程会消费掉发送给它的值。</p><p>协程常用的方法：<br>next()<br>send()<br>close()</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>有件事情必须要说明白：</p><p>我是宇宙中微不足道的一粒沙。即使作为沙粒，我想浸在湛蓝的海水中，也想沐浴在灿烂的阳光下，更想陪着孩童们构建沙滩城堡。自然清楚自己是一粒沙，但还是想要这样的生活。</p><p>所以，我愿意写这些微不足道的东西，来分享自己的价值。尽管，我也微不足道。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://blog.rainy.im/2016/04/07/python-thread-and-coroutine/" target="_blank" rel="noopener">Python 线程与协程</a></li><li><a href="http://pyzh.readthedocs.io/en/latest/the-python-yield-keyword-explained.html" target="_blank" rel="noopener">Python关键字yield的解释</a></li><li><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/" target="_blank" rel="noopener">Python yield 使用浅析</a></li><li><a href="http://www.cnblogs.com/rio2607/p/4440122.html" target="_blank" rel="noopener">Python高级编程之生成器(Generator)与coroutine(一):Generator</a></li><li><a href="http://www.cnblogs.com/rio2607/p/4456332.html" target="_blank" rel="noopener">Python高级编程之生成器(Generator)与coroutine(二):coroutine介绍</a></li><li><a href="http://www.cnblogs.com/rio2607/p/4472456.html" target="_blank" rel="noopener">Python高级编程之生成器(Generator)与coroutine(三):coroutine与pipeline(管道)和Dataflow(数据流_</a></li><li><a href="http://www.cnblogs.com/rio2607/p/4570353.html" target="_blank" rel="noopener">Python高级编程之生成器(Generator)与coroutine(四):一个简单的多任务系统</a></li><li><a href="http://python.jobbole.com/81881/" target="_blank" rel="noopener">Python迭代器和生成器</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;一直对Python的生成器、yield相关的东西比较晕。也终于到了这一天，算算总账，一口气理解这些东西。&lt;/p&gt;
&lt;p&gt;这篇文章的背景是对yield/生成器/协程知识的总结，用于自我提升。请静下心来读，走马观花就不会有自己的思考。&lt;/p&gt;
&lt;p&gt;懂分享的人，一定会快乐。笔者的内心期待着大家一起进步。所以，一起来分享自己的理解，留下你的评论吧。&lt;/p&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://guzhenping.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="Python" scheme="http://guzhenping.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>HA成功升级的总结</title>
    <link href="http://guzhenping.com/2017/02/02/HA%E6%88%90%E5%8A%9F%E5%8D%87%E7%BA%A7%E7%9A%84%E6%80%BB%E7%BB%93/"/>
    <id>http://guzhenping.com/2017/02/02/HA成功升级的总结/</id>
    <published>2017-02-02T09:04:48.000Z</published>
    <updated>2018-11-30T02:38:07.484Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>从升级到现在，一共过了1个半月，到昨天（20170511）总算踏实了。踩的那些坑，真的教会了笔者咋做人。升了，笔者并不后悔。当然脸被打了这么多下，也高兴不起来。</p><p>升级过程的所有问题，发生在当事人无法注意到、无法理解的地方。然而，只要时间充足，精力充沛，严谨严格，肯定能克服。</p><p>必须申明：本文是基于个体实际经验所写，难免片面，可能不具备参考价值。其次，本文是对一个已经存在数年有大数量的集群所做的一些升级，情况较为特殊。<br><a id="more"></a></p><h2 id="升级准备"><a href="#升级准备" class="headerlink" title="升级准备"></a>升级准备</h2><p>这里就不多说了，参见<a href="https://www.tapd.cn/20096511/markdown_wikis/#1120096511001000954" target="_blank" rel="noopener">《Hadoop Namenode HA升级》</a>。</p><p>总结一句，准备和成功概率成正比，有付出，才有收获。</p><h2 id="问题1：异构的配置文件"><a href="#问题1：异构的配置文件" class="headerlink" title="问题1：异构的配置文件"></a>问题1：异构的配置文件</h2><p>对于hadoop集群来说，很多人都觉得hadoop配置文件是一样的。错了，真的错了。hadoop集群支持异构模式。所以，配置可以不同。</p><p>在本次升级过程中，配置上，遇到的问题：个别机器的盘符不同于大部分机器，个别机器的datanode数据所在盘没有填写在<code>&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</code>下。</p><p>对于linux盘符不太熟悉的观众请戳：<a href="http://ilinuxkernel.com/?p=958" target="_blank" rel="noopener">《Linux硬盘盘符分配》</a></p><p>例如，有以下两种（及以上）配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">       &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;/home/deploy/hadoopdata/datanode/,/mnt/sdb/hadoopdata/datanode/&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">       &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;/home/deploy/hadoopdata/datanode/,/mnt/sdc/hadoopdata/datanode/,/mnt/sdd/hadoopdata/datanode/&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>这种错误的配置，会导致namenode在启动过程中无法找到block，处于安全模式无法退出。对于集群停机不能超过5小时的公司/团体来说，只能强制立刻安全模式。此时，将会产生坏块。</p><p>对于hadoop集群来说，必须清除坏块。换句话说，因为配置的失误，会导致datanode丢数据。这种丢法，和没有挂上的那个/些盘有关，而且一丢就是丢一个盘，后果严重。</p><p>当然hadoop的块备份是大于等于2，如果只是一个盘，对于集群来说就相当于没有丢。反之，则是随机丢失块数据。</p><h2 id="问题2：Hadoop堆内存"><a href="#问题2：Hadoop堆内存" class="headerlink" title="问题2：Hadoop堆内存"></a>问题2：Hadoop堆内存</h2><p>先说结论：对于HA来说，两个namenode（active/standby）的堆内存应该要比hadoop集群metadata大至少1倍。这个结论并没有理论根据，但确实实践所得的最可靠数据。假设一个运行5年的集群有20G metadata数据，则namenode(a/s)需要40G以上。</p><p>为啥这么说？因为，namenode主备自动切换时(即主namenode异常，备namenode启动)，standby namenode需要将metadata读到进程堆内存中。堆内存不住，NameNode进程会报GC堆已满。GC堆相关问题请戳：<a href="http://www.cnblogs.com/dingyingsi/p/3760447.html" target="_blank" rel="noopener">《深入理解JVM—JVM内存模型》</a></p><p>理解主备热切，大致有两个要点，本文不多说。只点一下，第一，单节点Namenode启动过程发生了哪些事情；第二，HA集群namenode启动中发生了哪些事情。主要就是围绕FSImage和EditsLog展开。请戳下方链接：</p><ul><li><a href="http://blog.csdn.net/cnhk1225/article/details/50786785" target="_blank" rel="noopener">《NameNode启动过程详细剖析 NameNode中几个关键的数据结构 FSImage》</a></li><li><a href="http://blog.csdn.net/dabokele/article/details/51686257" target="_blank" rel="noopener">《Hadoop-2.X HA模式下的FSImage和EditsLog合并过程》</a></li></ul><p>对于拥有20G左右metadata的集群，standby需要10-20分钟内就可以启动(堆内存够大)。如果热切时，standby namenode的内存就16G,那么半个小时差不多只能走到25%左右。</p><p>堆内存大点很易理解，数据多嘛。而且，网上有很多hadoop在提交作业时报jvm堆内存不足的问题，可以参考。</p><p>笔者的小伙伴在解决standby namenode启动过慢这个问题时，通过修改安装目录的<code>../etc/hadoop/hadoop-env.sh</code>文件中的HADOOP_HEAPSIZE这个参数。一开始,该参数同集群metadata数据大小无异，第二次改成约1.5倍，第三次改成约2倍。</p><p>HADOOP_HEAPSIZE的值会成为本地（不是每台机器共用的，可以不同）的JVM的堆大小。本地的各个Java守护进程都会共享这个堆，此时进程NameNoe能满足快速启动的条件。虽然会有其他java进程也用，但是HA模式下的namenode没有过多的java进程（其实就是DFSZKFailoverController）。</p><h2 id="问题3：NameNode-RPC调用方式"><a href="#问题3：NameNode-RPC调用方式" class="headerlink" title="问题3：NameNode RPC调用方式"></a>问题3：NameNode RPC调用方式</h2><p>未升级前，连接hadoop集群通过<code>&lt;name&gt;fs.defaultFS&lt;/name&gt;</code>下的hdfs://IP：PORT来连。升级后，需要采用命名空间的方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://mainhadoop&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>即：hdfs://命名空间。连接上命名空间后，zookeeper集群会自动分配程序连接到当前active的namenode。</p><p>对此，受影响较大的是一些执行脚本和已存在的hive表。执行脚本一般写死，只能一个一个修改。</p><p>对于hive表，需要进入到元数据所在数据库，修改数据Location指向。如果hive是以mysql作元数据存储，则需连上mysql，修改SDS和DBS两张表的数据。将“hdfs://ip:port/XXXXXXXXXX”改成新的hadoop命名空间。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;从升级到现在，一共过了1个半月，到昨天（20170511）总算踏实了。踩的那些坑，真的教会了笔者咋做人。升了，笔者并不后悔。当然脸被打了这么多下，也高兴不起来。&lt;/p&gt;
&lt;p&gt;升级过程的所有问题，发生在当事人无法注意到、无法理解的地方。然而，只要时间充足，精力充沛，严谨严格，肯定能克服。&lt;/p&gt;
&lt;p&gt;必须申明：本文是基于个体实际经验所写，难免片面，可能不具备参考价值。其次，本文是对一个已经存在数年有大数量的集群所做的一些升级，情况较为特殊。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="集群运维" scheme="http://guzhenping.com/tags/%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/"/>
    
      <category term="Hadoop" scheme="http://guzhenping.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 2.7.2 HA升级方案</title>
    <link href="http://guzhenping.com/2017/01/02/HA%E5%8D%87%E7%BA%A7%E8%BF%87%E7%A8%8B/"/>
    <id>http://guzhenping.com/2017/01/02/HA升级过程/</id>
    <published>2017-01-02T09:04:48.000Z</published>
    <updated>2018-11-30T02:39:17.615Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本次升级主要对Hadoop的core-site.xml和hdfs-site.xml文件进行修改，暂时不涉及其他配置。</p><p>这次升级过程，大致是三步：备份数据文件，修改配置文件，启动集群。如果升级中发现异常，启动回滚方案。</p><h2 id="备份数据"><a href="#备份数据" class="headerlink" title="备份数据"></a>备份数据</h2><h3 id="备份配置文件"><a href="#备份配置文件" class="headerlink" title="备份配置文件"></a>备份配置文件</h3><p>需要执行fabric脚本，在每台机器上进行备份。将core-site.xml和hdfs-site.xml分别cp为core-site.xml.ha.back和hdfs-site.xml.ha.back</p><p>以下是本次要修改的core-site.xml和hdfs-site.xml配置文件原内容。</p><ul><li>core-site.xml</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;hdfs://sha2hdpn01:9000&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;io.file.buffer.size&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;131072&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/home/deploy/hadoopdata/tmp&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hadoop.proxyuser.deploy.groups&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hadoop.proxyuser.deploy.hosts&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">         &lt;property&gt;</span><br><span class="line">                &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;10080&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">               &lt;name&gt;topology.script.file.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/topology.sh&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;net.topology.script.file.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/topology.sh&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li>hdfs-site.xml</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">           &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;sha2hdpn02:50090&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">           &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;sha2hdpn01:50070&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">           &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;/home/deploy/hadoopdata/namenode&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">           &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;/home/deploy/hadoopdata/datanode/,/mnt/sdb/hadoopdata/datanode/&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">           &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/deploy/hadoopdata/checkpoint&lt;/value&gt;</span><br><span class="line">        &lt;final&gt;true&lt;/final&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/deploy/hadoopdata/checkpoint&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;600&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;107374182400&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.fsdataset.volume.choosing.policy&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;10485760&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/exclude&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;52428800&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.max.transfer.threads&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;8192&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;</span><br><span class="line">            Specifies the maximum number of threads to use for transferring data</span><br><span class="line">            in and out of the DN. default is 4096,###Modified###</span><br><span class="line">        &lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">         &lt;name&gt;dfs.datanode.socket.write.timeout&lt;/name&gt;</span><br><span class="line">         &lt;value&gt;480000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">         &lt;name&gt;dfs.client.socket-timeout&lt;/name&gt;</span><br><span class="line">         &lt;value&gt;300000&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">         &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;</span><br><span class="line">         &lt;value&gt;8192&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">         &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;</span><br><span class="line">         &lt;value&gt;80&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="备份namenode数据"><a href="#备份namenode数据" class="headerlink" title="备份namenode数据"></a>备份namenode数据</h3><p>需要关闭集群后，在sha2hdpn01上备份namenode的元数据,位置：<code>/home/deploy/hadoopdata</code>,大小：25G。</p><h3 id="备份secondary-namenode的数据"><a href="#备份secondary-namenode的数据" class="headerlink" title="备份secondary namenode的数据"></a>备份secondary namenode的数据</h3><p>需要关闭集群后，在sha2hdpn02上备份snn的元数据，位置：<code>/home/deploy/hadoopdata</code>, 大小：774G。</p><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><h3 id="修改core-stie-xml"><a href="#修改core-stie-xml" class="headerlink" title="修改core-stie.xml"></a>修改core-stie.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;hdfs://hacluster&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;io.file.buffer.size&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;131072&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/home/deploy/hadoopdata/tmp&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;sha2hb01:2181,sha2hb02:2181,sha2hb03:2181&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hadoop.proxyuser.deploy.groups&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hadoop.proxyuser.deploy.hosts&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">         &lt;property&gt;</span><br><span class="line">                &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;10080&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">               &lt;name&gt;topology.script.file.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/topology.sh&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;net.topology.script.file.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/topology.sh&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="修改hdfs-site-xml"><a href="#修改hdfs-site-xml" class="headerlink" title="修改hdfs-site.xml"></a>修改hdfs-site.xml</h3><p>先删除secondary namenode配置，在添加和ha配置相关的内容。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">           &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;/home/deploy/hadoopdata/namenode&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">           &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;/home/deploy/hadoopdata/datanode/,/mnt/sdb/hadoopdata/datanode/&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">           &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hacluster&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.namenodes.hacluster&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.hacluster.nn1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;sha2hdpn01:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.hacluster.nn2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;sha2hdpn02:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">     &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.hacluster.nn1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;sha2hdpn01:50070&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.hacluster.nn2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;sha2hdpn02:50070&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;qjournal://sha2hdpw46:8485; sha2hdpw47:8485;sha2hdpw48:8485/hacluster&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/deploy/hadoopdata/journaldata&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.client.failover.proxy.provider.hacluster&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;</span><br><span class="line">            sshfence</span><br><span class="line">            shell(/bin/true)</span><br><span class="line">      &lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;/home/deploy/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;30000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;107374182400&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.fsdataset.volume.choosing.policy&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;10485760&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/exclude&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;52428800&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.max.transfer.threads&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;8192&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;</span><br><span class="line">            Specifies the maximum number of threads to use for transferring data</span><br><span class="line">            in and out of the DN. default is 4096,###Modified###</span><br><span class="line">        &lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">         &lt;name&gt;dfs.datanode.socket.write.timeout&lt;/name&gt;</span><br><span class="line">         &lt;value&gt;480000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">         &lt;name&gt;dfs.client.socket-timeout&lt;/name&gt;</span><br><span class="line">         &lt;value&gt;300000&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">         &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;</span><br><span class="line">         &lt;value&gt;8192&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">         &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;</span><br><span class="line">         &lt;value&gt;80&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h2 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h2><p>先启动zookeeper集群，并确定状态；</p><p>再启动journalnode集群，用于nn间数据同步（注意文件夹存储的位置和权限）；</p><p>在主节点上启动namenode；</p><p>在副节点上，同步namenode的元数据：<code>hdfs namenode -bootstandby</code>（metadata为7.3G, 注意磁盘空间及时长）；</p><p>在副节点上启动namenode;</p><p>在任意节点上，启动dfs: <code>sh start-dfs.sh</code>;</p><p>启动其他的。</p><h2 id="回滚方案"><a href="#回滚方案" class="headerlink" title="回滚方案"></a>回滚方案</h2><p>此处升级，主要是对core-site.xml和hdfs-site.xml文件进行修改，所以回滚方案的主要逻辑就是恢复这两份文件。</p><p>利用fabric写脚本，先每台机器上备份（cp命令），如需回滚，先关闭集群，再把该文件替换回来（mv命令）即可。</p><p>附脚本rollback_hadoop.py：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fabric.api <span class="keyword">import</span> run,sudo,roles,env,cd,execute</span><br><span class="line"></span><br><span class="line">env.roledefs = &#123;</span><br><span class="line">    <span class="string">'all_node'</span>: [<span class="string">'sha2hdpn01'</span>,<span class="string">'sha2hdpn02'</span>,<span class="string">'sha2hdpw01'</span>,<span class="string">'sha2hdpw02'</span>,<span class="string">'sha2hdpw03'</span>,<span class="string">'sha2hdpw04'</span>,<span class="string">'sha2hdpw05'</span>,<span class="string">'sha2hdpw06'</span>,<span class="string">'sha2hdpw07'</span>,<span class="string">'sha2hdpw08'</span>,<span class="string">'sha2hdpw09'</span>,<span class="string">'sha2hdpw10'</span>,<span class="string">'sha2hdpw11'</span>,<span class="string">'sha2hdpw12'</span>,<span class="string">'sha2hdpw13'</span>,<span class="string">'sha2hdpw14'</span>,<span class="string">'sha2hdpw15'</span>,<span class="string">'sha2hdpw16'</span>,<span class="string">'sha2hdpw17'</span>,<span class="string">'sha2hdpw18'</span>,<span class="string">'sha2hdpw19'</span>,<span class="string">'sha2hdpw20'</span>,<span class="string">'sha2hdpw21'</span>,<span class="string">'sha2hdpw22'</span>,<span class="string">'sha2hdpw23'</span>,<span class="string">'sha2hdpw24'</span>,<span class="string">'sha2hdpw25'</span>,<span class="string">'sha2hdpw26'</span>,<span class="string">'sha2hdpw27'</span>,<span class="string">'sha2hdpw28'</span>,<span class="string">'sha2hdpw29'</span>,<span class="string">'sha2hdpw30'</span>,<span class="string">'sha2hdpw31'</span>,<span class="string">'sha2hdpw32'</span>,<span class="string">'sha2hdpw33'</span>,<span class="string">'sha2hdpw34'</span>,<span class="string">'sha2hdpw35'</span>,<span class="string">'sha2hdpw36'</span>,<span class="string">'sha2hdpw37'</span>,<span class="string">'sha2hdpw38'</span>,<span class="string">'sha2hdpw39'</span>,<span class="string">'sha2hdpw40'</span>,<span class="string">'sha2hdpw41'</span>,<span class="string">'sha2hdpw42'</span>,<span class="string">'sha2hdpw43'</span>,<span class="string">'sha2hdpw44'</span>,<span class="string">'sha2hdpw45'</span>,<span class="string">'sha2hdpw46'</span>,<span class="string">'sha2hdpw47'</span>,<span class="string">'sha2hdpw48'</span>],</span><br><span class="line">    <span class="string">'namenode'</span>: [<span class="string">'sha2hdpn01'</span>],</span><br><span class="line">    <span class="string">'test_node'</span>: [<span class="string">'sha2hdpw48'</span>]</span><br><span class="line">&#125;</span><br><span class="line">env.user = <span class="string">'deploy'</span></span><br><span class="line">env.password = <span class="string">'XXXXXX'</span> <span class="comment"># yourself</span></span><br><span class="line"><span class="comment"># env.shell = '/bin/sh -c'</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@roles('all_node')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showfile</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># run('ll /usr/local/hadoop-default/etc/hadoop')i</span></span><br><span class="line">    <span class="keyword">with</span> cd(<span class="string">'/usr/local/hadoop-default/etc/hadoop'</span>):</span><br><span class="line">        run(<span class="string">"ls core-site.xml hdfs-site.xml"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@roles('namenode')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_hadoop</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'start hadoop cluster...'</span>)</span><br><span class="line">    <span class="comment"># run('sh /usr/local/hadoop-default/sbin/start-all.sh')</span></span><br><span class="line">    print(<span class="string">'done...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@roles('all_node')</span></span><br><span class="line"><span class="comment">#@roles('test_node')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backup</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'start backup...'</span>)</span><br><span class="line">    <span class="keyword">with</span> cd(<span class="string">'/usr/local/hadoop-default/etc/hadoop'</span>):</span><br><span class="line">        run(<span class="string">'cp core-site.xml core-site.xml.ha.back'</span>)</span><br><span class="line">        run(<span class="string">'cp hdfs-site.xml hdfs-site.xml.ha.back'</span>)</span><br><span class="line">    print(<span class="string">'done...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@roles('all_node')</span></span><br><span class="line"><span class="comment">#@roles('test_node')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rollback</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'start roolback...'</span>)</span><br><span class="line">    <span class="keyword">with</span> cd(<span class="string">'/usr/local/hadoop-default/etc/hadoop'</span>):</span><br><span class="line">        run(<span class="string">'mv core-site.xml.ha.back core-site.xml'</span>)</span><br><span class="line">        run(<span class="string">'mv hdfs-site.xml.ha.back hdfs-site.xml'</span>)</span><br><span class="line">    print(<span class="string">'done...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deploy</span><span class="params">()</span>:</span></span><br><span class="line">    execute(showfile)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_backup</span><span class="params">()</span>:</span></span><br><span class="line">    execute(backup)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_rollback</span><span class="params">()</span>:</span></span><br><span class="line">    execute(rollback)</span><br><span class="line">    execute(start_hadoop)</span><br></pre></td></tr></table></figure><h2 id="测试方案"><a href="#测试方案" class="headerlink" title="测试方案"></a>测试方案</h2><p>重跑airflow中金融/流量的报表任务，用于提交job，查看是否能够完整跑完。这些任务中有使用了hdfs及yarn的操作，如果成功，说明hadoop ha集群可以用。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>烧香拜佛，希望成功！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本次升级主要对Hadoop的core-site.xml和hdfs-site.xml文件进行修改，暂时不涉及其他配置。&lt;/p&gt;
&lt;p&gt;这次升级
      
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="集群运维" scheme="http://guzhenping.com/tags/%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/"/>
    
      <category term="Hadoop" scheme="http://guzhenping.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习指南(HA配置)</title>
    <link href="http://guzhenping.com/2017/01/02/Hadoop%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%88HA%E9%85%8D%E7%BD%AE%EF%BC%89/"/>
    <id>http://guzhenping.com/2017/01/02/Hadoop学习指南（HA配置）/</id>
    <published>2017-01-02T09:04:48.000Z</published>
    <updated>2018-11-30T02:50:33.953Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇介绍Hadoop的一些常用知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。</p><p>HA模式，主要是将namenode及resourcemanager都变成主备两个。这里先不讨论resourcemanager，主要针对namenode。</p><p>将namenode变成可主备自动切换的，主要是通过zookeeper集群对namenode的健康状态进行监控，然后选举一个健康的namenode做active(主)的，另一个成为standby(备)。因此，保证zookeeper集群的配置是正确且不易挂掉，是HA的基石。同时，注意HA升级过程中的相关进程的启动步骤即可完成。</p><a id="more"></a><p>官网有两种配置：NFS和QJM，两者区别参见:<a href="http://joshuasabrina.iteye.com/blog/1858448" target="_blank" rel="noopener">《HDFS v2 HA方案对比》</a></p><p>本文以NFS为例进行讨论。</p><p>首先声明环境：</p><ul><li>CentOS 6.5 64位</li><li>Hadoop 2.7.1</li><li>Java 1.8</li></ul><p>所有操作均基于拥有第一代hadoop集群的环境之上。同时满足Centos 6.5系统内网络互通、机器免密码登陆、防火墙关闭。重要的是/etc/hosts文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">192.168.20.2    hadoop1</span><br><span class="line">192.168.20.3    hadoop2</span><br><span class="line">192.168.20.4    hadoop3</span><br><span class="line">192.168.20.5    hadoop4</span><br><span class="line">192.168.20.6    hadoop5</span><br></pre></td></tr></table></figure><p>另外，温馨提示：在hadoop的各种文件配置中，最好不要出现空格。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop2:2181,  hadoop3:2181,  hadoop4:2181&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>hadoop将无法找到<code>hadoop3</code>和<code>hadoop4</code>这种IP。如果手贱多敲，那么将会浪费很多时间。</p><h2 id="进程功能介绍"><a href="#进程功能介绍" class="headerlink" title="进程功能介绍"></a>进程功能介绍</h2><p>zk:维护共享锁保证只有一个active的namenode</p><p>journalnode：在两个nn间同步元数据</p><h2 id="配置zookeeper集群"><a href="#配置zookeeper集群" class="headerlink" title="配置zookeeper集群"></a>配置zookeeper集群</h2><p>该集群最少需要3台机器（用于选举）。下面详述配置过程。</p><p>假设1： 3台机器是这样：</p><table><thead><tr><th>IP</th><th>标识</th></tr></thead><tbody><tr><td>192.168.20.3</td><td>hadoop2</td></tr><tr><td>192.168.20.4</td><td>hadoop3</td></tr><tr><td>192.168.20.5</td><td>hadoop4</td></tr></tbody></table><p>假设2： 下载完的zookeeper源码包位于：/home/deploy/zookeeper-3.4.9。</p><p>假设3：java环境为1.8版本，位于：/home/deploy/jdk1.8.0_111。</p><p>第一步，配置zoo.cfg:<br><code>vi /home/deploy/zookeeper-3.4.9/conf/zoo.cfg</code></p><p>修改配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># The number of milliseconds of each tick</span><br><span class="line">tickTime=2000</span><br><span class="line"></span><br><span class="line"># The number of ticks that the initial</span><br><span class="line"># synchronization phase can take</span><br><span class="line">initLimit=10</span><br><span class="line"></span><br><span class="line"># The number of ticks that can pass between</span><br><span class="line"># sending a request and getting an acknowledgement</span><br><span class="line">syncLimit=5</span><br><span class="line"></span><br><span class="line"># the directory where the snapshot is stored.</span><br><span class="line"># do not use /tmp for storage, /tmp here is just</span><br><span class="line"># example sakes.</span><br><span class="line">dataDir=/home/deploy/zookeeper-3.4.9/data</span><br><span class="line"></span><br><span class="line"># logs dir</span><br><span class="line">dataLogDir=/home/deploy/zookeeper-3.4.9/logs</span><br><span class="line"></span><br><span class="line"># the port at which the clients will connect</span><br><span class="line">clientPort=2181</span><br><span class="line"></span><br><span class="line">server.1=hadoop3:2888:3888</span><br><span class="line">server.2=hadoop4:2888:3888</span><br><span class="line">server.3=hadoop2:2888:3888</span><br></pre></td></tr></table></figure><p>第二步，配置myid文件</p><p>在zoo.cfg中dataDir路径：dataDir=/home/deploy/zookeeper-3.4.9/data下，新建myid文件。<code>vi myid</code><br>在hadoop3的机器上，该文件内容为1，<br>hadoop4机器上，该文件内容为2，<br>hadoop5机器上，该文件内容为3。内容应保持同zoo.cfg中的server.x的x值相同</p><p>第三步，开/关zookeeper集群：</p><p>开：<code>sh /home/deploy/zookeeper-3.4.9/bin/zkServer.sh start</code></p><p>关：<code>sh /home/deploy/zookeeper-3.4.9/bin/zkServer.sh stop</code></p><p>请在将三台机器全部开启后，查看状态：<code>zkServer.sh status</code>。</p><h2 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h2><p>在原文件上进行添加:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 指定hdfs的nameservice为h01，需与dfs.nameservices一致 --&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;hdfs://mycluster&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定zookeeper地址 --&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop2:2181,hadoop3:2181,hadoop4:2181&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="配置hdfs-site-xml"><a href="#配置hdfs-site-xml" class="headerlink" title="配置hdfs-site.xml"></a>配置hdfs-site.xml</h2><p>这里有两步，第一步是删除关于secondary namenode的配置，第二步是添加HA的配置。</p><p>删：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 以下3个 property 的配置，是非HA模式下的，即一个集群只有一个namenode，在这里不可使用 --&gt;  </span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.http.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;h01.vm.com:50070&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Secondary get fsimage and edits via dfs.http.address&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.secondary.http.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;h02.vm.com:50090&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/home/vagrant/VMBigData/hadoop/data/namesecondary&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>添：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 命名空间的逻辑名称 --&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mycluster&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 命名空间中所有NameNode的唯一标示。该标识指示DataNode集群中有哪些NameNode --&gt;  </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop1:9091&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop2:9091&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop1:9092&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop2:9092&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- JournalNode URLs，ActiveNameNode 会将 Edit Log 写入这些 JournalNode 所配置的本地目录即 dfs.journalnode.edits.dir --&gt;  </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;qjournal://hadoop2:8485;hadoop3:8485;hadoop4:8485/mycluster&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- JournalNode 用于存放 editlog 和其他状态信息的目录 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/deploy/hadoop-2.7.1/journaldata&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; </span><br><span class="line">&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 一种关于 NameNode 的隔离机制(fencing) --&gt;  </span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">&lt;value&gt;</span><br><span class="line">sshfence</span><br><span class="line">shell(/bin/true)</span><br><span class="line">&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/home/deploy/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;</span><br><span class="line">&lt;value&gt;30000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="修改slaves文件"><a href="#修改slaves文件" class="headerlink" title="修改slaves文件"></a>修改slaves文件</h2><p>主要是把作为namenode节点的ip从该文件中删除，视各自机器环境而定。</p><h2 id="启动HA集群"><a href="#启动HA集群" class="headerlink" title="启动HA集群"></a>启动HA集群</h2><p>启动顺序，非常讲究。。。</p><p>重要的一点：不容许使用 hdfs namenode -format的命令，此命令会删除原集群的所有数据。</p><p>第一步，先关所有的集群进程。</p><p>第二步，打开zookeeper集群：sh /home/deploy/zookeeper-3.4.9/bin/zkServer.sh start</p><p>第三步，打开journalnode进程，该进程在两个nn间同步元数据。在hadoop2\3\4上都执行：<br>sh hadoop-daemon.sh start journalnode</p><p>第四步，在原namenode节点上执行：sh hadoop-daemon.sh start namenode。<br>此操作会将namenode状态变成active。</p><p>第五步，在备（standby）节点执行同步namenode数据的命令：hdfs namenode -bootstrapStandby。<br>切记不要使用scp的方式同步元数据，会导致文件权限问题。</p><p>第六步，启动备namendoe：sh hadoop-daemon.sh start namenode</p><p>第七步，初始化zkfc。在主备两台上任意一台执行：hdfs zkfc -formatZK</p><p>第七步，启动zk(DFSZKFailoverController)，该进程维护共享锁保证只有一个active的namenode。分别在两台作为NN的节点上执行：sh hadoop-daemon.sh start zkfc</p><p>第八步，启动hdfs集群（就是打开所有的datanode进程）：sh start-dfs.sh</p><p>第九步，打开没有任何变化的yarn：sh start-yarn.sh。上面说了暂时不讨论resourcemanger的升级。</p><p>至此，hadoop集群双NN的升级就完成了。</p><h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><p>第一步， 打开两个NN的监控页：</p><p><img src="static/ScreenShot2017-02-16at11.12.24.png" alt=""></p><p><img src="static/ScreenShot2017-02-16at11.12.33.png" alt=""></p><p>第二步：kill hadoop1的namenode进程，查看hadoop2中namenode的状态由standby变为active：<br><img src="static/ScreenShot2017-02-16at11.14.55.png" alt=""><br>重复几次，主备仍能自切。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html#Hardware_resources" target="_blank" rel="noopener">官方文档: HDFS High Availability</a></li><li><a href="https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="noopener">官方文档: ResourceManager High Availability</a></li><li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="noopener">Hadoop NameNode 高可用 (High Availability) 实现解析</a></li><li><a href="http://blog.csdn.net/dingchenxixi/article/details/51131493" target="_blank" rel="noopener">Hadoop的HA机制(Zookeeper集群+Hadoop集群)配置记录</a></li><li><a href="http://www.jianshu.com/p/8a8fb958f11f" target="_blank" rel="noopener">hadoop HA高可用集群模式搭建指南</a>    </li><li><a href="http://blog.csdn.net/knowledgeaaa/article/details/23759099" target="_blank" rel="noopener">Journal Storage Directory not formatted</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本篇介绍Hadoop的一些常用知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。&lt;/p&gt;
&lt;p&gt;HA模式，主要是将namenode及resourcemanager都变成主备两个。这里先不讨论resourcemanager，主要针对namenode。&lt;/p&gt;
&lt;p&gt;将namenode变成可主备自动切换的，主要是通过zookeeper集群对namenode的健康状态进行监控，然后选举一个健康的namenode做active(主)的，另一个成为standby(备)。因此，保证zookeeper集群的配置是正确且不易挂掉，是HA的基石。同时，注意HA升级过程中的相关进程的启动步骤即可完成。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="集群运维" scheme="http://guzhenping.com/tags/%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/"/>
    
      <category term="Hadoop" scheme="http://guzhenping.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习指南（集群运维篇）</title>
    <link href="http://guzhenping.com/2016/12/09/Hadoop%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%88%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E7%AF%87%EF%BC%89/"/>
    <id>http://guzhenping.com/2016/12/09/Hadoop学习指南（集群运维篇）/</id>
    <published>2016-12-09T09:04:48.000Z</published>
    <updated>2018-11-30T02:34:31.633Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇介绍Hadoop的一些常用知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。</p><a id="more"></a><h2 id="常用HDFS命令"><a href="#常用HDFS命令" class="headerlink" title="常用HDFS命令"></a>常用HDFS命令</h2><ul><li>hadoop fs -ls URI</li><li>hadoop fs -du -h URI</li><li>hadoop fs -cat URI [文件较大，hadoop fs -cat xxxx | head]</li><li>hadoop fs -put URI</li><li>hadoop fs -get URI</li><li>hadoop fs -rmr URI</li><li>hadoop fs -stat %b,%o,%n,%r,%y URI (%b：文件大小, %o：Block 大小, %n：文件名, %r：副本个数, %y 或%Y：最后一次修改日期和时间)</li><li>hadoop fs -tail [-f] URI</li><li>hdfs dfsadmin -report</li><li>hadoop fs -appendToFile URI1[,URI2,…] URI（hadoop fs -appendToFile helloCopy1.txt helloCopy2.txt /user/tmp/hello.txt）</li><li>hadoop fsck / -files -blocks</li></ul><h2 id="关键的clusterID"><a href="#关键的clusterID" class="headerlink" title="关键的clusterID"></a>关键的clusterID</h2><p>这是hadoop集群的id号，只有拥有相同id的各个节点才能加入的这个集群中来。</p><p>大致是在：hdfs namenode -format命令之后生成这个id。</p><p>很多情况下，你的集群如果可能拥有不同的id号哦。就比如：</p><p>启动datanode是遇到： “DataNode: Initialization failed for Block pool”</p><p>此时应当查看：<br>cat /home/deploy/hadoop-2.7.1/hdfs/name/current/VERSION</p><p>cat /home/deploy/hadoop-2.7.1/hdfs/data/current/VERSION</p><p>一个是datanode数据的文件夹，一个是namenode数据的文件夹。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#Fri Feb 17 15:48:44 CST 2017</span><br><span class="line">namespaceID=74707331</span><br><span class="line">clusterID=CID-e3e7c80e-3099-461d-9fa9-404f4910def5</span><br><span class="line">cTime=0</span><br><span class="line">storageType=NAME_NODE</span><br><span class="line">blockpoolID=BP-1729560578-192.168.20.2-1487317724421</span><br><span class="line">layoutVersion=-63</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#Thu Dec 22 09:57:13 CST 2016</span><br><span class="line">storageID=DS-63997596-6d60-46e8-a08c-94426208f9d9</span><br><span class="line">clusterID=CID-076b4e8a-9ed9-47e9-b6e0-4c16440c33e8</span><br><span class="line">cTime=0</span><br><span class="line">datanodeUuid=3ae0642e-2d72-4b45-ba63-de5e6ca1c7c7</span><br><span class="line">storageType=DATA_NODE</span><br><span class="line">layoutVersion=-56</span><br></pre></td></tr></table></figure><h2 id="添加删除节点"><a href="#添加删除节点" class="headerlink" title="添加删除节点"></a>添加删除节点</h2><p><a href="https://my.oschina.net/MrMichael/blog/291802" target="_blank" rel="noopener">Hadoop添加删除节点</a></p><h2 id="重启丢失节点"><a href="#重启丢失节点" class="headerlink" title="重启丢失节点"></a>重启丢失节点</h2><h3 id="子节点DataNode丢失"><a href="#子节点DataNode丢失" class="headerlink" title="子节点DataNode丢失"></a>子节点DataNode丢失</h3><p><code>sbin/hadoop-daemon.sh start datanode</code></p><h3 id="子节点NodeManager丢失"><a href="#子节点NodeManager丢失" class="headerlink" title="子节点NodeManager丢失"></a>子节点NodeManager丢失</h3><p><code>sbin/yarn-daemon.sh start nodemanager</code></p><h3 id="主节点丢失"><a href="#主节点丢失" class="headerlink" title="主节点丢失"></a>主节点丢失</h3><p><code>sbin/start-all.sh</code></p><p>or</p><p><code>sbin/hadoop-daemon.sh start namenode</code></p><p><code>sbin/hadoop-daemon.sh start secondarynamenode</code></p><p><code>sbin/yarn-daemon.sh start resourcemanager</code></p><h2 id="配置文件出错"><a href="#配置文件出错" class="headerlink" title="配置文件出错"></a>配置文件出错</h2><p>管理hadoop集群，会经常遇到配置文件的相关问题。这里举一个例子，比如yarn的nodemanager起不来的问题。</p><p>yarn的相关配置文件有两个：yarn-site.xml和yarn-env.sh</p><p>在yarn-site.xml文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>在yarn-env.sh文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HEAP_MAX=-Xmx1024m</span><br></pre></td></tr></table></figure><p>应该保证yarn-site.xml中的memory-mb数值比yarn-env.sh中JAVA_HEAP_MAX的数值小。yarn-site.xml的配置是要求nodemanager启动的最少内存，低于该值无法启动。实际启动时，使用yarn-env.sh中的配置。修改比如：<code>JAVA_HEAP_MAX=-Xmx2048m</code></p><h2 id="no-xxx-to-stop"><a href="#no-xxx-to-stop" class="headerlink" title="no xxx to stop"></a>no xxx to stop</h2><p>hadoop会经常有这个问题，大概就是没有找到该进程的PID文件，所以报错。</p><p>具体参见连接：<br><a href="http://blog.csdn.net/gyqjn/article/details/50805472" target="_blank" rel="noopener">解决关闭Hadoop时no namenode to stop异常</a></p><p>每次启动hadoop（<code>./start-all.sh</code>）时，PID文件被生成，存储进程号。关闭时，PID文件被删除。</p><p>在hadoop2.7.1版本中，关于HADOOP_PID_DIR(文件路径：../etc/hadoop/hadoop-env.sh)的描述是这样的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># The directory where pid files are stored. /tmp by default.</span><br><span class="line"># NOTE: this should be set to a directory that can only be written to by</span><br><span class="line">#       the user that will run the hadoop daemons.  Otherwise there is the</span><br><span class="line">#       potential for a symlink attack.</span><br><span class="line">export HADOOP_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span><br><span class="line">export HADOOP_SECURE_DN_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span><br></pre></td></tr></table></figure><p>最好将PID文件放在只写目录中。</p><h2 id="关于mapred-site-xml配置"><a href="#关于mapred-site-xml配置" class="headerlink" title="关于mapred-site.xml配置"></a>关于mapred-site.xml配置</h2><p>参见blog:<a href="https://www.iteblog.com/archives/1945" target="_blank" rel="noopener">《如何给运行在Yarn的MapReduce作业配置内存》</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>-<a href="https://www.iteblog.com/archives/1945" target="_blank" rel="noopener">如何给运行在Yarn的MapReduce作业配置内存</a></p><ul><li><a href="https://segmentfault.com/a/1190000002672666" target="_blank" rel="noopener">hadoop HDFS常用文件操作命令</a></li><li><a href="http://book.51cto.com/art/201409/452359.htm" target="_blank" rel="noopener">HDFS 文件操作命令</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本篇介绍Hadoop的一些常用知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="集群运维" scheme="http://guzhenping.com/tags/%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/"/>
    
      <category term="Hadoop" scheme="http://guzhenping.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>集群搭建指南（上卷）</title>
    <link href="http://guzhenping.com/2016/11/16/%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97%EF%BC%88%E4%B8%8A%E5%8D%B7%EF%BC%89/"/>
    <id>http://guzhenping.com/2016/11/16/集群搭建指南（上卷）/</id>
    <published>2016-11-16T09:04:48.000Z</published>
    <updated>2018-11-30T02:36:17.890Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>阅读本文，需要具备Linux、计算机网络的基础知识。所以在文中出现的相关基础知识，均以链接的形式给出，务必理解该链接的内容后，继续阅读本指南。</p><p>集群搭建的环境多种多样，本文采用VitualBox安装5台虚拟机构建集群。具体环境：</p><ul><li>CentOS 6.5 64位</li><li>VirtualBox 5.1.10 Mac版</li><li>本机macOS Sierra 10.12.1 i7 8G内存<a id="more"></a><h2 id="单台装机"><a href="#单台装机" class="headerlink" title="单台装机"></a>单台装机</h2>先下载CentOS的iso格式纯净镜像，可以下载LiveDVD和minimal两个版本。<br>本文采用LiveDVD版做演示。</li></ul><h3 id="打开virtualbox："><a href="#打开virtualbox：" class="headerlink" title="打开virtualbox："></a>打开virtualbox：</h3><p><img src="static/屏幕快照2016-12-1410.18.50.png" alt=""></p><h3 id="点击”新建”按钮："><a href="#点击”新建”按钮：" class="headerlink" title="点击”新建”按钮："></a>点击”新建”按钮：</h3><p><img src="static/屏幕快照2016-12-1410.21.21.png" alt=""></p><h3 id="选择版本，并命名："><a href="#选择版本，并命名：" class="headerlink" title="选择版本，并命名："></a>选择版本，并命名：</h3><p><img src="static/屏幕快照2016-12-1410.22.22.png" alt=""></p><h3 id="点击”继续”按钮-改内存大小："><a href="#点击”继续”按钮-改内存大小：" class="headerlink" title="点击”继续”按钮,改内存大小："></a>点击”继续”按钮,改内存大小：</h3><p><img src="static/屏幕快照2016-12-1410.23.45.png" alt=""></p><h3 id="点击”继续”按钮-改虚拟硬盘："><a href="#点击”继续”按钮-改虚拟硬盘：" class="headerlink" title="点击”继续”按钮,改虚拟硬盘："></a>点击”继续”按钮,改虚拟硬盘：</h3><p><img src="static/屏幕快照2016-12-1410.25.34.png" alt=""></p><h3 id="点击“创建”按钮，改硬盘类型："><a href="#点击“创建”按钮，改硬盘类型：" class="headerlink" title="点击“创建”按钮，改硬盘类型："></a>点击“创建”按钮，改硬盘类型：</h3><p>关于磁盘类型，参考：<a href="https://zhidao.baidu.com/question/1302436594642278379.html" target="_blank" rel="noopener">https://zhidao.baidu.com/question/1302436594642278379.html</a><br><img src="static/屏幕快照2016-12-1410.27.09.png" alt=""></p><h3 id="点击”继续”按钮，改存储方式："><a href="#点击”继续”按钮，改存储方式：" class="headerlink" title="点击”继续”按钮，改存储方式："></a>点击”继续”按钮，改存储方式：</h3><p><img src="static/屏幕快照2016-12-1410.28.09.png" alt=""></p><h3 id="继续，改文件位置和大小："><a href="#继续，改文件位置和大小：" class="headerlink" title="继续，改文件位置和大小："></a>继续，改文件位置和大小：</h3><p><img src="static/屏幕快照2016-12-1410.29.17.png" alt=""></p><h3 id="点击”创建”-主界面显示该机器："><a href="#点击”创建”-主界面显示该机器：" class="headerlink" title="点击”创建”,主界面显示该机器："></a>点击”创建”,主界面显示该机器：</h3><p><img src="static/屏幕快照2016-12-1410.30.19.png" alt=""></p><h3 id="选中该机器，点击主界面”设置”按钮："><a href="#选中该机器，点击主界面”设置”按钮：" class="headerlink" title="选中该机器，点击主界面”设置”按钮："></a>选中该机器，点击主界面”设置”按钮：</h3><p><img src="static/屏幕快照2016-12-1410.32.12.png" alt=""></p><h3 id="点击”系统”，修改”启动顺序”："><a href="#点击”系统”，修改”启动顺序”：" class="headerlink" title="点击”系统”，修改”启动顺序”："></a>点击”系统”，修改”启动顺序”：</h3><p><img src="static/屏幕快照2016-12-1410.33.53.png" alt=""></p><h3 id="点击”网络”-修改网卡1："><a href="#点击”网络”-修改网卡1：" class="headerlink" title="点击”网络”,修改网卡1："></a>点击”网络”,修改网卡1：</h3><p><img src="static/屏幕快照2016-12-1410.38.29.png" alt=""></p><h3 id="为修改网卡2，先关闭该对话框，打开vitualbox管理界面："><a href="#为修改网卡2，先关闭该对话框，打开vitualbox管理界面：" class="headerlink" title="为修改网卡2，先关闭该对话框，打开vitualbox管理界面："></a>为修改网卡2，先关闭该对话框，打开vitualbox管理界面：</h3><p><img src="static/屏幕快照2016-12-1410.39.42.png" alt=""></p><h3 id="点击偏好设置："><a href="#点击偏好设置：" class="headerlink" title="点击偏好设置："></a>点击偏好设置：</h3><p><img src="static/屏幕快照2016-12-1410.41.17.png" alt=""></p><h3 id="点击”网络”："><a href="#点击”网络”：" class="headerlink" title="点击”网络”："></a>点击”网络”：</h3><p><img src="static/屏幕快照2016-12-1410.42.19.png" alt=""></p><h3 id="选择”仅主机（Host-Only）网络”："><a href="#选择”仅主机（Host-Only）网络”：" class="headerlink" title="选择”仅主机（Host-Only）网络”："></a>选择”仅主机（Host-Only）网络”：</h3><p><img src="static/屏幕快照2016-12-1410.43.45.png" alt=""></p><h3 id="点击添加按钮（右边绿色按钮），新增vboxnet1："><a href="#点击添加按钮（右边绿色按钮），新增vboxnet1：" class="headerlink" title="点击添加按钮（右边绿色按钮），新增vboxnet1："></a>点击添加按钮（右边绿色按钮），新增vboxnet1：</h3><p><img src="static/屏幕快照2016-12-1410.44.56.png" alt=""></p><h3 id="点击OK保存，再次选中该虚机（hadoop6）-打开主界面的”设置”按钮，点击”网络”，点击”网卡2”，勾选”启用网络连接”，选择连接方式："><a href="#点击OK保存，再次选中该虚机（hadoop6）-打开主界面的”设置”按钮，点击”网络”，点击”网卡2”，勾选”启用网络连接”，选择连接方式：" class="headerlink" title="点击OK保存，再次选中该虚机（hadoop6）,打开主界面的”设置”按钮，点击”网络”，点击”网卡2”，勾选”启用网络连接”，选择连接方式："></a>点击OK保存，再次选中该虚机（hadoop6）,打开主界面的”设置”按钮，点击”网络”，点击”网卡2”，勾选”启用网络连接”，选择连接方式：</h3><p><img src="static/屏幕快照2016-12-1410.48.53.png" alt=""></p><h3 id="选vboxnet1-则其余所有节点均需保持一致。点击OK保存。此时点击主界面的”启动”按钮："><a href="#选vboxnet1-则其余所有节点均需保持一致。点击OK保存。此时点击主界面的”启动”按钮：" class="headerlink" title="选vboxnet1,则其余所有节点均需保持一致。点击OK保存。此时点击主界面的”启动”按钮："></a>选vboxnet1,则其余所有节点均需保持一致。点击OK保存。此时点击主界面的”启动”按钮：</h3><p><img src="static/屏幕快照2016-12-1410.51.24.png" alt=""></p><h3 id="选择Centos的iso文件所在位置，点击”启动”。在弹出黑色界面时（有9秒时间），按一次任意的某个键，进入Boot界面。选择第一种安装方式：verify-and-boot："><a href="#选择Centos的iso文件所在位置，点击”启动”。在弹出黑色界面时（有9秒时间），按一次任意的某个键，进入Boot界面。选择第一种安装方式：verify-and-boot：" class="headerlink" title="选择Centos的iso文件所在位置，点击”启动”。在弹出黑色界面时（有9秒时间），按一次任意的某个键，进入Boot界面。选择第一种安装方式：verify and boot："></a>选择Centos的iso文件所在位置，点击”启动”。在弹出黑色界面时（有9秒时间），按一次任意的某个键，进入Boot界面。选择第一种安装方式：verify and boot：</h3><p><img src="static/屏幕快照2016-12-1410.59.14.png" alt=""></p><h3 id="按下回车键，进入centos系统："><a href="#按下回车键，进入centos系统：" class="headerlink" title="按下回车键，进入centos系统："></a>按下回车键，进入centos系统：</h3><p><img src="static/屏幕快照2016-12-1411.01.29.png" alt=""></p><h3 id="双击系统桌面的”install-to-hard-drive”"><a href="#双击系统桌面的”install-to-hard-drive”" class="headerlink" title="双击系统桌面的”install to hard drive”:"></a>双击系统桌面的”install to hard drive”:</h3><p><img src="static/屏幕快照2016-12-1411.03.09.png" alt=""></p><h3 id="进入安装界面："><a href="#进入安装界面：" class="headerlink" title="进入安装界面："></a>进入安装界面：</h3><p><img src="static/屏幕快照2016-12-1411.04.13.png" alt=""></p><h3 id="一路点击next按钮-点击yes即可。时区选亚洲上海-root账户的密码要牢记。然后，进入等待界面："><a href="#一路点击next按钮-点击yes即可。时区选亚洲上海-root账户的密码要牢记。然后，进入等待界面：" class="headerlink" title="一路点击next按钮,点击yes即可。时区选亚洲上海,root账户的密码要牢记。然后，进入等待界面："></a>一路点击next按钮,点击yes即可。时区选亚洲上海,root账户的密码要牢记。然后，进入等待界面：</h3><p><img src="static/屏幕快照2016-12-1411.08.53.png" alt=""></p><h3 id="点击”close”-关闭安装界面："><a href="#点击”close”-关闭安装界面：" class="headerlink" title="点击”close”,关闭安装界面："></a>点击”close”,关闭安装界面：</h3><p><img src="static/屏幕快照2016-12-1411.13.53.png" alt=""></p><h3 id="关闭该虚拟机，调整系统启动顺序。选中该虚机，点击”设置”，点击”系统”-调整”启动顺序”，将”硬盘”放在第一位，将”光驱”放在最后一位："><a href="#关闭该虚拟机，调整系统启动顺序。选中该虚机，点击”设置”，点击”系统”-调整”启动顺序”，将”硬盘”放在第一位，将”光驱”放在最后一位：" class="headerlink" title="关闭该虚拟机，调整系统启动顺序。选中该虚机，点击”设置”，点击”系统”,调整”启动顺序”，将”硬盘”放在第一位，将”光驱”放在最后一位："></a>关闭该虚拟机，调整系统启动顺序。选中该虚机，点击”设置”，点击”系统”,调整”启动顺序”，将”硬盘”放在第一位，将”光驱”放在最后一位：</h3><p><img src="static/屏幕快照2016-12-1411.18.40.png" alt=""></p><h3 id="点击ok保存，再次启动该虚机，进入设置页面："><a href="#点击ok保存，再次启动该虚机，进入设置页面：" class="headerlink" title="点击ok保存，再次启动该虚机，进入设置页面："></a>点击ok保存，再次启动该虚机，进入设置页面：</h3><p><img src="static/屏幕快照2016-12-1411.21.10.png" alt=""></p><h3 id="一路forward即可，创建用户名和密码自己定义："><a href="#一路forward即可，创建用户名和密码自己定义：" class="headerlink" title="一路forward即可，创建用户名和密码自己定义："></a>一路forward即可，创建用户名和密码自己定义：</h3><p><img src="static/屏幕快照2016-12-1411.23.28.png" alt=""></p><h3 id="为集群选择同一时间，需要勾选synchronize-date-and-time-over-the-network-防止某个节点挂了以后和其他节点的系统时间不一致："><a href="#为集群选择同一时间，需要勾选synchronize-date-and-time-over-the-network-防止某个节点挂了以后和其他节点的系统时间不一致：" class="headerlink" title="为集群选择同一时间，需要勾选synchronize date and time over the network.防止某个节点挂了以后和其他节点的系统时间不一致："></a>为集群选择同一时间，需要勾选synchronize date and time over the network.防止某个节点挂了以后和其他节点的系统时间不一致：</h3><p><img src="static/屏幕快照2016-12-1411.27.04.png" alt=""></p><h3 id="一路forward，点击finish-进入登陆界面："><a href="#一路forward，点击finish-进入登陆界面：" class="headerlink" title="一路forward，点击finish,进入登陆界面："></a>一路forward，点击finish,进入登陆界面：</h3><p><img src="static/屏幕快照2016-12-1411.29.02.png" alt=""></p><p><strong>注：如果采用minimal安装方法，装机方式仅有两处与上述不同：</strong></p><p><strong>选第一种安装方式</strong><br><img src="static/屏幕快照2016-12-1418.27.32.png" alt=""></p><p><strong>选skip，跳过检测</strong><br><img src="static/屏幕快照2016-12-1418.29.18.png" alt=""></p><p><strong>其余过程，同上述LiveDVD版安装过程。</strong></p><h2 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h2><p>重复上述步骤，安装5台虚拟机。此时需要配置集群的网络：配置网卡，修改hostname,添加host解析，添加普通用户。</p><h3 id="配置网卡"><a href="#配置网卡" class="headerlink" title="配置网卡"></a>配置网卡</h3><p>通过上述步骤的装机方法，每台机器中都将有两个网卡eth0和eth1。eth0作为虚机网络的公网网口，eth1作为虚拟机网络的内网网口。</p><p>配置网卡，需要在目录：/etc/sysconfig/network-scripts下新建以ifcfg-ethX（X是一个数字，从0开始，一般到3结束。）文件。<br>所以，关于eth0的配置在：/etc/sysconfig/network-scripts/ifcfg-eth0;</p><p>关于eth1的配置在：/etc/sysconfig/network-scripts/ifcfg-eth1。</p><p>如果有不明白的地方，可以参考下文—-CentOS网络配置详解。</p><h5 id="配置eth0"><a href="#配置eth0" class="headerlink" title="配置eth0"></a>配置eth0</h5><p>在System-&gt;Preferences-&gt;Network Connections，进行配置。<br><img src="static/屏幕快照2016-12-1414.15.07.png" alt=""></p><p>先编辑Auto eth0:<br><img src="static/屏幕快照2016-12-1414.16.43.png" alt=""></p><p>连接名修改为eth0，勾选所有用户可用，设置IPv4,选择DHCP即可：<br><img src="static/屏幕快照2016-12-1414.19.59.png" alt=""></p><p>点击Apply，输入root账户验证即可。</p><p>如果采用minimal版本的CentOS安装，文件配置方法，需要在/etc/sysconfig/network-scripts/ifcfg-eth0中修改：</p><ul><li>ONBOOT=yes</li><li>BOOTPROTO=dhcp</li></ul><p>其他参数均由系统自动生成：<br><img src="static/屏幕快照2016-12-1418.15.03.png" alt=""></p><p>其余几台节点的eth0和上述相同。这样所有节点的公网IP是相同的。只要宿主机可以上网， 那么所有节点均可上网。</p><h5 id="配置eth1"><a href="#配置eth1" class="headerlink" title="配置eth1"></a>配置eth1</h5><p>eth1采用host-only模式，在Manual方法下填写ip。网段确定在192.168.xx.xx中。<br>对于5个节点的集群，主机名为hadoop1-hadoop5，则ip分别是：</p><ul><li>192.168.20.2  hadoop1</li><li>192.168.20.3  hadoop2</li><li>192.168.20.4  hadoop3</li><li>192.168.20.5  hadoop4</li><li>192.168.20.6  hadoop5</li></ul><p>先编辑Auto eth1, 设置IPv4，选择Manual,添加ip,子网掩码，网关:<br><img src="static/屏幕快照2016-12-1414.26.03.png" alt=""></p><p>点击Apply,验证root账户，即可成功。这样192.168.20.2就分配给这台机器了（下面说把这台机器变成hadoop2）。</p><p>如果采用minimal版本的CentOS安装，文件配置方法，需要在/etc/sysconfig/network-scripts/ifcfg-eth1中修改：</p><ul><li>ONBOOT=yes</li><li>BOOTPROTO=none</li><li>IPADDR=192.168.20.2</li><li>NETMASK=255.255.255.0</li></ul><p>其他参数(注意不要配置gateway)均由系统自动生成：<br><img src="static/屏幕快照2016-12-1418.21.02.png" alt=""></p><p>其余几台机器同上述配置过程，只需要更改IP(IPADDR参数)即可，子网掩码和网关（minimal不用配）均相同。</p><h3 id="修改hostname"><a href="#修改hostname" class="headerlink" title="修改hostname"></a>修改hostname</h3><p>一般是localhost开头，但是不容易标识机器。改成可标识的。有5台机器，那么这5台机器可以对应hadoop1-5。</p><p>修改文件的目录在: /etc/sysconfig/network</p><p><img src="static/屏幕快照2016-12-1414.33.26.png" alt=""></p><p>将HOSTNAME这个参数改掉即可。对于5个节点，分别在每台机器上修改为hadoop1-5。</p><h3 id="添加host解析"><a href="#添加host解析" class="headerlink" title="添加host解析"></a>添加host解析</h3><p>每台虚机都有ip，但是ip难记，加个host解析，方便使用。</p><p>配置文件在:/etc/hosts</p><p><img src="static/屏幕快照2016-12-1414.35.59.png" alt=""></p><p>先把127.0.0.1改成你已经修改的hostname的值，比如在hadoop1(192.168.20.2)这台机器上，需要改成：</p><ul><li>127.0.0.1 hadoop1</li><li>::1  hadoop1</li></ul><p>这里，每个节点是不同的。</p><p>所有的节点均需添加内容：</p><ul><li>192.168.20.2  hadoop1</li><li>192.168.20.3  hadoop2</li><li>192.168.20.4  hadoop3</li><li>192.168.20.5  hadoop4</li><li>192.168.20.6  hadoop5</li></ul><p>保存即可。</p><h3 id="添加普通用户"><a href="#添加普通用户" class="headerlink" title="添加普通用户"></a>添加普通用户</h3><p>添加普通账号，比如：deploy,分配给使用者。配置/etc/sudoers文件，使得该账户也可以进行sudo操作。</p><p>此处较为简单，参考下文—-linux的账户。</p><h2 id="装机的思考"><a href="#装机的思考" class="headerlink" title="装机的思考"></a>装机的思考</h2><h3 id="为什么需要两个网卡？"><a href="#为什么需要两个网卡？" class="headerlink" title="为什么需要两个网卡？"></a>为什么需要两个网卡？</h3><p>如果只使用默认的 NAT，会发现一旦宿主机断开公网，自己的几台虚机之间也会无法通。最简单的解决方案是双网卡，如下图所示：<br><img src="static/集群实验平台.png" alt=""></p><h3 id="为什么修改两次系统启动顺序？"><a href="#为什么修改两次系统启动顺序？" class="headerlink" title="为什么修改两次系统启动顺序？"></a>为什么修改两次系统启动顺序？</h3><p>第一次装机是系统是来自iso文件的（光驱启动）。每次都从该文件启动，则无法对系统进行定制修改。将其装在硬盘上（自己的虚拟硬盘），每次从硬盘启动即可对系统进行定制修改。</p><p>所以，需要改变启动顺序。装到硬盘后，从硬盘启动。</p><h3 id="linux的账户"><a href="#linux的账户" class="headerlink" title="linux的账户"></a>linux的账户</h3><p>root账户权限太大，必须给开发者一个使用账户。</p><p>相关链接：<a href="http://linuxme.blog.51cto.com/1850814/347086/" target="_blank" rel="noopener">http://linuxme.blog.51cto.com/1850814/347086/</a></p><h3 id="CentOS网络配置详解"><a href="#CentOS网络配置详解" class="headerlink" title="CentOS网络配置详解"></a>CentOS网络配置详解</h3><p>包含网络配置的很多东西：<br><a href="http://blog.chinaunix.net/uid-26495963-id-3230810.html" target="_blank" rel="noopener">http://blog.chinaunix.net/uid-26495963-id-3230810.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;阅读本文，需要具备Linux、计算机网络的基础知识。所以在文中出现的相关基础知识，均以链接的形式给出，务必理解该链接的内容后，继续阅读本指南。&lt;/p&gt;
&lt;p&gt;集群搭建的环境多种多样，本文采用VitualBox安装5台虚拟机构建集群。具体环境：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CentOS 6.5 64位&lt;/li&gt;
&lt;li&gt;VirtualBox 5.1.10 Mac版&lt;/li&gt;
&lt;li&gt;本机macOS Sierra 10.12.1 i7 8G内存
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="集群运维" scheme="http://guzhenping.com/tags/%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/"/>
    
      <category term="Hadoop" scheme="http://guzhenping.com/tags/Hadoop/"/>
    
  </entry>
  
</feed>
