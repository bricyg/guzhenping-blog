<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>谷震平的博客</title>
  
  <subtitle>写点一路的风景，都很普通，主要还是留给自己。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://guzhenping.com/"/>
  <updated>2018-11-29T09:10:40.619Z</updated>
  <id>http://guzhenping.com/</id>
  
  <author>
    <name>谷震平[guzhenping, hehzyx@gmail.com]</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>前端开发学习笔记（一）</title>
    <link href="http://guzhenping.com/2018/11/29/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/11/29/前端开发学习笔记（一）/</id>
    <published>2018-11-29T09:04:48.000Z</published>
    <updated>2018-11-29T09:10:40.619Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>还是在全沾的道路上越走越远，特地记录下一路上的问题。</p><h2 id="初识vue-js"><a href="#初识vue-js" class="headerlink" title="初识vue.js"></a>初识vue.js</h2><p>待续…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;还是在全沾的道路上越走越远，特地记录下一路上的问题。&lt;/p&gt;
&lt;h2 id=&quot;初识vue-js&quot;&gt;&lt;a href=&quot;#初识vue-js&quot; c
      
    
    </summary>
    
    
      <category term="前端开发" scheme="http://guzhenping.com/tags/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://guzhenping.com/2018/11/06/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E4%B9%A0%EF%BC%88Postgre%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/11/06/数据库学习（Postgre）/</id>
    <published>2018-11-05T16:00:00.000Z</published>
    <updated>2018-11-28T06:32:44.234Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据库学习（Postgre）"><a href="#数据库学习（Postgre）" class="headerlink" title="数据库学习（Postgre）"></a>数据库学习（Postgre）</h1><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h2 id="sql基础"><a href="#sql基础" class="headerlink" title="sql基础"></a>sql基础</h2><ul><li>修改字段类型：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-- 将string类型变成int</span><br><span class="line"></span><br><span class="line">LTER TABLE the_table ALTER COLUMN col_name TYPE integer USING (col_name::integer);    -- 字段值没有空格</span><br><span class="line"></span><br><span class="line">ALTER TABLE the_table ALTER COLUMN col_name TYPE integer USING (trim(col_name)::integer);    -- 字段值有空格</span><br><span class="line"></span><br><span class="line">ALTER TABLE onetruereport.report_jinrong_submit </span><br><span class="line">ALTER COLUMN createdtime TYPE integer USING (trim(createdtime)::integer);</span><br></pre></td></tr></table></figure><ul><li>转多列</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">SELECT unnest(string_to_array(&apos;</span><br><span class="line">  xxx@xx.com,</span><br><span class="line">  ccc@cc.com&apos;, &apos;,&apos;)) as email</span><br></pre></td></tr></table></figure><p>string_to_array：将字符串处理成array数组，unnest将array转成多列。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数据库学习（Postgre）&quot;&gt;&lt;a href=&quot;#数据库学习（Postgre）&quot; class=&quot;headerlink&quot; title=&quot;数据库学习（Postgre）&quot;&gt;&lt;/a&gt;数据库学习（Postgre）&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Redash开发指南</title>
    <link href="http://guzhenping.com/2018/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Redash%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/11/02/大数据开发学习（Redash）/</id>
    <published>2018-11-02T09:04:48.000Z</published>
    <updated>2018-11-30T02:48:21.177Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-Redash介绍"><a href="#一-Redash介绍" class="headerlink" title="一 Redash介绍"></a>一 Redash介绍</h2><p>Redash是一款融合多数据源的可视化查询工具，用于Ad-hoc查询再好不过。除了官方支持的数据源，还可以通过复用代码开发支持Kylin、Clickhouse、TiDB、Palo、Druid等。<br><a id="more"></a></p><h2 id="二-测试环境"><a href="#二-测试环境" class="headerlink" title="二 测试环境"></a>二 测试环境</h2><h4 id="官方提供的测试环境启动方式"><a href="#官方提供的测试环境启动方式" class="headerlink" title="官方提供的测试环境启动方式"></a>官方提供的测试环境启动方式</h4><p>redash开发文档 ： <a href="https://redash.io/help-onpremise/" target="_blank" rel="noopener">https://redash.io/help-onpremise/</a></p><h4 id="自主搭建"><a href="#自主搭建" class="headerlink" title="自主搭建"></a>自主搭建</h4><p>安装虚拟环境管理工具anaconda, 创建虚拟环境redash:<code>conda create -n redash python=2.7</code>,激活该环境:<code>source activate redash</code>。</p><p>创建虚拟环境（也可以不创建），激活后。利用pip安装redash所需要的包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt      # 程序基础依赖</span><br><span class="line">pip install -r requirements_all_ds.txt    # 所有数据库的依赖</span><br></pre></td></tr></table></figure><p>在上述安装过程中大概率会出现安装错误(遇到过10多回)。请将出错的包单独安装。</p><p>安装npm，在主目录执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install    # 安装node模块</span><br><span class="line">npm run build    # 编译前端的东西</span><br></pre></td></tr></table></figure><p>启动服务器:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/run ./manage.py runserver --debugger --reload</span><br></pre></td></tr></table></figure></p><p>启动celery的woker和调度器:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/run celery worker --app=redash.worker --beat -Qscheduled_queries,queries,celery -c2</span><br></pre></td></tr></table></figure></p><p>也可以分开启动woker和调度器，调度器启动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/run celery worker --app=redash.worker -c4 -Qscheduled_queries --maxtasksperchild=10 -Ofair</span><br></pre></td></tr></table></figure><p>worker启动:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/run celery worker --app=redash.worker --beat -c8 -Qqueries,celery --maxtasksperchild=10 -Ofair</span><br></pre></td></tr></table></figure></p><p><strong>根据机器机器实际情况，调节-c后的参数，用来指定启动多少个进程数量。</strong></p><h4 id="启动方式"><a href="#启动方式" class="headerlink" title="启动方式"></a>启动方式</h4><p>除了上面的启动方式，还推荐使用guncoin的启动方式。</p><p>启动服务器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 前台启动</span><br><span class="line">gunicorn -b 127.0.0.1:5000 --name redash -w 4 --max-requests 1000 redash.wsgi:app</span><br><span class="line"></span><br><span class="line"># 后台启动</span><br><span class="line">nohup /home/hadoop/anaconda3/envs/redash/bin/python /home/hadoop/anaconda3/envs/redash/bin/gunicorn -b 0.0.0.0:5000 --name redash -w 4 --max-requests 1000 redash.wsgi:app &gt;&gt; redash.log &amp;</span><br></pre></td></tr></table></figure><p>没有gunicorn命令的，需要装Python包。</p><p>启动调度器进程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 前台启动</span><br><span class="line">bin/run celery worker --app=redash.worker -c4 -Qscheduled_queries --maxtasksperchild=10 -Ofair</span><br><span class="line"></span><br><span class="line"># 后台启动</span><br><span class="line">nohup /home/hadoop/anaconda3/envs/redash/bin/celery worker --app=redash.worker -c4 -Qscheduled_queries --maxtasksperchild=10 -Ofair &gt;&gt; schedular.log &amp;</span><br></pre></td></tr></table></figure><p>启动worker进程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 前台启动</span><br><span class="line">bin/run celery worker --app=redash.worker --beat -c8 -Qqueries,celery --maxtasksperchild=10 -Ofair  </span><br><span class="line"></span><br><span class="line"># 后台启动</span><br><span class="line">nohup /home/hadoop/anaconda3/envs/redash/bin/celery worker --app=redash.worker --beat -c8 -Qqueries,celery --maxtasksperchild=10 -Ofair  &gt;&gt; worker.log &amp;</span><br></pre></td></tr></table></figure><p>也可以用nginx做下代理，配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">worker_processes  4;</span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile        on;</span><br><span class="line">    keepalive_timeout  3600;</span><br><span class="line"></span><br><span class="line">    upstream rd_servers &#123;</span><br><span class="line">         server 127.0.0.1:5000;</span><br><span class="line">    &#125;</span><br><span class="line">    server &#123;</span><br><span class="line">          server_tokens off;</span><br><span class="line">          listen 5123 default;</span><br><span class="line">          access_log /var/log/nginx/rd.access.log;</span><br><span class="line">          gzip on;</span><br><span class="line">          gzip_types *;</span><br><span class="line">          gzip_proxied any;</span><br><span class="line">          location / &#123;</span><br><span class="line">              proxy_set_header Host $http_host;</span><br><span class="line">              proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">              proxy_set_header X-Forwarded-Proto $scheme;</span><br><span class="line">              proxy_pass       http://rd_servers;</span><br><span class="line">          &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动NGINX:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/nginx/sbin</span><br><span class="line">sudo ./nginx</span><br></pre></td></tr></table></figure><h2 id="三-Redash-VS-Superset"><a href="#三-Redash-VS-Superset" class="headerlink" title="三 Redash VS Superset"></a>三 Redash VS Superset</h2><p>关于Redash：<a href="https://redash.io/；" target="_blank" rel="noopener">https://redash.io/；</a></p><p>与Superset的区别与联系：<a href="https://www.zhihu.com/question/60369195/answer/258298127。" target="_blank" rel="noopener">https://www.zhihu.com/question/60369195/answer/258298127。</a></p><h2 id="Bug"><a href="#Bug" class="headerlink" title="Bug"></a>Bug</h2><p>hive 读 schema<br>show columns in table , 无法处理中文comment,<br>desc table , 无法处理没有字段的表，</p><p>报错：Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.ClassNotFoundException Class org.apache.hive.hcatalog.data.JsonSerDe not found</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-Redash介绍&quot;&gt;&lt;a href=&quot;#一-Redash介绍&quot; class=&quot;headerlink&quot; title=&quot;一 Redash介绍&quot;&gt;&lt;/a&gt;一 Redash介绍&lt;/h2&gt;&lt;p&gt;Redash是一款融合多数据源的可视化查询工具，用于Ad-hoc查询再好不过。除了官方支持的数据源，还可以通过复用代码开发支持Kylin、Clickhouse、TiDB、Palo、Druid等。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Redash" scheme="http://guzhenping.com/tags/Redash/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发学习（Redis）</title>
    <link href="http://guzhenping.com/2018/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Redis%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/11/02/大数据开发学习（Redis）/</id>
    <published>2018-11-02T09:04:48.000Z</published>
    <updated>2018-11-30T02:49:33.473Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-前言"><a href="#一-前言" class="headerlink" title="一 前言"></a>一 前言</h2><p>经常使用redis, 特地进行总结。</p><a id="more"></a><h2 id="二-基础"><a href="#二-基础" class="headerlink" title="二 基础"></a>二 基础</h2><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>下载安装包，或者：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://download.redis.io/releases/redis-3.2.0.tar.gz</span><br><span class="line">$ tar xzf redis-3.2.0.tar.gz</span><br><span class="line">$ cd redis-3.2.0</span><br><span class="line">$ make</span><br></pre></td></tr></table></figure><p>就是make命令~中间过程，报什么错搞定什么即可。</p><p>Ubuntu下比较简单，就是apt-get install redis-server</p><p>默认安装位置： 利用whereis redis 去找redis.conf ，需要修改是否后台运行daemonize （为yes）等属性</p><p>完成后，自动启动，可用命令：ps -aux| grep redis</p><p>redis是依赖，因为我们用python，不得不装一个wrapper——redis-py:<br><code>sudo pip install redis</code></p><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><p>要让Redis-server在后台运行！</p><p>1.简单的启动：</p><p>进到redis目录下的src文件夹下，输入：<br>redis-server</p><p>2.后台启动：</p><p>先去找redis.conf文件，修改daemonize属性，从no变为yes<br>进到redis目录下的src文件夹下，<br>输入：<br>redis-server &amp;</p><p>3.通过配置文件启动：</p><p>需要配置启动文件，在Redis工程目录下有个redis.conf文件，修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#修改daemonize为yes，即默认以后台程序方式运行（还记得前面手动使用&amp;号强制后台运行吗）。</span><br><span class="line">daemonize yes</span><br><span class="line"></span><br><span class="line">#可修改默认监听端口，别改了，万一你忘了</span><br><span class="line">port 6379</span><br><span class="line"></span><br><span class="line">#修改生成默认日志文件位置</span><br><span class="line">logfile  &quot;/home/futeng/logs/redis.log&quot;</span><br><span class="line"></span><br><span class="line">#配置持久化文件存放位置</span><br><span class="line">dir /home/futeng/data/redisData</span><br></pre></td></tr></table></figure><p>配置完以后，启动：<br>还是 src目录下，<code>redis-server ./redis.conf</code></p><p>if 改了端口，使用redis-cli命令连接时，需要带上端口，比如：<br>redis-cli -p xxxx  [再次强调：仍在src目录下]</p><p>4.使用redis启动脚本，设置开机自启<br>在生产环境中使用这种方式。</p><p>5.启动无密码验证的</p><p><code>redis-server --protected-mode no</code></p><p>设置连接数</p><p><code>redis-server --protected-mode no --maxclients 100000</code></p><h4 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h4><p>还是进到redis目录下的src文件夹下，输入：<br>redis-cli</p><p>在弹出的交互界面测试下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">redis&gt; set foo bar</span><br><span class="line">OK</span><br><span class="line">redis&gt; get foo</span><br><span class="line">&quot;bar&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-前言&quot;&gt;&lt;a href=&quot;#一-前言&quot; class=&quot;headerlink&quot; title=&quot;一 前言&quot;&gt;&lt;/a&gt;一 前言&lt;/h2&gt;&lt;p&gt;经常使用redis, 特地进行总结。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Redis" scheme="http://guzhenping.com/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://guzhenping.com/2018/11/02/Redash%20Redis%E9%98%9F%E5%88%97%E8%BF%90%E7%BB%B4/"/>
    <id>http://guzhenping.com/2018/11/02/Redash Redis队列运维/</id>
    <published>2018-11-01T16:00:00.000Z</published>
    <updated>2018-11-28T06:32:44.233Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redash-队列运维"><a href="#Redash-队列运维" class="headerlink" title="Redash 队列运维"></a>Redash 队列运维</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> datetime, json, redis</span><br><span class="line"></span><br><span class="line">r = redis.StrictRedis(host=<span class="string">'xx.xxx.xx.xx'</span>, port=<span class="number">123</span>, db=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">list = [<span class="string">'query_task_trackers:waiting'</span>, <span class="string">'query_task_trackers:in_progress'</span>]</span><br><span class="line">pipe = r.pipeline()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> list:</span><br><span class="line">    t = item</span><br><span class="line">    ids = r.zrevrange(t, <span class="number">0</span>, <span class="number">-1</span>)</span><br><span class="line">    print(<span class="string">"The queue "</span> + item + <span class="string">" has "</span> + str(len(ids)))</span><br><span class="line">    <span class="keyword">if</span> len(ids) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">for</span> id <span class="keyword">in</span> ids:</span><br><span class="line">        pipe.get(id)</span><br><span class="line">        p = pipe.execute()</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 可以调试看看p的数据结构，现阶段值会返回list，index=0</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            json_text = json.loads(p[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        timestamp = json_text[<span class="string">'created_at'</span>]</span><br><span class="line">        date = datetime.datetime.fromtimestamp(timestamp)</span><br><span class="line">        date_str = date.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line">        print(date_str)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 选择自己合适的时间</span></span><br><span class="line">        flag_day = datetime.datetime.today() - datetime.timedelta(days=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># flag_day = datetime.datetime.today() - datetime.timedelta(hours=12)</span></span><br><span class="line">        print(flag_day)</span><br><span class="line">        <span class="keyword">if</span> flag_day &gt; date:</span><br><span class="line">            print(<span class="string">"可以删除..."</span>)</span><br><span class="line">            print(<span class="string">"delete task: "</span> + str(id))</span><br><span class="line">            r.delete(id)</span><br><span class="line">            r.zrem(t, id)</span><br><span class="line">            print(<span class="string">"deleted success..."</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"不需要删除..."</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redash-队列运维&quot;&gt;&lt;a href=&quot;#Redash-队列运维&quot; class=&quot;headerlink&quot; title=&quot;Redash 队列运维&quot;&gt;&lt;/a&gt;Redash 队列运维&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;tab
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://guzhenping.com/2018/10/31/Hadoop%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%88Yarn%E7%AF%87%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/10/31/Hadoop学习指南（Yarn篇）/</id>
    <published>2018-10-30T16:00:00.000Z</published>
    <updated>2018-11-28T06:32:44.222Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop学习指南-YARN篇"><a href="#Hadoop学习指南-YARN篇" class="headerlink" title="Hadoop学习指南(YARN篇)"></a>Hadoop学习指南(YARN篇)</h1><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>从业2年多，也该总结下关于Yarn的东西了。</p><p>架构图：</p><p><img src="static/yarn/yarn-architecture.png" alt=""></p><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><ul><li>ResourceManager: 全局资源管理和任务调度</li><li>NodeManager: 单个节点的资源管理和监控</li><li>ApplicationMaster: 单个作业的资源管理和任务监控</li><li>Container: 资源申请的单位和任务运行的容器</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://wiki.jikexueyuan.com/project/hadoop/yarn.html" target="_blank" rel="noopener">极客学院 Yarn资料</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hadoop学习指南-YARN篇&quot;&gt;&lt;a href=&quot;#Hadoop学习指南-YARN篇&quot; class=&quot;headerlink&quot; title=&quot;Hadoop学习指南(YARN篇)&quot;&gt;&lt;/a&gt;Hadoop学习指南(YARN篇)&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;前言
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>大数据开发学习（Flink）</title>
    <link href="http://guzhenping.com/2018/10/29/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Flink%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/10/29/大数据开发学习（Flink）/</id>
    <published>2018-10-29T09:04:48.000Z</published>
    <updated>2018-11-30T02:46:37.477Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Flink在实时流处理领域越来越热，以阿里为首的企业正在投入更多的资源。在实际工作中也遇到了流处理的场景，特此学习一下。</p><a id="more"></a><h2 id="Demo制作"><a href="#Demo制作" class="headerlink" title="Demo制作"></a>Demo制作</h2><p>先看一个demo:</p><p>WordCount.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gzp.batch;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.DataSet;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line">        <span class="keyword">final</span> ExecutionEnvironment env = ExecutionEnvironment.createCollectionsEnvironment();</span><br><span class="line">        DataSet&lt;String&gt; text = WordCountData.getDefaultTextLineDataSet(env);</span><br><span class="line">        DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; counts =</span><br><span class="line">                <span class="comment">// split up the lines in pairs (2-tuples) containing: (word,1)</span></span><br><span class="line">                text.flatMap(<span class="keyword">new</span> Tokenizer())</span><br><span class="line">                    .groupBy(<span class="number">0</span>)</span><br><span class="line">                    .sum(<span class="number">1</span>);</span><br><span class="line">        counts.print();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Tokenizer</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// emit the pairs</span></span><br><span class="line">            <span class="keyword">for</span> (String token : value.toLowerCase().split(<span class="string">"\\W+"</span>)) &#123;</span><br><span class="line">                <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(token, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>WordCountData.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">package com.gzp.batch;</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.java.DataSet;</span><br><span class="line">import org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"></span><br><span class="line">public class WordCountData &#123;</span><br><span class="line">    public static final String[] WORDS = new String[] &#123;</span><br><span class="line">            &quot;To be, or not to be,--that is the question:--&quot;,</span><br><span class="line">            &quot;Whether &apos;tis nobler in the mind to suffer&quot;,</span><br><span class="line">            &quot;The slings and arrows of outrageous fortune&quot;,</span><br><span class="line">            &quot;Or to take arms against a sea of troubles,&quot;,</span><br><span class="line">            &quot;And by opposing end them?--To die,--to sleep,--&quot;,</span><br><span class="line">            &quot;No more; and by a sleep to say we end&quot;,</span><br><span class="line">            &quot;The heartache, and the thousand natural shocks&quot;,</span><br><span class="line">            &quot;That flesh is heir to,--&apos;tis a consummation&quot;,</span><br><span class="line">            &quot;Devoutly to be wish&apos;d. To die,--to sleep;--&quot;,</span><br><span class="line">            &quot;To sleep! perchance to dream:--ay, there&apos;s the rub;&quot;,</span><br><span class="line">            &quot;For in that sleep of death what dreams may come,&quot;,</span><br><span class="line">            &quot;When we have shuffled off this mortal coil,&quot;,</span><br><span class="line">            &quot;Must give us pause: there&apos;s the respect&quot;,</span><br><span class="line">            &quot;That makes calamity of so long life;&quot;,</span><br><span class="line">            &quot;For who would bear the whips and scorns of time,&quot;,</span><br><span class="line">            &quot;The oppressor&apos;s wrong, the proud man&apos;s contumely,&quot;,</span><br><span class="line">            &quot;The pangs of despis&apos;d love, the law&apos;s delay,&quot;,</span><br><span class="line">            &quot;The insolence of office, and the spurns&quot;,</span><br><span class="line">            &quot;That patient merit of the unworthy takes,&quot;,</span><br><span class="line">            &quot;When he himself might his quietus make&quot;,</span><br><span class="line">            &quot;With a bare bodkin? who would these fardels bear,&quot;,</span><br><span class="line">            &quot;To grunt and sweat under a weary life,&quot;,</span><br><span class="line">            &quot;But that the dread of something after death,--&quot;,</span><br><span class="line">            &quot;The undiscover&apos;d country, from whose bourn&quot;,</span><br><span class="line">            &quot;No traveller returns,--puzzles the will,&quot;,</span><br><span class="line">            &quot;And makes us rather bear those ills we have&quot;,</span><br><span class="line">            &quot;Than fly to others that we know not of?&quot;,</span><br><span class="line">            &quot;Thus conscience does make cowards of us all;&quot;,</span><br><span class="line">            &quot;And thus the native hue of resolution&quot;,</span><br><span class="line">            &quot;Is sicklied o&apos;er with the pale cast of thought;&quot;,</span><br><span class="line">            &quot;And enterprises of great pith and moment,&quot;,</span><br><span class="line">            &quot;With this regard, their currents turn awry,&quot;,</span><br><span class="line">            &quot;And lose the name of action.--Soft you now!&quot;,</span><br><span class="line">            &quot;The fair Ophelia!--Nymph, in thy orisons&quot;,</span><br><span class="line">            &quot;Be all my sins remember&apos;d.&quot;</span><br><span class="line">    &#125;;</span><br><span class="line">    public static DataSet&lt;String&gt; getDefaultTextLineDataSet(ExecutionEnvironment env) &#123;</span><br><span class="line">        return env.fromElements(WORDS);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述代码核心是将一段文本按单词切割，统计词频。flatMap()调用udf将文本切割并生成结构化数据，按单词分组后再sum。</p><p>不得不说，java写的flink task代码太长….</p><h2 id="Core-Concepts"><a href="#Core-Concepts" class="headerlink" title="Core Concepts"></a>Core Concepts</h2><p>####</p><h2 id="Flink-Case"><a href="#Flink-Case" class="headerlink" title="Flink Case"></a>Flink Case</h2><h4 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h4><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/cli.html" target="_blank" rel="noopener">使用简介</a></p><h2 id="参考资料、"><a href="#参考资料、" class="headerlink" title="参考资料、"></a>参考资料、</h2><ul><li>核心概念翻译</li></ul><p><a href="https://blog.csdn.net/xiaoping2017/article/details/79846158" target="_blank" rel="noopener">Flink架构、原理与部署测试详解</a></p><ul><li>简单的Scala Demo应用： </li></ul><p><a href="https://github.com/liguohua-bigdata/simple-flink/blob/master/book/stream/customSource/customSourceScala.md" target="_blank" rel="noopener">flink demo</a></p><ul><li>关于动态表: </li></ul><p><a href="http://www.10tiao.com/html/157/201707/2653162664/1.html" target="_blank" rel="noopener">在数据流中使用SQL查询：Apache Flink中的动态表的持续查询</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Flink在实时流处理领域越来越热，以阿里为首的企业正在投入更多的资源。在实际工作中也遇到了流处理的场景，特此学习一下。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="实时流" scheme="http://guzhenping.com/tags/%E5%AE%9E%E6%97%B6%E6%B5%81/"/>
    
      <category term="Flink" scheme="http://guzhenping.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Kylin应用篇</title>
    <link href="http://guzhenping.com/2018/10/26/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Kylin%E5%BA%94%E7%94%A8%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/10/26/大数据开发学习（Kylin应用）/</id>
    <published>2018-10-26T09:04:48.000Z</published>
    <updated>2018-11-30T02:56:21.818Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Kylin是一款处理海量数据，提供SQL和多维度分析的OLAP工具。Kylin用于处理hadoop/spark场景下大量数据的预聚合，用户可自定义数据模型用于解决超过100亿+条记录的查询。</p><a id="more"></a><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="Not-Support"><a href="#Not-Support" class="headerlink" title="Not Support"></a>Not Support</h3><ul><li><p>建表或构建模型时，请勿使用中文列名。</p></li><li><p>不支持临时创建新列的统计</p></li></ul><p>原表有两个字段a和b，通过concat进行拼接，然后做count()或者count(distinct *)。Kylin并不支持上述做法，因为无法命中相关的cube。</p><p>不能统计下述例子：</p><p>count(distinct concat(cast(a as varchar),b))</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select count(distinct concat(cast(a as varchar),b))</span><br><span class="line">from table_a</span><br><span class="line">where dt = &apos;2018-05-01&apos;</span><br></pre></td></tr></table></figure><h3 id="Cube命中"><a href="#Cube命中" class="headerlink" title="Cube命中"></a>Cube命中</h3><p>所有的SQL需要命中相关Cube才可以使用。如果不是使用姿势问题，请联系管理员构建新的Cube。</p><ul><li>在join时，需要用事实表join维度表，负责容易出现：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">No realization found for OLAPContext, CUBE_NOT_READY, CUBE_NOT_READY, CUBE_NOT_READY, MODEL_UNMATCHED_JOIN, MODEL_UNMATCHED_JOIN</span><br></pre></td></tr></table></figure><ul><li>在写子查询时，不能将事实表写在查询中，Cube可能无法命中。</li></ul><h2 id="调度脚本"><a href="#调度脚本" class="headerlink" title="调度脚本"></a>调度脚本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime, requests</span><br><span class="line"></span><br><span class="line">auth_str = <span class="string">"Basic YWRtaW46S1lMSU6="</span></span><br><span class="line">url_str = <span class="string">"http://xxxxx.com:7070/kylin/api"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">auth</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用户认证</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    url = <span class="string">"&#123;url_str&#125;/user/authentication"</span>.format(url_str=url_str)</span><br><span class="line">    payload = <span class="string">"="</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Content-Type'</span>: <span class="string">"application/x-www-form-urlencoded"</span>,</span><br><span class="line">        <span class="string">'Authorization'</span>: auth_str,</span><br><span class="line">        <span class="string">'Cache-Control'</span>: <span class="string">"no-cache"</span></span><br><span class="line">        &#125;</span><br><span class="line">    response = requests.request(<span class="string">"POST"</span>, url, data=payload, headers=headers)</span><br><span class="line">    print(response.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cube</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    获取cube信息</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    url = <span class="string">"&#123;url_str&#125;/cubes"</span>.format(url_str=url_str)</span><br><span class="line">    querystring = &#123;<span class="string">"cubeName"</span>: <span class="string">"test_join"</span>&#125;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Cache-Control'</span>: <span class="string">"no-cache"</span>,</span><br><span class="line">        <span class="string">'Authorization'</span>: auth_str</span><br><span class="line">        &#125;</span><br><span class="line">    response = requests.request(<span class="string">"GET"</span>, url, headers=headers, params=querystring)</span><br><span class="line">    print(response.json())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_cube</span><span class="params">(cube_name, start_date, end_date, build_type)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    构建指定cube</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    startTime 和 endTime 应该是utc时间。</span></span><br><span class="line"><span class="string">    buildType 可以是 BUILD 、 MERGE 或 REFRESH。</span></span><br><span class="line"><span class="string">        - BUILD 用于构建一个新的segment，</span></span><br><span class="line"><span class="string">        - REFRESH 用于刷新一个已有的segment，</span></span><br><span class="line"><span class="string">        - MERGE 用于合并多个已有的segment生成一个较大的segment</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    url = <span class="string">"&#123;url_str&#125;/cubes/&#123;cube_name&#125;/rebuild"</span>.format(cube_name=cube_name, url_str=url_str)</span><br><span class="line">    start_stamp = int(datetime.datetime.strptime(start_date, <span class="string">'%Y-%m-%d %H:%M:%S'</span>).timestamp() * <span class="number">1000</span>)</span><br><span class="line">    end_stamp = int(datetime.datetime.strptime(end_date, <span class="string">'%Y-%m-%d %H:%M:%S'</span>).timestamp() * <span class="number">1000</span>)</span><br><span class="line">    payload = <span class="string">"&#123;\"startTime\": %d, \"endTime\": %d, \"buildType\": \"%s\"&#125;"</span> % (start_stamp, end_stamp, build_type)</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Content-Type'</span>: <span class="string">"application/json"</span>,</span><br><span class="line">        <span class="string">'Authorization'</span>: auth_str,</span><br><span class="line">        <span class="string">'Cache-Control'</span>: <span class="string">"no-cache"</span></span><br><span class="line">        &#125;</span><br><span class="line">    response = requests.request(<span class="string">"PUT"</span>, url, data=payload, headers=headers)</span><br><span class="line">    print(response.json())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    cube_name = <span class="string">"test_api"</span></span><br><span class="line">    start_date = <span class="string">'2018-05-01 04:00:00'</span></span><br><span class="line">    end_date = <span class="string">'2018-05-01 08:00:00'</span></span><br><span class="line">    build_type = <span class="string">'BUILD'</span></span><br><span class="line">    build_cube(cube_name, start_date, end_date, build_type)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;Kylin是一款处理海量数据，提供SQL和多维度分析的OLAP工具。Kylin用于处理hadoop/spark场景下大量数据的预聚合，用户可自定义数据模型用于解决超过100亿+条记录的查询。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Kylin" scheme="http://guzhenping.com/tags/Kylin/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://guzhenping.com/2018/08/14/README/"/>
    <id>http://guzhenping.com/2018/08/14/README/</id>
    <published>2018-08-13T16:00:00.000Z</published>
    <updated>2018-11-28T06:32:44.200Z</updated>
    
    <content type="html"><![CDATA[<h1 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h1><hr><p>这里是谷震平个人的博客文章，描述了关于大数据技术的一些实践结果和理论知识。包括，集群搭建、大数据工具学习、运维、学术资料整理。请记住这个域名：<a href="http://guzhenping.com">http://guzhenping.com</a>。</p><p>笔者作为一名数据仓库工程师，经历过两个数据仓库系统，一大一小。大的让笔者学到了很多，小的是笔者自己从0到1攒的。</p><p><img src="static/2017_DW_in_baixing.jpeg" alt="大的"></p><p align="center"><br>图1 基于hadoop的大数仓<br></p><p><img src="static/WechatIMG53.jpeg" alt="小的"></p><p align="center">图2 基于PG 10的小数仓</p><p>正在开启第三段职业生涯。</p><h1 id="文章列表"><a href="#文章列表" class="headerlink" title="文章列表"></a>文章列表</h1><hr><h2 id="集群搭建与运维"><a href="#集群搭建与运维" class="headerlink" title="集群搭建与运维"></a>集群搭建与运维</h2><ul><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97%EF%BC%88%E4%B8%8A%E5%8D%B7%EF%BC%89.md" target="_blank" rel="noopener">2016-11 集群搭建指南（上卷）</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97%EF%BC%88%E4%B8%AD%E5%8D%B7%EF%BC%89.md" target="_blank" rel="noopener">2016-11 集群搭建指南（中卷）</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97%EF%BC%88%E4%B8%8B%E5%8D%B7%EF%BC%89.md" target="_blank" rel="noopener">2016-11 集群搭建指南（下卷）</a><ul><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/Hadoop%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97(%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E7%AF%87" target="_blank" rel="noopener">2016-12 Hadoop学习指南（集群运维篇）</a>.md)</li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/Hadoop%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%88HA%E9%85%8D%E7%BD%AE%EF%BC%89.md" target="_blank" rel="noopener">2017-01 Hadoop学习指南（HA配置）</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/HA%E5%8D%87%E7%BA%A7%E8%BF%87%E7%A8%8B.md" target="_blank" rel="noopener">2017-01 HA升级过程</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/HA%E6%88%90%E5%8A%9F%E5%8D%87%E7%BA%A7%E7%9A%84%E6%80%BB%E7%BB%93.md" target="_blank" rel="noopener">2017-02 HA成功升级的总结</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Fabric%EF%BC%89.md" target="_blank" rel="noopener">2017-10 大数据开发学习（Fabric）</a></li></ul></li></ul><h2 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h2><ul><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%A6%82%E5%BF%B5%E7%AF%87%EF%BC%89.md" target="_blank" rel="noopener">2016-07 数据仓库学习（概念篇）</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AD%A6%E4%B9%A0%EF%BC%88ETL%EF%BC%89.md" target="_blank" rel="noopener">2017-05 数据仓库学习（ETL）</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%EF%BC%89.md" target="_blank" rel="noopener">2018-01 数据仓库学习（维度建模）</a></li></ul><h2 id="Hadoop生态圈"><a href="#Hadoop生态圈" class="headerlink" title="Hadoop生态圈"></a>Hadoop生态圈</h2><ul><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/Hadoop%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%88HDFS%E7%AF%87%EF%BC%89.md" target="_blank" rel="noopener">2016-11 Hadoop学习指南（HDFS篇）</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Hive%EF%BC%89.md" target="_blank" rel="noopener">2016-11 大数据开发学习（Hive）</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Zookeeper%EF%BC%89.md" target="_blank" rel="noopener">2016-11 大数据开发学习（ZooKeeper）</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88HBase%EF%BC%89.md" target="_blank" rel="noopener">2017-12 Hadoop学习指南（HBase篇）</a></li></ul><h2 id="OLAP-数据查询与可视化"><a href="#OLAP-数据查询与可视化" class="headerlink" title="OLAP/数据查询与可视化"></a>OLAP/数据查询与可视化</h2><ul><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Redash%EF%BC%89.md" target="_blank" rel="noopener">2017-12 大数据开发学习（Redash）</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Kylin%EF%BC%89.md" target="_blank" rel="noopener">2018-01 大数据开发学习（Kylin）</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/Redash%E4%B8%AD%E6%96%87%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3.md" target="_blank" rel="noopener">2018-01 Redash使用指南</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/Redash%20%E6%9D%83%E9%99%90%E5%88%86%E9%85%8D.md" target="_blank" rel="noopener">2018-01 Redash权限分配</a></li><li><a href="https://github.com/guzhenping/guzhenping-blog/blob/master/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Clickhouse%EF%BC%89.md" target="_blank" rel="noopener">2018-04 大数据开发学习（Clickhouse）</a></li></ul><h2 id="区块链"><a href="#区块链" class="headerlink" title="区块链"></a>区块链</h2><ul><li><a href="https://mp.weixin.qq.com/s/aw-8AmgasB20IwJHpCzxeA" target="_blank" rel="noopener">2018-07 区块链简史</a></li><li><a href="https://mp.weixin.qq.com/s/LNUPRo0fo5Qw02pP4bV2UQ" target="_blank" rel="noopener">2018-07 区块链分类</a></li></ul><p>工作关系，学习到一点区块链的知识。特地开了一个公众号进行写作，欢迎扫码关注：</p><p><img src="static/认知区块链.jpg" alt=""></p><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-89345053-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-89345053-2');</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;导读&quot;&gt;&lt;a href=&quot;#导读&quot; class=&quot;headerlink&quot; title=&quot;导读&quot;&gt;&lt;/a&gt;导读&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;这里是谷震平个人的博客文章，描述了关于大数据技术的一些实践结果和理论知识。包括，集群搭建、大数据工具学习、运维、学术资料整理。请记
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Clickhouse 高级SQL</title>
    <link href="http://guzhenping.com/2018/07/27/Clickhouse%20%E9%AB%98%E7%BA%A7SQL/"/>
    <id>http://guzhenping.com/2018/07/27/Clickhouse 高级SQL/</id>
    <published>2018-07-27T09:04:48.000Z</published>
    <updated>2018-11-30T02:43:37.103Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>测试数据集：</p><table><thead><tr><th>ca</th><th>cb</th><th>cc</th></tr></thead><tbody><tr><td>A</td><td>W</td><td>1</td></tr><tr><td>A</td><td>W</td><td>2</td></tr><tr><td>B</td><td>X</td><td>1</td></tr><tr><td>B</td><td>Z</td><td>2</td></tr><tr><td>B</td><td>Z</td><td>4</td></tr></tbody></table><h3 id="按最大-最小值-TOP1去重"><a href="#按最大-最小值-TOP1去重" class="headerlink" title="按最大/最小值/TOP1去重"></a>按最大/最小值/TOP1去重</h3><p>按ca和cb取cc最小值取值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">ca, </span><br><span class="line">cb, </span><br><span class="line">min(cc)</span><br><span class="line">from table</span><br><span class="line">group by ca, </span><br><span class="line">cb</span><br></pre></td></tr></table></figure><table><thead><tr><th>column_A</th><th>column_B</th><th>column_C</th></tr></thead><tbody><tr><td>A</td><td>W</td><td>1</td></tr><tr><td>B</td><td>X</td><td>1</td></tr><tr><td>B</td><td>Z</td><td>2</td></tr></tbody></table><p>取最大，则使用max()函数。取一组值的TOP1也是同理。</p><h3 id="按列合并多行（多-gt-少）"><a href="#按列合并多行（多-gt-少）" class="headerlink" title="按列合并多行（多-&gt;少）"></a>按列合并多行（多-&gt;少）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">ca, </span><br><span class="line">cb, </span><br><span class="line">groupUniqArray(cc)</span><br><span class="line">from table</span><br><span class="line">group by ca, cb</span><br></pre></td></tr></table></figure><table><thead><tr><th>column_A</th><th>column_B</th><th>column_C</th></tr></thead><tbody><tr><td>A</td><td>W</td><td>[2,1]</td></tr><tr><td>B</td><td>X</td><td>[1]</td></tr><tr><td>B</td><td>Z</td><td>[4,2]</td></tr></tbody></table><p>这是一种分组的概念，将相同的数据放在一起，需要被统计的数据放在array数据类型中。统计array，可以使用length()，获取数组长度，相当于分组count</p><p>此外，groupArray也可以满足需求。</p><h3 id="分组排序后取TopN"><a href="#分组排序后取TopN" class="headerlink" title="分组排序后取TopN"></a>分组排序后取TopN</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">SELECT ca,</span><br><span class="line">       groupArray(1)(cc) </span><br><span class="line">FROM</span><br><span class="line">  ( SELECT *</span><br><span class="line">   FROM table</span><br><span class="line">   ORDER BY ca,</span><br><span class="line">            cb,</span><br><span class="line">            cc )</span><br><span class="line">GROUP BY ca</span><br></pre></td></tr></table></figure><table><thead><tr><th>column_A</th><th>column_B</th><th>column_C</th></tr></thead><tbody><tr><td>A</td><td>W</td><td>[1]</td></tr><tr><td>B</td><td>X</td><td>[1]</td></tr><tr><td>B</td><td>Z</td><td>[2]</td></tr></tbody></table><p>以上是按cc列的值升序去top1。通过order by x改变顺序，再用groupArry(N)()函数处理获取top值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;测试数据集：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ca&lt;/th&gt;
&lt;th&gt;cb&lt;/th&gt;
&lt;th&gt;cc&lt;/th&gt;
&lt;/
      
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Clickhouse" scheme="http://guzhenping.com/tags/Clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>Redash Model源码分析</title>
    <link href="http://guzhenping.com/2018/07/26/Redash%20Model%E6%95%B4%E7%90%86%E5%88%86%E6%9E%90/"/>
    <id>http://guzhenping.com/2018/07/26/Redash Model整理分析/</id>
    <published>2018-07-26T09:04:48.000Z</published>
    <updated>2018-11-30T02:43:09.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>掌握Redash执行原理，对于深度的二次开发至关重要。</p><a id="more"></a><h2 id="query功能"><a href="#query功能" class="headerlink" title="query功能"></a>query功能</h2><p>SQL查询是Redash的核心功能之一。通常情况下，用户在前端会生成如下参数：</p><ul><li>query.data_source：下列列表中的数据源，必须选</li><li>parameter_values：用户自定义的参数值，可无</li><li>query.query_text：用户自定义的SQL文本，必须写</li><li>query.id：前端自动生成的查询id，系统生成</li></ul><p>在上述条件具备后，将会调用Redash API： <code>/api/queries/</code>进行任务提交。</p><p>所有任务将会被redash.handlers.query_result.run_query()方法接受处理。此处将进行参数校验，语法解析，任务传输等方面的处理。</p><p>多数的查询场景，先将SQL文本hash后，去缓存队列和后台数据库（Redash的Postgre）进行寻找，找不到再发给celery让worker执行该SQL。</p><p>负责具体执行：redash.models.QueryResult()类实例 和redash.tasks.queries.enqueue_query()方法。前者：负责查现有的结果；后者：立即执行相关SQL语句。</p><p>在enqueue_query()方法里，先连Redis,将任务加入队列，持续监听查询结果。</p><p>核心代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">while try_count &lt; 5:</span><br><span class="line">    try_count += 1</span><br><span class="line"></span><br><span class="line">    pipe = redis_connection.pipeline()</span><br><span class="line">    try:</span><br><span class="line">        pipe.watch(_job_lock_id(query_hash, data_source.id))</span><br><span class="line">        job_id = pipe.get(_job_lock_id(query_hash, data_source.id))</span><br><span class="line">        if job_id:</span><br><span class="line">            logging.info(&quot;[%s] Found existing job: %s&quot;, query_hash, job_id)</span><br><span class="line"></span><br><span class="line">            job = QueryTask(job_id=job_id)</span><br><span class="line"></span><br><span class="line">            if job.ready():</span><br><span class="line">                logging.info(&quot;[%s] job found is ready (%s), removing lock&quot;, query_hash, job.celery_status)</span><br><span class="line">                redis_connection.delete(_job_lock_id(query_hash, data_source.id))</span><br><span class="line">                job = None</span><br><span class="line"></span><br><span class="line">        if not job:</span><br><span class="line">            pipe.multi()</span><br><span class="line"></span><br><span class="line">            time_limit = None</span><br><span class="line"></span><br><span class="line">            if scheduled_query:</span><br><span class="line">                queue_name = data_source.scheduled_queue_name</span><br><span class="line">                scheduled_query_id = scheduled_query.id</span><br><span class="line">            else:</span><br><span class="line">                queue_name = data_source.queue_name</span><br><span class="line">                scheduled_query_id = None</span><br><span class="line">                time_limit = settings.ADHOC_QUERY_TIME_LIMIT</span><br><span class="line"></span><br><span class="line">            result = execute_query.apply_async(args=(query, data_source.id, metadata, user_id, scheduled_query_id),</span><br><span class="line">                                               queue=queue_name,</span><br><span class="line">                                               time_limit=time_limit)</span><br><span class="line">            job = QueryTask(async_result=result)</span><br><span class="line">            tracker = QueryTaskTracker.create(</span><br><span class="line">                result.id, &apos;created&apos;, query_hash, data_source.id,</span><br><span class="line">                scheduled_query is not None, metadata)</span><br><span class="line">            tracker.save(connection=pipe)</span><br><span class="line"></span><br><span class="line">            logging.info(&quot;[%s] Created new job: %s&quot;, query_hash, job.id)</span><br><span class="line">            pipe.set(_job_lock_id(query_hash, data_source.id), job.id, settings.JOB_EXPIRY_TIME)</span><br><span class="line">            pipe.execute()</span><br><span class="line">        break</span><br><span class="line"></span><br><span class="line">    except redis.WatchError:</span><br><span class="line">        continue</span><br></pre></td></tr></table></figure><p>以上是Query功能的分析。主要组件：</p><ul><li>celery分布式框架</li><li>redis缓存和任务队列</li><li>postgre后台DB。</li></ul><p>相关功能的设计比较中规中矩，报错异常的检测较少，后续值得优化。</p><h2 id="Model层"><a href="#Model层" class="headerlink" title="Model层"></a>Model层</h2><p>model层是Redash的设计核心，每个类对应后台数据库一张表和一个功能。</p><ul><li>access_permission, 权限表</li><li>alembic_version,版本号         </li><li>alert_subscriptions，报警描述</li><li>alerts，报警列表         </li><li>api_keys, api key管理</li><li>changes，升级改动</li><li>dashboards，报表存储</li><li>data_source_groups,每个组对应的数据源</li><li>data_sources，所有数据源</li><li>events，后台日志</li><li>groups，所有分组列表</li><li>notification_destinations，报警的模板和目的地</li><li>organizations， 组织         </li><li>queries，所有queris         </li><li>query_results，query的结果，另一种缓存</li><li>query_snippets，SQL的评论         </li><li>users, 用户已列表</li><li>visualizations，可视化图表存储</li><li>widgets，可视化控件</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;掌握Redash执行原理，对于深度的二次开发至关重要。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="可视化" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="Redash" scheme="http://guzhenping.com/tags/Redash/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发学习（Kudu）</title>
    <link href="http://guzhenping.com/2018/05/25/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Kudu%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/05/25/大数据开发学习（Kudu）/</id>
    <published>2018-05-25T09:04:48.000Z</published>
    <updated>2018-11-30T02:57:14.186Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>kudu，作为OLAP工具十分强劲。本文记录了笔者对其学习和使用的过程。</p><p>注明：这篇文章是研究Kudu在OLAP的场景，不准备讨论其他Case。</p><h2 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h2><p>最新版安装：<a href="https://kudu.apache.org/docs/installation.html" target="_blank" rel="noopener">Installing Apache Kudu</a></p><p>推荐使用VM进行快速体验，<a href="https://kudu.apache.org/docs/quickstart.html" target="_blank" rel="noopener">Apache Kudu Quickstart</a>。</p><p>当需要使用kudu client功能时，需要安装：kudu-client、kudu-client-devel这两个C++的库。</p><h2 id="常规使用"><a href="#常规使用" class="headerlink" title="常规使用"></a>常规使用</h2><p>教学Demo参见：<a href="https://kudu.apache.org/docs/quickstart.html" target="_blank" rel="noopener">Apache Kudu Quickstart</a></p><p>以上内容是对静态数据的使用，如果是一条实时的数据流，则采用kudu API的方式。需要去做定制开发。</p><h2 id="架构体系"><a href="#架构体系" class="headerlink" title="架构体系"></a>架构体系</h2><p>kudu架构：<br><img src="static/kudu/kudu.png" alt=""></p><p>关于运行原理的文章，推荐：<a href="http://www.nosqlnotes.com/technotes/kudu-design/" target="_blank" rel="noopener">Kudu设计原理初探</a></p><h2 id="OLAP服务"><a href="#OLAP服务" class="headerlink" title="OLAP服务"></a>OLAP服务</h2><p>作为OLAP服务，ETL环节的处理至关重要。以下列举了小米公司的场景：<br><img src="static/kudu/kudu-etl1.jpg" alt=""></p><p>对比：<br><img src="static/kudu/kudu-etl2.jpg" alt=""></p><p>可以看出，除了日志数据外，线上业务数据都可以实时同步到kudu里。基于kudu对外提供OLAP服务，数据的实时性非常可观。</p><p>在小米，采用impala作为查询（计算）引擎，但是网上也有presto on kudu的组件可供选型（<a href="https://github.com/MartinWeindel/presto-kudu" target="_blank" rel="noopener">传送门</a>）。</p><h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>微店：</p><ul><li><a href="https://datascience.weidian-inc.com/kudu_impala/" target="_blank" rel="noopener">Kudu+Impala介绍</a><ul><li><a href="https://datascience.weidian-inc.com/kudu_schema_design/" target="_blank" rel="noopener">Kudu的Schema表结构设计</a></li></ul></li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>运行原理：<a href="https://blog.csdn.net/cdxxx5708/article/details/79074763" target="_blank" rel="noopener">kudu内部机制</a></p><p>关于Kudu的介绍： <a href="https://kudu.apache.org/docs/#_kudu_impala_integration_features" target="_blank" rel="noopener">Introducing Apache Kudu</a> </p><p>基于Kudu搭建OLAP工具：<a href="https://myslide.cn/slides/3584?vertical=1" target="_blank" rel="noopener">小米：使用Kudu搭建OLAP服务</a></p><p>基于Kudu的实际应用：<a href="http://www.infoq.com/cn/articles/spark-streaming-kudu-impala" target="_blank" rel="noopener">使用Spark Streaming + Kudu + Impala构建一个预测引擎</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;kudu，作为OLAP工具十分强劲。本文记录了笔者对其学习和使用的过程。&lt;/p&gt;
&lt;p&gt;注明：这篇文章是研究Kudu在OLAP的场景，不准备
      
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Kudu" scheme="http://guzhenping.com/tags/Kudu/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发学习（Clickhouse）</title>
    <link href="http://guzhenping.com/2018/05/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Clickhouse%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/05/24/大数据开发学习（Clickhouse）/</id>
    <published>2018-05-24T09:04:48.000Z</published>
    <updated>2018-11-30T02:10:54.811Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>1年前，用过Greenplum，这是第一次接触MPP结构的OLAP系统。今天市面上常见的MPP架构工具，还有Clickhouse和Palo等。(当然，SQL on Hadoop体系的Presto和Impala也算是MPP结构，只是数据存储方面没有自己的东西，都是依赖hdfs,mysql等。)</p><p>工作需要，对Clickhouse进行学习。<br><a id="more"></a></p><h2 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h2><p>Ubuntu上比较好安装，但是一般公司用的服务器都是Centos。这里只讨论在Centos 7的安装方式，centos 6请在Altinity公司提供的下载界面中自行寻找。</p><h3 id="安装命令"><a href="#安装命令" class="headerlink" title="安装命令"></a>安装命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">如果是装54362版本的包，其余所有依赖都需要是一致的。</span><br><span class="line"></span><br><span class="line">准备源依赖，由Altinity公司提供：</span><br><span class="line">curl -s https://packagecloud.io/install/repositories/Altinity/clickhouse/script.rpm.sh | sudo bash</span><br><span class="line"></span><br><span class="line">server-common:</span><br><span class="line">sudo yum install clickhouse-server-common-1.1.54362-1.el7.x86_64</span><br><span class="line">sudo yum install clickhouse-server-common-1.1.54380-1.el7.x86_64</span><br><span class="line"></span><br><span class="line">server:</span><br><span class="line">sudo yum install clickhouse-server-1.1.54362-1.el7.x86_64</span><br><span class="line">sudo yum install clickhouse-server-1.1.54380-1.el7.x86_64</span><br><span class="line"></span><br><span class="line">client：</span><br><span class="line">sudo yum install clickhouse-client-1.1.54362-1.el7.x86_64</span><br><span class="line">sudo yum install clickhouse-client-1.1.54380-1.el7.x86_64</span><br></pre></td></tr></table></figure><p>关于Altinity公司的其他版本，可访问<a href="https://packagecloud.io/Altinity/clickhouse" target="_blank" rel="noopener">这里下载</a>。</p><p>以上安装如有疑问，可以使用下方安装方式：</p><ul><li><a href="https://github.com/red-soft-ru/clickhouse-rpm" target="_blank" rel="noopener">https://github.com/red-soft-ru/clickhouse-rpm</a></li></ul><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server端：</span><br><span class="line">sudo /etc/rc.d/init.d/clickhouse-server start</span><br><span class="line"></span><br><span class="line">client端：</span><br><span class="line">clickhouse-client</span><br></pre></td></tr></table></figure><h2 id="使用经验"><a href="#使用经验" class="headerlink" title="使用经验"></a>使用经验</h2><h3 id="常用SQL"><a href="#常用SQL" class="headerlink" title="常用SQL"></a>常用SQL</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-- 查看集群情况</span><br><span class="line">select * from system.clusters;</span><br><span class="line"></span><br><span class="line">-- 查看分区情况</span><br><span class="line">select </span><br><span class="line">partition,</span><br><span class="line">name,</span><br><span class="line">rows</span><br><span class="line">from system.parts;</span><br></pre></td></tr></table></figure><h3 id="MergeTree"><a href="#MergeTree" class="headerlink" title="MergeTree"></a>MergeTree</h3><p>选择engine时，尽量用merge tree.MergeTree引擎支持以主键和日期作为索引，提供实时更新数据的可能性。这是Clickhouse中最先进的表引擎。</p><p>数据表将数据分割为小的索引块进行处理。每个索引块之间依照主键排序。每个索引块记录了指定的开始日期和结束日期。插入数据时，MergeTree会对数据进行排序，以保证存储在索引块中的数据有序。当写入新数据时，会放在新的文件夹下，索引块之间的合并过程会在系统后台定期自动执行。MergeTree引擎会选择几个相邻的索引块进行合并，然后对二者合并排序。</p><p><img src="static/clickhouse/mergetree1.png" alt=""></p><p><img src="static/clickhouse/mergetree2.png" alt=""></p><p><img src="static/clickhouse/mergetree3.png" alt=""></p><p><img src="static/clickhouse/mergetree4.png" alt=""></p><p><img src="static/clickhouse/mergetree5.png" alt=""></p><p><img src="static/clickhouse/mergetree6.png" alt=""></p><p><img src="static/clickhouse/mergetree7.png" alt=""></p><h3 id="Distributed"><a href="#Distributed" class="headerlink" title="Distributed"></a>Distributed</h3><p>相当于数据库的视图，并不存储数据，而是用来做分布式的写入和查询，与其他引擎结合使用。</p><p>一种集群拓扑结构：<br><img src="static/clickhouse/distributed.jpg" alt=""></p><h2 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题"></a>实际问题</h2><ol><li>关键字大小写敏感，对强转支持不太好。</li><li>特性上不支持事务，不支持update/delete。</li><li>关于分布式<br><img src="static/clickhouse/question_clickhouse1.png" alt=""></li><li>在分布式表上执行count()时，发现结果不一致。</li></ol><p>暂无解决办法。问题:<a href="https://github.com/yandex/ClickHouse/issues/1443" target="_blank" rel="noopener">https://github.com/yandex/ClickHouse/issues/1443</a></p><ol start="5"><li>查询时，内存超出限制：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Progress: 4.84 million rows, 42.70 MB (45.34 million rows/s., 399.59 MB/s.)  87%</span><br><span class="line"></span><br><span class="line">Received exception from server:</span><br><span class="line">Code: 241. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Memory limit (for query) exceeded: would use 12.15 GiB (attempt to allocate chunk of 4294967296 bytes), maximum: 9.31 GiB.</span><br><span class="line"></span><br><span class="line">0 rows in set. Elapsed: 48.023 sec. Processed 4.84 million rows, 42.70 MB (100.89 thousand rows/s., 889.21 KB/s.)</span><br><span class="line"></span><br><span class="line">Progress: 4.89 million rows, 43.81 MB (23.23 thousand rows/s., 208.09 KB/s.)  88%</span><br><span class="line"></span><br><span class="line">Received exception from server:</span><br><span class="line">Code: 241. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Memory limit (for query) exceeded: would use 80.15 GiB (attempt to allocate chunk of 17179869184 bytes), maximum: 74.51 GiB.</span><br><span class="line"></span><br><span class="line">3668208133 rows in set. Elapsed: 235.536 sec. Processed 4.89 million rows, 43.81 MB (20.77 thousand rows/s., 186.00 KB/s.)</span><br><span class="line"></span><br><span class="line">Received exception from server (version 1.1.54362):</span><br><span class="line">Code: 241. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Memory limit (for query) exceeded: would use 96.14 GiB (attempt to allocate chunk of 17179869184 bytes), maximum: 93.13 GiB.</span><br><span class="line"></span><br><span class="line">8816716667 rows in set. Elapsed: 428.543 sec. Processed 4.89 million rows, 43.81 MB (11.41 thousand rows/s., 102.23 KB/s.)</span><br></pre></td></tr></table></figure><p>当处理的数据集超过设定的阈值以后，会触发限制，并返回已经处理好的结果。</p><p>解决办法：<br>修改max_memory_usage的值（user.xml）</p><p>不同的节点可以处理设置不同。在高并发分流的时候，尽量把query分摊到各个机器上，否则会将某一节点资源耗尽。</p><p>这个问题导致Clickhouse的高并发特性很差。在前端暴露端口时，不能单独暴露一个host,需要做成有命名空间的方式，或者有个query平衡器的机制。</p><p>解决办法：<br>就上述问题，发现一个公司采用如下架构：</p><p><img src="static/clickhouse/clickhouse_rebalance.jpg" alt=""></p><p>即：域名轮询。</p><h2 id="SQL-实战"><a href="#SQL-实战" class="headerlink" title="SQL 实战"></a>SQL 实战</h2><p>第一步建表：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table test_analysis (created_at DateTime, dt Date,  user String,  page_id String ) ENGINE=MergeTree(dt, (user, dt), 8192);</span><br></pre></td></tr></table></figure></p><p>插入测试数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert into table test_analysis Format Values</span><br><span class="line">(&apos;2018-4-24 18:45&apos;,&apos;2018-4-24&apos;,&apos;A&apos;,&apos;首页&apos;),</span><br><span class="line">(&apos;2018-4-24 18:46&apos;,&apos;2018-4-24&apos;,&apos;A&apos;,&apos;购物车&apos;),</span><br><span class="line">(&apos;2018-4-24 18:45&apos;,&apos;2018-4-24,&apos;B&apos;,&apos;首页&apos;),</span><br><span class="line">(&apos;2018-4-24 18:48&apos;,&apos;2018-4-24&apos;,&apos;B&apos;,&apos;商品详情&apos;),</span><br><span class="line">(&apos;2018-4-24 18:49&apos;,&apos;2018-4-24&apos;,&apos;B&apos;,&apos;购物车&apos;),</span><br><span class="line">(&apos;2018-4-24 18:46&apos;,&apos;2018-4-24&apos;,&apos;C&apos;,&apos;商品详情&apos;);</span><br></pre></td></tr></table></figure><p>第二步，建立模型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">SELECT `user`,</span><br><span class="line">       created_at,</span><br><span class="line">       page_id,</span><br><span class="line">       gap1/60 AS &quot;与第一个动作的间隔时间&quot;,</span><br><span class="line">       if(gap1 == 0, 0, runningDifference(gap1)/60) AS &quot;与上一个动作的间隔时间&quot;</span><br><span class="line">FROM</span><br><span class="line">  (SELECT `user`,</span><br><span class="line">          created_at,</span><br><span class="line">          fist_created_at,</span><br><span class="line">          page_id,</span><br><span class="line">          created_at-fist_created_at AS gap1</span><br><span class="line">   FROM test_analysis ANY</span><br><span class="line">   LEFT JOIN</span><br><span class="line">     (SELECT `user` ,</span><br><span class="line">             min(created_at) AS fist_created_at</span><br><span class="line">      FROM test_analysis</span><br><span class="line">      GROUP BY `user`) using(`user`)) AS t</span><br></pre></td></tr></table></figure><p>获得的结果：</p><p><a href="http://www.clickhouse.com.cn/topic/5adf0d0a9d28dfde2ddc5fb2" target="_blank" rel="noopener">clickhouse 实战</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><h3 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h3><p><a href="https://clickhouse.yandex/docs/en/single/#introduction" target="_blank" rel="noopener">What is ClickHouse</a></p><p>官方写的体系结构文章：<a href="https://clickhouse.yandex/docs/en/development/architecture/" target="_blank" rel="noopener">Overview of ClickHouse architecture</a></p><p>翻译:<a href="http://www.clickhouse.com.cn/topic/5a3df7b02141c2917483557c" target="_blank" rel="noopener">ClickHouse 内部架构介绍</a></p><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p><a href="https://kuaibao.qq.com/s/20180409G06IIM00?refer=spider" target="_blank" rel="noopener">ClickHouse相关配置剖析</a></p><p><a href="http://note.abeffect.com/note/articles/2017/12/18/1513590469620.html" target="_blank" rel="noopener">ClickHouse的分布式引擎</a></p><h3 id="用户权限"><a href="#用户权限" class="headerlink" title="用户权限"></a>用户权限</h3><p><a href="https://www.jianshu.com/p/e339336e7bb9" target="_blank" rel="noopener">ClickHouse 用户名密码设置</a></p><p><a href="http://www.cnblogs.com/gomysql/p/6708796.html" target="_blank" rel="noopener">ClickHouse之访问权限控制</a></p><h3 id="引擎介绍"><a href="#引擎介绍" class="headerlink" title="引擎介绍"></a>引擎介绍</h3><p><a href="https://www.jianshu.com/p/48dbf2db2765" target="_blank" rel="noopener">ClickHouse MergeTree引擎介绍</a></p><p><a href="http://www.clickhouse.com.cn/topic/5a3e768d2141c2917483557e" target="_blank" rel="noopener">ClickHouse Distribute 引擎深度解读</a></p><p><a href="http://www.linkedkeeper.com/detail/blog.action?bid=1117" target="_blank" rel="noopener">实时大数据分析引擎ClickHouse介绍</a></p><h3 id="数据同步"><a href="#数据同步" class="headerlink" title="数据同步"></a>数据同步</h3><p>kafka-&gt;Clickhouse：<a href="http://jackpgao.github.io/2017/12/27/ClickHouse-with-Hangout/" target="_blank" rel="noopener">Hangout with ClickHouse</a></p><p>mysql-&gt;Clickhouse：<a href="http://jackpgao.github.io/2018/02/04/ClickHouse-Use-MySQL-Data/" target="_blank" rel="noopener">使用ClickHouse一键接管MySQL数据分析</a></p><h3 id="杂文"><a href="#杂文" class="headerlink" title="杂文"></a>杂文</h3><p>奶clickhouse的文章：<a href="https://zhuanlan.zhihu.com/p/33371816" target="_blank" rel="noopener">ClickHouse Beijing Meetup-数据分析领域的黑马-ClickHouse-新浪-高鹏</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;1年前，用过Greenplum，这是第一次接触MPP结构的OLAP系统。今天市面上常见的MPP架构工具，还有Clickhouse和Palo等。(当然，SQL on Hadoop体系的Presto和Impala也算是MPP结构，只是数据存储方面没有自己的东西，都是依赖hdfs,mysql等。)&lt;/p&gt;
&lt;p&gt;工作需要，对Clickhouse进行学习。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Clickhouse" scheme="http://guzhenping.com/tags/Clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库学习（概念篇）</title>
    <link href="http://guzhenping.com/2018/05/24/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%A6%82%E5%BF%B5%E7%AF%87%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/05/24/数据仓库学习（概念篇）/</id>
    <published>2018-05-24T09:04:48.000Z</published>
    <updated>2018-11-30T02:53:03.735Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>写点数据仓库（DW）的一些常用知识,架构等方面。加深基本功，保持进阶的心。</p><a id="more"></a><h2 id="典型架构"><a href="#典型架构" class="headerlink" title="典型架构"></a>典型架构</h2><p><img src="static/ScreenShot2017-01-13at11.09.58.png" alt="数据仓库系统架构"></p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul><li>数据仓库</li></ul><blockquote><p>数据仓库是一个面向主题的、集成的、非易失的（nonvolatile）、随时间变化的（time-variant）用来支持管理人员决策的数据集合。</p></blockquote><blockquote><p>—-William H.Inmon</p></blockquote><p>以上也是数据仓库区别于业务系统的4个特征。</p><ul><li>OLAP</li></ul><p>联机分析处理（On-line Analytical Processing），该概念由数据库创始人E. F. Codd于1993年提出。OLAP理事会（OLAP Council）进一步完善：OLAP是一种软件技术，它使分析人员、经理和执行官能够迅速、一致、交互的从各方面观察信息，以达到深入理解数据的目的。</p><ul><li>OLTP</li></ul><p>联机事务处理（On-line Transaction Processing）</p><ul><li>ODS </li></ul><p>操作数据存储（Operational Data Store, 简称ODS）。在数据仓库系统中，ODS存储了原始数据经过集成统一后的数据。</p><ul><li>DW</li></ul><p>数据仓库（Data Warehouse，简称DW）。在数据仓库系统中，DW数据库存储了整个企业的所有历史数据，是狭义上的数据仓库。DW数据库需要满足企业数据分析的各种需求，以及各个部门建设数据集市的需求，通常存储企业的基础数据和通用数据。</p><ul><li>数据集市(Data Market)</li></ul><p>数据集市是指针对特定部门和主题的小型数据仓库，数据从DW中获取。</p><ul><li>ETL</li></ul><p>ETL（Extract，Transform，Load）表示抽取、转换和装载，数据从多个同构或异构的数据源抽取出来，经过自定义的转换操作，最终装载进入目标表的过程叫做一次ETL。ETL是数据进入ODS、DW、Data Market的主要方式。</p><h2 id="建模方法"><a href="#建模方法" class="headerlink" title="建模方法"></a>建模方法</h2><ul><li>维度建模法（Dimensional Modeling）</li></ul><p>Ralph Kimball主张，</p><ul><li>实体-关系建模（Entity-Relationship Medeling）</li></ul><p>实体-关系建模也叫做第三范式建模（Third Normal Form, 3NF）,是William H.Inmon主张的一种数据仓库建模方法。</p><h2 id="数据与建模"><a href="#数据与建模" class="headerlink" title="数据与建模"></a>数据与建模</h2><p><a href="https://www.zhihu.com/question/19955124" target="_blank" rel="noopener">OLAP中roll-up和drill-down和slicing？</a></p><p><img src="static/olap.jpg" alt=""><br>真正好的建模，应该契合这些功能转换。见过Tableau做的很好，但不算是OLAP系统自带的功能。</p><h2 id="数仓进化史"><a href="#数仓进化史" class="headerlink" title="数仓进化史"></a>数仓进化史</h2><p>比较推荐这篇文章：<a href="http://lxw1234.com/archives/2016/03/624.htm" target="_blank" rel="noopener">从数据仓库到大数据，数据平台这25年是怎样进化的？</a></p><p>在架构方面，偏理论一点的文章：<a href="http://www.clickhouse.com.cn/topic/5ae13a599d28dfde2ddc5fc5#5aea82689d28dfde2ddc6014" target="_blank" rel="noopener">大数据分析的下一代架构–IOTA</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://segmentfault.com/a/1190000002672666" target="_blank" rel="noopener">hadoop HDFS常用文件操作命令</a></li><li><a href="http://book.51cto.com/art/201409/452359.htm" target="_blank" rel="noopener">HDFS 文件操作命令</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;写点数据仓库（DW）的一些常用知识,架构等方面。加深基本功，保持进阶的心。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="数据仓库" scheme="http://guzhenping.com/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Redash权限管理</title>
    <link href="http://guzhenping.com/2018/05/21/Redash%20%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/"/>
    <id>http://guzhenping.com/2018/05/21/Redash 权限管理/</id>
    <published>2018-05-21T09:04:48.000Z</published>
    <updated>2018-11-30T02:42:24.499Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Reash是一个数据查询平台，必定会涉及权限管理。主要由3个概念：组（group）,用户（user）， 数据源（data source）。</p><p>group在最上层，一个group对应多个user 和 data source。反之也可行，但不利于权限的管理。</p><p>权限功能主要基于组（group）和所属数据源(data source)来控制。一个用户（user）必须属于一个或多个组，当新用户进入时，该用户默认在“default组”。在新增一个数据源时，该数据源默认归属于”default组”。</p><a id="more"></a><h2 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h2><p>数据源有两个权限设置：</p><ul><li>Full access，即：该组的用户可操作被保存的查询（可以改SQL源码），创建一个新的查询</li><li>View Only access，即：该组的用户只能阅读被保存的查询（看不到SQL源码）及其结果</li></ul><p>对于不需要写SQL取结果的用户群来说，应该与需要写SQL取结果的用户群体区分在不同组（group）中。比如：基础架构查看报表组，基础架构制作报表组。</p><p>如果想对一个组的用户做table级别的查询限制，官方提供的方案：</p><blockquote><p>The idea is to leverage your database’s security model and hence create a user with access to the tables/columns you want to give access to. Create a data source that’s using this user and then associate it with a group of users who need this level of access.</p></blockquote><p>翻译过来：在建立数据源时，配置一个带有权限控制的数据库用户即可。比如，连接mysql时，用一个只能查询测试db/table的用户名进行连接。把这个数据源赋给某个组，然后该组的所有用户，只能看到这个数据源里的测试db/table。</p><h2 id="组"><a href="#组" class="headerlink" title="组"></a>组</h2><p>组可设置的权限有：</p><ul><li>admin/super_admin,管理员/超级管理员，可用所用功能</li><li>create_dashboard,创建dashboard</li><li>create_query,创建SQL查询</li><li>edit_dashboard,编辑自己/别人的dashboard</li><li>edit_query,编辑自己/别人的SQL查询</li><li>view_query,查看已经存在的SQL</li><li>view_source,查看SQL源码</li><li>execute_query,执行SQL</li><li>list_users,看到所有用户</li><li>schedule_query,设置定时刷新</li><li>list_dashboards,看到所用的dashboard</li><li>list_alerts,看到所有的提醒任务</li><li>list_data_sources,看到所有的数据源</li></ul><p>将以上权限赋给不同的组，每个组的用户就可以实现不同的功能。Redash不强调对用户做太多的权限控制，因为一个用户必须要归属于一个组。所以，对组做现在即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Reash是一个数据查询平台，必定会涉及权限管理。主要由3个概念：组（group）,用户（user）， 数据源（data source）。&lt;/p&gt;
&lt;p&gt;group在最上层，一个group对应多个user 和 data source。反之也可行，但不利于权限的管理。&lt;/p&gt;
&lt;p&gt;权限功能主要基于组（group）和所属数据源(data source)来控制。一个用户（user）必须属于一个或多个组，当新用户进入时，该用户默认在“default组”。在新增一个数据源时，该数据源默认归属于”default组”。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="可视化" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="Redash" scheme="http://guzhenping.com/tags/Redash/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://guzhenping.com/2018/05/16/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/"/>
    <id>http://guzhenping.com/2018/05/16/分布式算法设计/</id>
    <published>2018-05-15T16:00:00.000Z</published>
    <updated>2018-11-28T06:32:44.102Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分布式算法设计"><a href="#分布式算法设计" class="headerlink" title="分布式算法设计"></a>分布式算法设计</h1><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在分布式存储领域有两种常见的算法：Paxos和Raft。因为CAP原理在，所以在分布式算法的选择上有很多的考量。</p><h2 id="Paxos算法"><a href="#Paxos算法" class="headerlink" title="Paxos算法"></a>Paxos算法</h2><ul><li>Paxos</li><li>一致性哈希</li></ul><h2 id="Raft算法"><a href="#Raft算法" class="headerlink" title="Raft算法"></a>Raft算法</h2><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://timyang.net/distributed/paxos-scenarios/" target="_blank" rel="noopener">Paxos在大型系统中常见的应用场景</a></li><li><a href="http://www.jdon.com/artichect/paxos.html" target="_blank" rel="noopener">分布式系统Paxos算法</a></li><li><a href="http://36kr.com/p/5048221.html" target="_blank" rel="noopener">从一到N，掘金区块——区块链行业研究报告</a></li><li><a href="http://wenku.baidu.com/view/f26c8d916bec0975f465e236.html" target="_blank" rel="noopener">比特币：一种点对点的电子现金系统</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;分布式算法设计&quot;&gt;&lt;a href=&quot;#分布式算法设计&quot; class=&quot;headerlink&quot; title=&quot;分布式算法设计&quot;&gt;&lt;/a&gt;分布式算法设计&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://guzhenping.com/2018/05/16/OLAP%E5%B7%A5%E5%85%B7%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6/"/>
    <id>http://guzhenping.com/2018/05/16/OLAP工具的关键技术研究/</id>
    <published>2018-05-15T16:00:00.000Z</published>
    <updated>2018-11-28T06:32:44.102Z</updated>
    
    <content type="html"><![CDATA[<h1 id="OLAP工具的关键技术研究"><a href="#OLAP工具的关键技术研究" class="headerlink" title="OLAP工具的关键技术研究"></a>OLAP工具的关键技术研究</h1><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>OLAP工具种类繁多，所用技术都有独到之处。这里一一研究。</p><h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><p>这里总结和计算领域相关的技术点。</p><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p><a href="http://mysql.taobao.org/monthly/2017/01/06/" target="_blank" rel="noopener">PgSQL · 引擎介绍 · 向量化执行引擎简介</a></p><h2 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h2><p>这里总结和存储相关的技术点。</p><h3 id="Bitmap"><a href="#Bitmap" class="headerlink" title="Bitmap"></a>Bitmap</h3><p>Bit即比特，是目前计算机系统里边数据的最小单位，8个bit即为一个Byte。一个bit的值，或者是0，或者是1；也就是说一个bit能存储的最多信息是2。</p><p>Bitmap可以理解为通过一个bit数组来存储特定数据的一种数据结构；由于bit是数据的最小单位，所以这种数据结构往往是非常节省存储空间。比如一个公司有8个员工，现在需要记录公司的考勤记录，传统的方案是记录下每天正常考勤的员工的ID列表，比如2012-01-01:[1,2,3,4,5,6,7,8]。假如员工ID采用byte数据类型，则保存每天的考勤记录需要N个byte，其中N是当天考勤的总人数。另一种方案则是构造一个8bit（01110011）的数组，将这8个员工跟员工号分别映射到这8个位置，如果当天正常考勤了，则将对应的这个位置置为1，否则置为0；这样可以每天采用恒定的1个byte即可保存当天的考勤记录。</p><p>综上所述，Bitmap节省大量的存储空间，因此可以被一次性加载到内存中。</p><p><a href="http://www.infoq.com/cn/articles/the-secret-of-bitmap" target="_blank" rel="noopener">Bitmap的秘密</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;OLAP工具的关键技术研究&quot;&gt;&lt;a href=&quot;#OLAP工具的关键技术研究&quot; class=&quot;headerlink&quot; title=&quot;OLAP工具的关键技术研究&quot;&gt;&lt;/a&gt;OLAP工具的关键技术研究&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://guzhenping.com/2018/05/16/%E5%88%86%E5%B8%83%E5%BC%8F%E6%A0%B8%E5%BF%83%E8%AE%BE%E8%AE%A1/"/>
    <id>http://guzhenping.com/2018/05/16/分布式核心设计/</id>
    <published>2018-05-15T16:00:00.000Z</published>
    <updated>2018-11-28T06:32:44.102Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分布式核心设计"><a href="#分布式核心设计" class="headerlink" title="分布式核心设计"></a>分布式核心设计</h1><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇介绍分布式的一些核心设计知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。</p><h2 id="常用算法"><a href="#常用算法" class="headerlink" title="常用算法"></a>常用算法</h2><ul><li>Paxos</li><li>一致性哈希</li></ul><h2 id="从区块链看分布式核心设计"><a href="#从区块链看分布式核心设计" class="headerlink" title="从区块链看分布式核心设计"></a>从区块链看分布式核心设计</h2><p>此处以Bitcoin为例进行相关说明。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://timyang.net/distributed/paxos-scenarios/" target="_blank" rel="noopener">Paxos在大型系统中常见的应用场景</a></li><li><a href="http://www.jdon.com/artichect/paxos.html" target="_blank" rel="noopener">分布式系统Paxos算法</a></li><li><a href="http://36kr.com/p/5048221.html" target="_blank" rel="noopener">从一到N，掘金区块——区块链行业研究报告</a></li><li><a href="http://wenku.baidu.com/view/f26c8d916bec0975f465e236.html" target="_blank" rel="noopener">比特币：一种点对点的电子现金系统</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;分布式核心设计&quot;&gt;&lt;a href=&quot;#分布式核心设计&quot; class=&quot;headerlink&quot; title=&quot;分布式核心设计&quot;&gt;&lt;/a&gt;分布式核心设计&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://guzhenping.com/2018/05/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Palo%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/05/16/大数据开发学习（Palo）/</id>
    <published>2018-05-15T16:00:00.000Z</published>
    <updated>2018-11-28T06:32:44.110Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Palo介绍"><a href="#Palo介绍" class="headerlink" title="Palo介绍"></a>Palo介绍</h2><p>Palo是2017年由百度开源的一款基于MPP架构的、对SQL友好的的数据仓储，主要由于报表和分析。</p><blockquote><p>Palo is an MPP-based interactive SQL data warehousing for reporting and analysis. Palo mainly integrates the technology of Google Mesa and Apache Impala.</p></blockquote><p>PALO = Mesa的存储引擎 + Impala查询引擎</p><p>接触Palo主要是为了调研OLAP的工具。截止目前(2018-04-12 11:24:00)，Palo在github上的star数为658。</p><h2 id="架构体系"><a href="#架构体系" class="headerlink" title="架构体系"></a>架构体系</h2><p><img src="static/palo/palo_architecture.jpg" alt=""></p><h2 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h2><p>导入数据部分：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">-- 一个demo, 在mysql执行。 broker_name需要自己提前定义</span><br><span class="line"> LOAD LABEL test_palo.gzp_2</span><br><span class="line"> (</span><br><span class="line"> DATA INFILE(&quot;hdfs://nanenode1:x020/user/hadoop/xxx/test_happysql&quot;)</span><br><span class="line"> INTO TABLE `test_happysql`</span><br><span class="line"> COLUMNS TERMINATED BY &quot;,&quot;</span><br><span class="line"> )</span><br><span class="line">WITH BROKER broker_test (&quot;username&quot;=&quot;haxxx&quot;, &quot;password&quot;=&quot;&quot;)</span><br></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>网络相关参数解释：<br><img src="static/palo/palo_network_con.png" alt=""></p><h2 id="常用SQL"><a href="#常用SQL" class="headerlink" title="常用SQL"></a>常用SQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-- 查看 BE 状态</span><br><span class="line">show proc &apos;/backends&apos;</span><br><span class="line"></span><br><span class="line">-- 查看 FE 状态</span><br><span class="line">show proc &apos;/frontends&apos;</span><br></pre></td></tr></table></figure><h2 id="SQL-高级用法"><a href="#SQL-高级用法" class="headerlink" title="SQL 高级用法"></a>SQL 高级用法</h2><p><a href="https://cloud.baidu.com/doc/PALO/SQLGuide.html#.E5.86.85.E7.BD.AE.E5.87.BD.E6.95.B0" target="_blank" rel="noopener">SQL高级用法</a></p><h2 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h2><p>暂时未掌握。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>palo和mesa的关联较大，推荐阅读: <a href="http://neoremind.com/2017/09/浅谈从google-mesa到百度palo/" target="_blank" rel="noopener">《浅谈从Google Mesa到百度PALO》</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Palo介绍&quot;&gt;&lt;a href=&quot;#Palo介绍&quot; class=&quot;headerlink&quot; title=&quot;Palo介绍&quot;&gt;&lt;/a&gt;Palo介绍&lt;/h2&gt;&lt;p&gt;Palo是2017年由百度开源的一款基于MPP架构的、对SQL友好的的数据仓储，主要由于报表和分析。&lt;/p&gt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://guzhenping.com/2018/05/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%EF%BC%88Mesa%EF%BC%89/"/>
    <id>http://guzhenping.com/2018/05/16/大数据开发学习（Mesa）/</id>
    <published>2018-05-15T16:00:00.000Z</published>
    <updated>2018-11-28T06:32:44.103Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Google Mesa是一种高性能、近实时、大规模数据分析仓储体系。</p><blockquote><p>ABSTRACT</p></blockquote><p>Mesa is a highly scalable analytic data warehousing system<br>that stores critical measurement data related to Google’s<br>Internet advertising business. Mesa is designed to satisfy<br>a complex and challenging set of user and systems requirements,<br>including near real-time data ingestion and queryability,<br>as well as high availability, reliability, fault tolerance,<br>and scalability for large data and query volumes. Specifi-<br>cally, Mesa handles petabytes of data, processes millions of<br>row updates per second, and serves billions of queries that<br>fetch trillions of rows per day. Mesa is geo-replicated across<br>multiple datacenters and provides consistent and repeatable<br>query answers at low latency, even when an entire datacenter<br>fails. This paper presents the Mesa system and reports<br>the performance and scale that it achieves.</p><blockquote><p>from <strong>‘Mesa: Geo-Replicated, Near Real-Time, Scalable Data<br>Warehousing’</strong></p></blockquote><p>Mesa设计出了主要是为了广告业务，而且也有解决跨数据中心的问题，在其他场景下可能不适用。</p><p>网评，Mesa最大的贡献在其数据模型上，这一点可以找找ORCFile、Parquet进行对比学习。</p><h2 id="架构体系"><a href="#架构体系" class="headerlink" title="架构体系"></a>架构体系</h2><p>在单一数据中心（data center）部署下，主要由两个子系统构成了Mesa。</p><ul><li>Update/Maintenance Subsystem</li><li>Query Subsystem</li></ul><p>对于跨数据中心的部署，架构体系和单DC差不都。</p><h3 id="Update-Maintenance-Subsystem"><a href="#Update-Maintenance-Subsystem" class="headerlink" title="Update/Maintenance Subsystem"></a>Update/Maintenance Subsystem</h3><p>主要职责包括:</p><ol><li>加载update，并且按照存储模型保存到Mesa的物理存储上。</li><li>执行多级的compaction。</li><li>在线做schema change。</li><li>执行一些表的checksum检查。</li></ol><p><img src="static/mesa/mesa_update_subsystem.png" alt=""></p><h3 id="Query-Subsystem"><a href="#Query-Subsystem" class="headerlink" title="Query Subsystem"></a>Query Subsystem</h3><p>工作职责：</p><blockquote><p>Mesa’s query subsystem consists of query servers, illustrated<br>in Figure 5. These servers receive user queries, look<br>up table metadata, determine the set of files storing the<br>required data, perform on-the-fly aggregation of this data,<br>and convert the data from the Mesa internal format to the<br>client protocol format before sending the data back to the<br>client</p></blockquote><p><img src="static/mesa/mesa_query_subsystem.png" alt=""></p><h3 id="跨DC部署"><a href="#跨DC部署" class="headerlink" title="跨DC部署"></a>跨DC部署</h3><p>架构图：</p><p><img src="static/mesa/multi_dc_mesa.png" alt=""></p><h2 id="Mesa的数据模型"><a href="#Mesa的数据模型" class="headerlink" title="Mesa的数据模型"></a>Mesa的数据模型</h2><p>此处推荐阅读<a href="http://neoremind.com/2017/09/%E6%B5%85%E8%B0%88%E4%BB%8Egoogle-mesa%E5%88%B0%E7%99%BE%E5%BA%A6palo/" target="_blank" rel="noopener">《浅谈从Google Mesa到百度PALO》</a>关于mesa的存储部分。</p><p>暂时没有理解。需要与ORCFile和Parquet进行对比学习。</p><h2 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h2><p>暂时未掌握。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/42851.pdf" target="_blank" rel="noopener">Mesa: Geo-Replicated, Near Real-Time, Scalable Data<br>Warehousing</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Google Mesa是一种高性能、近实时、大规模数据分析仓储体系。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ABSTRACT&lt;/p&gt;
&lt;/
      
    
    </summary>
    
    
  </entry>
  
</feed>
