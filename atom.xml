<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>谷震平的博客</title>
  
  <subtitle>写点一路的风景，都很普通，主要还是留给自己。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://guzhenping.com/"/>
  <updated>2019-03-19T05:52:26.163Z</updated>
  <id>http://guzhenping.com/</id>
  
  <author>
    <name>谷震平[guzhenping, hehzyx@gmail.com]</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【置顶】个人简介</title>
    <link href="http://guzhenping.com/2048/10/24/README/"/>
    <id>http://guzhenping.com/2048/10/24/README/</id>
    <published>2048-10-24T09:04:48.000Z</published>
    <updated>2019-03-19T05:52:26.163Z</updated>
    
    <content type="html"><![CDATA[<h1 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h1><hr><p>这里是谷震平个人的博客文章，描述了关于大数据技术的一些实践结果和理论知识。包括，集群搭建、大数据工具学习、运维、学术资料整理。请记住这里：<a href="http://guzhenping.com">http://guzhenping.com</a>。</p><p>笔者作为一名数据仓库工程师，经历过两个数据仓库系统，一大一小。大的让笔者学到了很多，小的是笔者自己从0到1攒的。</p><a id="more"></a><p><img src="static/2017_DW_in_baixing.jpeg" alt="大的"></p><p align="center"><br>图1 基于hadoop的大数仓<br></p><p><img src="static/WechatIMG53.jpeg" alt="小的"></p><p align="center">图2 基于PG 10的小数仓</p><p>正在开启第三段职业生涯。</p><h1 id="文章列表"><a href="#文章列表" class="headerlink" title="文章列表"></a>文章列表</h1><hr><h2 id="集群搭建与运维"><a href="#集群搭建与运维" class="headerlink" title="集群搭建与运维"></a>集群搭建与运维</h2><ul><li>[2016-11 集群搭建指南（上卷）]</li><li>[2016-11 集群搭建指南（中卷）]</li><li>[2016-11 集群搭建指南（下卷）]</li><li>[2016-12 Hadoop学习指南（集群运维篇）]</li><li>[2017-01 Hadoop学习指南（HA配置）]</li><li>[2017-01 HA升级过程]</li><li>[2017-02 HA成功升级的总结]</li><li>[2017-10 大数据开发学习（Fabric）]</li></ul><h2 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h2><ul><li>[2016-07 数据仓库学习（概念篇）]</li><li>[2017-05 数据仓库学习（ETL）]</li><li>[2018-01 数据仓库学习（维度建模）]</li></ul><h2 id="Hadoop生态圈"><a href="#Hadoop生态圈" class="headerlink" title="Hadoop生态圈"></a>Hadoop生态圈</h2><ul><li>[2016-11 Hadoop学习指南（HDFS篇）]</li><li>[2016-11 大数据开发学习（Hive）]</li><li>[2016-11 大数据开发学习（ZooKeeper）]</li><li>[2017-12 Hadoop学习指南（HBase篇）]</li></ul><h2 id="OLAP-数据查询与可视化"><a href="#OLAP-数据查询与可视化" class="headerlink" title="OLAP/数据查询与可视化"></a>OLAP/数据查询与可视化</h2><ul><li>[2017-12 大数据开发学习（Redash）]</li><li>[2018-01 大数据开发学习（Kylin）]</li><li>[2018-01 Redash使用指南]</li><li>[2018-01 Redash权限分配]</li><li>[2018-04 大数据开发学习（Clickhouse）]</li></ul><h2 id="区块链"><a href="#区块链" class="headerlink" title="区块链"></a>区块链</h2><ul><li><a href="https://mp.weixin.qq.com/s/aw-8AmgasB20IwJHpCzxeA" target="_blank" rel="noopener">2018-07 区块链简史</a></li><li><a href="https://mp.weixin.qq.com/s/LNUPRo0fo5Qw02pP4bV2UQ" target="_blank" rel="noopener">2018-07 区块链分类</a></li></ul><p>工作关系，学习到一点区块链的知识。特地开了一个公众号进行写作，欢迎扫码关注：</p><p><img src="static/认知区块链.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;导读&quot;&gt;&lt;a href=&quot;#导读&quot; class=&quot;headerlink&quot; title=&quot;导读&quot;&gt;&lt;/a&gt;导读&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;这里是谷震平个人的博客文章，描述了关于大数据技术的一些实践结果和理论知识。包括，集群搭建、大数据工具学习、运维、学术资料整理。请记住这里：&lt;a href=&quot;http://guzhenping.com&quot;&gt;http://guzhenping.com&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;笔者作为一名数据仓库工程师，经历过两个数据仓库系统，一大一小。大的让笔者学到了很多，小的是笔者自己从0到1攒的。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="数据仓库" scheme="http://guzhenping.com/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>区块链学习笔记--EKT设计要素</title>
    <link href="http://guzhenping.com/2019/03/19/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-EKT%E8%AE%BE%E8%AE%A1%E8%A6%81%E7%B4%A0/"/>
    <id>http://guzhenping.com/2019/03/19/区块链学习笔记-EKT设计要素/</id>
    <published>2019-03-19T08:22:17.000Z</published>
    <updated>2019-03-19T08:22:17.730Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>解析过EKT项目，把认为对自己有用的点都总结下。</p><p>区块链项目的设计，多数类似BTC和ETH。当然还有像Fabric这种企业级联盟链，用到了kafka和gossip等技术栈。</p><p>笔者做了一段时间的底层开发，深知架构设计的重要性。对于高手来说，没有的轮子是可以自己造的，造个大规模消息/任务队列都只是想不想写的事情。但在企业中开发，追求的是稳定、性能、成本等等，所以通常希望使用开源组件，二次开发。</p><p>鉴于自己还不是高手，就把之前接触过的EKT底层组件都总结下。</p><h2 id="账户设计"><a href="#账户设计" class="headerlink" title="账户设计"></a>账户设计</h2><p>和ETH类似，用了账户模型，结合Merkle树进行设计，通过记录nonce值防止双花攻击。</p><p>核心逻辑：</p><pre><code>func GenerateKeyPair() (pubkey, privkey []byte) {    key, err := ecdsa.GenerateKey(S256(), rand.Reader)    if err != nil {        panic(err)    }    pubkey = elliptic.Marshal(S256(), key.X, key.Y)    return pubkey, math.PaddedBigBytes(key.D, 32)}</code></pre><p>EKT采用ECDSA（椭圆曲线数字签名算法）生成地址，secp256k1方法作为该算法参数。</p><p>工程中，ecdsa和sha3_256算是两个主流加密算法。ecdsa(椭圆曲线数字签名算法)是一种非对称公钥加密算法，也是数字签名算法类比中的佼佼者，用于防止数据串改和验证数据真实性，对标RSA算法。sha3_256是一种哈希算法，也叫摘要技术，防止数据被篡改。</p><p>ECDSA相比于RSA有如下特点：</p><ul><li>ECDSA的加密密钥更短</li><li>ECDSA的加密运算更快而安全性和RSA相当</li><li>RSA的私钥和公钥是可以互换加解密的，但ECDSA只能私钥加密公钥解密</li></ul><p>ECDSA的核心是利用数论中大数分解比较困难。这里列出一些推荐的扩展阅读：</p><ul><li><a href="http://www.freebuf.com/articles/database/155912.html" target="_blank" rel="noopener">椭圆曲线算法（ECC）学习（一）</a></li><li><a href="https://4hou.win/wordpress/?p=17620" target="_blank" rel="noopener">椭圆曲线算法（ECC）学习（二）之Secp256k1</a></li><li><a href="https://zh.wikipedia.org/wiki/%E7%B4%A0%E6%95%B0" target="_blank" rel="noopener">质数</a></li><li><a href="https://zh.wikipedia.org/wiki/%E8%B4%B9%E9%A9%AC%E5%B0%8F%E5%AE%9A%E7%90%86" target="_blank" rel="noopener">费马小定理</a></li></ul><h2 id="存储相关"><a href="#存储相关" class="headerlink" title="存储相关"></a>存储相关</h2><p>EKT的数据库采用LevelDB和sync.map。LevelDB是Key-Value型数据库，用于数据持久化。sync.map是一种GO语言的数据结构，可用于缓存。EKT封装了sync.map，开发了自己的内存型K-V数据库。</p><p>早期，有两个核心的文件：db/levedb.go和db/MemKVDatabase.go。</p><p>在实际代码中，EKT将本地KV和内存KV组装在一起，构成混合型KV数据库。核心文件db/ComposedKVDatabase.go 代码：</p><pre><code>type ComposedKVDatabase struct {    mem     *MemKVDatabase    // 引用内存型K-V数据库    levelDB *LevelDB          // 引用本地K-V数据库}// 抽象该混合型KV数据库func NewComposedKVDatabase(filePath string) *ComposedKVDatabase {    return &amp;ComposedKVDatabase{        mem:     NewMemKVDatabase(),        levelDB: NewLevelDB(filePath),    }}</code></pre><p>该数据库只有三个常用方法：</p><ul><li>Set(key, value []byte):插入数据</li><li>Get(key []byte):查找数据</li><li>Delete(key []byte):删除数据</li></ul><p>因为采用线性结构的区块链基因，所以并不会涉及update。</p><p>随着代码的迭代，笔者实测后发现：数据存在丢失的情况。之后，EKT官方去掉了自己的内存型K-V数据库，仅保留了leveldb相关。</p><p>这里，再次证明，稳定性好的东西，实在不好做。</p><h2 id="链结构相关"><a href="#链结构相关" class="headerlink" title="链结构相关"></a>链结构相关</h2><p>链的结构包含了14个元素，依赖了外部包：i_consensus/consensus.go, pool/TxPool.go, police.go, block_manager.go</p><pre><code>type BlockChain struct {    ChainId       int64    Consensus     i_consensus.ConsensusType    // 确认采用DPoS，Pow, Pos    currentLocker sync.RWMutex    currentBlock  Block    currentHeight int64    Locker        sync.RWMutex    Status        int    Fee           int64    Difficulty    []byte    Pool          *pool.TxPool                // 交易池    BlockInterval time.Duration    Police        BlockPolice                 // 用于记录从其他节点过来的block    BlockManager  *BlockManager               // 区块管理器    PackLock      sync.RWMutex}</code></pre><p>各字段的解释官方没有给出，之后通过对代码的详细分析，再给出精准定义。</p><p>简单提下创世过程。当主链在启动时发现没有区块的时候，将执行写创世区块的功能。</p><p>创世核心源码：</p><pre><code>// 将创世块写入数据库accounts := conf.EKTConfig.GenesisBlockAccountsblock = &amp;blockchain.Block{    Height:       0,    Nonce:        0,    Fee:          dpos.Blockchain.Fee,    TotalFee:     0,    PreviousHash: nil,    CurrentHash:  nil,    BlockBody:    blockchain.NewBlockBody(),    Body:         nil,    Timestamp:    0,    Locker:       sync.RWMutex{},    StatTree:     MPTPlus.NewMTP(db.GetDBInst()),    StatRoot:     nil,    TxTree:       MPTPlus.NewMTP(db.GetDBInst()),    TxRoot:       nil,    TokenTree:    MPTPlus.NewMTP(db.GetDBInst()),    TokenRoot:    nil,}// 为每个创世账户更新默克尔树根for _, account := range accounts {    block.CreateGenesisAccount(account)}// 更新默克尔树根，改变StatRoot,使得block.StatRoot = block.StatTree.Rootblock.UpdateMPTPlusRoot()// 计算当前区块Hash值block.CaculateHash()// 持久化dpos.Blockchain.SaveBlock(*block)</code></pre><p>获取创世区块的账户(可以是多个账户)，由主链启动时配置得到。生成首个区块的数据，做了一些改动后，写入数据库。</p><h2 id="主链启动"><a href="#主链启动" class="headerlink" title="主链启动"></a>主链启动</h2><p>经过对主链、共识机制的初始化，再运行共识模块的Run()即可启动。</p><p>主要有两步：</p><ol><li>从本地数据库中恢复当前节点已同步的区块</li><li>同步区块</li></ol><p>其中，同步区块方面有3个核心步骤：</p><ul><li>从其他节点同步，执行dpos.SyncHeight(Height)</li><li>当区块同步失败，尝试3次，3次之后判断是否超级节点</li><li>如果当前节点同步失败，且是超级节点，则通过投票结果来同步区块，执行dpos.startDelegateThread()进入打包区块的流程</li></ul><p>主网启动流程图如下：</p><p><img src="static/主网启动流程图.jpg" alt=""></p><h2 id="数据同步与恢复"><a href="#数据同步与恢复" class="headerlink" title="数据同步与恢复"></a>数据同步与恢复</h2><p>一般是刚启动的节点从其他节点同步数据。</p><ul><li>第一步：GetRound()获取当前打包节点信息</li><li>第二步：循环向各个节点发送请求，执行getBlockHeader()获取区块数据</li><li>第三步：再请求该区块的投票结果，执行getVotes()获取投票结果</li><li>第四步：执行Validate()校验投票结果的完整性和真实性，不合法重复第二步</li><li>第五步：校验合法后，执行getBlockEvents()获取交易明细数据，再执行ValidateNextBlock()验证交易明细数据和区块数据是否合法，不合法重复第二步</li><li>第六步：以上都合法，执行RecieveVoteResult()写入区块</li></ul><p>流程图如下：</p><p><img src="static/普通数据同步流程图.jpg" alt=""></p><p>后续会对RecieveVoteResult()单独分析，该函数集成的功能较多，包括：验证投票、管理区块、改变状态、记录打包间隔、写区块等功能。</p><p>本地数据恢复流程，一图可以描述，不再多说。</p><p><img src="static/恢复本地数据流程图.jpg" alt=""></p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>这篇文章的内容已经足够长且多。如果反响不错，会继续在深入写下对别人有价值的东西。</p><p>真正理解，还需要多多阅读源码。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;解析过EKT项目，把认为对自己有用的点都总结下。&lt;/p&gt;
&lt;p&gt;区块链项目的设计，多数类似BTC和ETH。当然还有像Fabric这种企业级联
      
    
    </summary>
    
    
      <category term="区块链" scheme="http://guzhenping.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
  </entry>
  
  <entry>
    <title>Redash 二次开发入门</title>
    <link href="http://guzhenping.com/2019/02/14/Redash-%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8/"/>
    <id>http://guzhenping.com/2019/02/14/Redash-二次开发入门/</id>
    <published>2019-02-14T02:36:41.000Z</published>
    <updated>2019-02-26T02:37:35.769Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在Redash二次开发上做了不少工作，修改bug，定制样式，定制功能，增加数据源等等。聊聊自己的二次开发经验。</p><p><strong>懂分享的人，一定会快乐！</strong></p><a id="more"></a><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>Redash依赖的外部环境比较多，特别是Python相关的包，大多数时候一次装不成功，需要单独装或者更新安装其他系统依赖库，祝大家一次成功！</p><p>环境准备可以参考另一篇blog，没用全说，其余的自己动动脑筋比较好。</p><p>附传送门：<a href="http://guzhenping.com/2018/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0--Redash/">Redash开发指南</a></p><h2 id="系统架构介绍"><a href="#系统架构介绍" class="headerlink" title="系统架构介绍"></a>系统架构介绍</h2><p>总的来说，Redash分为2部分：前端展示，后端数据库连接查询。在这中间，Redash采用celery进行任务调度，满足大量用户查询。</p><p>接下来会从三个方面进行总结：</p><ul><li>总体结构</li><li>前端逻辑</li><li>后端逻辑</li></ul><h4 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h4><p><img src="static/redash-architecture.png" alt=""></p><p>Redash在应用层、前后端均带有自己的核心模块。总而言之，当用户在使用Query、Dashboard、Alert功能时，会由前端提供基础的展示，</p><p>再把用户的请求通过JS和后端的Server层交互。用户所有的请求会放在任务队列中，对于需要查询的任务，则会去找对应的数据查询引擎。</p><h4 id="前端逻辑"><a href="#前端逻辑" class="headerlink" title="前端逻辑"></a>前端逻辑</h4><p>前端主要是控制页面元素和CSS样式，提供可视化组件和基本的功能。所有与后端交互的部分由JS业务逻辑控制，实现数据传输与功能跳转。</p><h4 id="后端逻辑"><a href="#后端逻辑" class="headerlink" title="后端逻辑"></a>后端逻辑</h4><p><img src="static/redash-backend.png" alt=""></p><p>后端的大部分功能依赖Celery进行完成，使用Redis做消息中间件。在任务队列中，大多数任务互相独立，进行数据查询时，worker起到主要作用。</p><p>Redash的一个特色是连接了28种以上的数据源，所以自带的数据查询引擎模块融合了这些数据库（引擎）的Python调用接口，十分丰富。</p><h2 id="代码结构介绍"><a href="#代码结构介绍" class="headerlink" title="代码结构介绍"></a>代码结构介绍</h2><p>Redash的模块化做的非常好，基本上可以通过目录和文件名分析出代码的功能定位。嗯，看过的都说好，改过的都说爽。</p><p><img src="static/directory.jpg" alt=""></p><p>一图其实可以省略很多字。实在不明白，欢迎留言评论，加小密圈哈。</p><h2 id="案例上手"><a href="#案例上手" class="headerlink" title="案例上手"></a>案例上手</h2><p>Redash有一个潜在的性能问题：Hive获取几万张表的schema信息时，促使celery worker任务队列堵塞，资源响应过慢。</p><p>所以，就以这个问题来看看源码修改。</p><p>问题分析：Reash采用desc命令获取hive table schema信息（db名，table列名），每张表执行一次desc，速度过慢。</p><p>解决方案：直接通过hive metadata获取，即，从mysql database中获取schema信息。（不懂hive的metadata为啥用mysql的，可以去查查）</p><p>准备查询mysql的SQL代码：</p><pre><code># 获取DB名db_sql = &quot;select name as db from DBS&quot;# 获取table名db_list = [&#39;xx1&#39;, &#39;xx2&#39;]for db in db_list:    tabel_sql = &quot;select tbl_name as table_name from DBS as db inner join TBLS as tbl on db.db_id = tbl.db_id where db.name = &#39;{}&#39;&quot;.format(db[0])# 获取column名table_list = [&#39;t1&#39;, &#39;t2&#39;]for table in table_list:    sql = &quot;select * from ( &quot;    sql += &quot;select col.column_name, col.type_name as column_type from DBS as db inner join TBLS as tbl on db.db_id = tbl.db_id inner join SDS as sds on tbl.sd_id = sds.sd_id inner join COLUMNS_V2 as col on sds.cd_id = col.cd_id&quot;    sql += &quot; where db.name = &#39;{}&#39; and tbl_name = &#39;{}&#39;&quot;.format(db[0], table[0])    sql += &quot; union all &quot;    sql += &quot;select pt.PKEY_NAME as column_name, pt.PKEY_TYPE as column_type from DBS as db inner join TBLS as tbl on db.db_id = tbl.db_id left join PARTITION_KEYS as pt on tbl.tbl_id = pt.tbl_id&quot;    sql += &quot; where db.name = &#39;{}&#39; and tbl_name = &#39;{}&#39;&quot;.format(db[0], table[0])    sql += &quot; ) as t where t.column_name is not null&quot;    </code></pre><p>前端添加hive数据源连接串的地方，增加mysql配置信息：</p><pre><code># 修改前端配置def configuration_schema(cls):    return {        &quot;type&quot;: &quot;object&quot;,        &quot;properties&quot;: {            &quot;host&quot;: {                &quot;type&quot;: &quot;string&quot;            },            &quot;port&quot;: {                &quot;type&quot;: &quot;number&quot;            },            &quot;database&quot;: {                &quot;type&quot;: &quot;string&quot;            },            &quot;username&quot;: {                &quot;type&quot;: &quot;string&quot;            },            &quot;mysql_host&quot;: {                &quot;type&quot;: &quot;string&quot;,            },            &quot;mysql_port&quot;: {                &quot;type&quot;: &quot;number&quot;,                &quot;default&quot;: 3306            },            &quot;mysql_database&quot;: {                &quot;type&quot;: &quot;string&quot;,                &quot;default&quot;: &quot;hive_test&quot;            },            &quot;mysql_username&quot;: {                &quot;type&quot;: &quot;string&quot;,                &quot;default&quot;: &quot;root&quot;            },            &quot;passwd&quot;: {                &quot;type&quot;: &quot;string&quot;,                &quot;title&quot;: &quot;Mysql Password&quot;            }        },        &quot;order&quot;: [&quot;host&quot;, &quot;port&quot;, &quot;database&quot;, &quot;username&quot;, &quot;mysql_host&quot;, &quot;mysql_port&quot;, &quot;mysql_database&quot;, &quot;mysql_username&quot;, &quot;passwd&quot;],        &quot;required&quot;: [&quot;host&quot;, &quot;mysql_host&quot;, &quot;mysql_port&quot;, &quot;mysql_database&quot;, &quot;mysql_username&quot;, &quot;passwd&quot;],        &#39;secret&#39;: [&#39;passwd&#39;]    }</code></pre><p>修改获取schema信息的处理函数_get_table(self, schema), 将其改为通过配置拿mysql连接串，联系上述SQL逻辑，获取schema信息即可。此处不做演示了。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>以上，算是笔者总结的二次开发入门资料吧。如有问题，欢迎加入小圈子，一起交流！</p><p><img src="static/xmq.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在Redash二次开发上做了不少工作，修改bug，定制样式，定制功能，增加数据源等等。聊聊自己的二次开发经验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;懂分享的人，一定会快乐！&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Redash" scheme="http://guzhenping.com/tags/Redash/"/>
    
  </entry>
  
  <entry>
    <title>前端开发学习笔记(一)</title>
    <link href="http://guzhenping.com/2018/11/29/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)/"/>
    <id>http://guzhenping.com/2018/11/29/前端开发学习笔记(一)/</id>
    <published>2018-11-29T09:04:48.000Z</published>
    <updated>2018-12-14T03:42:51.852Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在做一个叫计算平台的项目，主要是将机器学习、深度学习和大数据结合在一起，对标该领域中，阿里的计算平台、第四范式的先知平台等知名项目。目前，我们实现了基于Spark的可视化ETL功能，还有很多事情要做。</p><p>在这个项目中，用到了Vue.js的技术栈。为了加快项目进度，笔者还是在全沾的道路上越走越远，所以特地记录一路上的问题。</p><a id="more"></a><h2 id="Vue-js"><a href="#Vue-js" class="headerlink" title="Vue.js"></a>Vue.js</h2><p>入门Vue.js不算特别难，一定要多看几遍官方文档！一个完整的初级Demo大致涨这样：</p><p><img src="static/vue项目简易.png" alt=""></p><p>项目Demo地址: <a href="https://github.com/guzhenping/hello-vue" target="_blank" rel="noopener">hello-vue</a></p><p>初学时，搞不清<a href="https://cn.vuejs.org/v2/guide/single-file-components.html" target="_blank" rel="noopener">单文件组件</a>。没人带，就直接想用高级的姿势，是可怕的。既然初学，还是得一步一步来…</p><h2 id="前后端通讯：unicode-string"><a href="#前后端通讯：unicode-string" class="headerlink" title="前后端通讯：unicode/string"></a>前后端通讯：unicode/string</h2><p>Case: 前端通过JSON将string数据转发的Python后端，结果该类型变成了Unicode类型。废掉了如下代码：</p><pre><code>if isinstance(data, str):    do somethings....</code></pre><p>按道理说，data应该是string类型才对。</p><p>后端将js ajax转发来的数据通过flask.request.get_json()方法解析，相关方法：</p><pre><code>// 获取数据s = request.get_json()// get_json()方法def get_json(self, force=False, silent=False, cache=True):        &quot;&quot;&quot;Parse and return the data as JSON. If the mimetype does not        indicate JSON (:mimetype:`application/json`, see        :meth:`is_json`), this returns ``None`` unless ``force`` is        true. If parsing fails, :meth:`on_json_loading_failed` is called        and its return value is used as the return value.        :param force: Ignore the mimetype and always try to parse JSON.        :param silent: Silence parsing errors and return ``None``            instead.        :param cache: Store the parsed JSON to return for subsequent            calls.        &quot;&quot;&quot;        if cache and self._cached_json[silent] is not Ellipsis:            return self._cached_json[silent]        if not (force or self.is_json):            return None        data = self._get_data_for_json(cache=cache)        try:            rv = json.loads(data)        except ValueError as e:            if silent:                rv = None                if cache:                    normal_rv, _ = self._cached_json                    self._cached_json = (normal_rv, rv)            else:                rv = self.on_json_loading_failed(e)                if cache:                    _, silent_rv = self._cached_json                    self._cached_json = (rv, silent_rv)        else:            if cache:                self._cached_json = (rv, rv)        return rv</code></pre><p>原因：</p><pre><code># 导入环境from flask import json# 测试数据集s = &#39;{&quot;user_id&quot;:&quot;zhangshan&quot;,&quot;plan_id&quot;:&quot;m00_for_newOperator&quot;,&quot;operator_type&quot;:&quot;map&quot;,&quot;fast_mode&quot;:false,&quot;module_name&quot;:&quot;test_map1&quot;,&quot;func_name&quot;:[&quot;map1&quot;],&quot;operator_resource&quot;:[&quot;test_map1&quot;],&quot;input_rows&quot;:[{&quot;table_name&quot;:&quot;id_t3&quot;,&quot;fields&quot;:[{&quot;col_name&quot;:&quot;e&quot;,&quot;data_type&quot;:&quot;string&quot;},{&quot;col_name&quot;:&quot;f&quot;,&quot;data_type&quot;:&quot;boolean&quot;},{&quot;col_name&quot;:&quot;g&quot;,&quot;data_type&quot;:&quot;array&lt;double&gt;[1]&quot;},{&quot;col_name&quot;:&quot;h&quot;,&quot;data_type&quot;:&quot;binary&quot;},{&quot;col_name&quot;:&quot;i&quot;,&quot;data_type&quot;:&quot;array&lt;binary&gt;[1]&quot;},{&quot;col_name&quot;:&quot;a&quot;,&quot;data_type&quot;:&quot;string&quot;},{&quot;col_name&quot;:&quot;b&quot;,&quot;data_type&quot;:&quot;double&quot;},{&quot;col_name&quot;:&quot;c&quot;,&quot;data_type&quot;:&quot;long&quot;},{&quot;col_name&quot;:&quot;d&quot;,&quot;data_type&quot;:&quot;string&quot;}],&quot;values&quot;:[[&quot;2018-11-28 12:22:59.862776&quot;,true,[11],&quot;f1 e4&quot;,[&quot;01 02&quot;],&quot;hello&quot;,3.14,1,&quot;2018-11-28&quot;]]}]}&#39;# python 2.7&gt;&gt;&gt; json.loads(s)&gt;&gt;&gt; {u&#39;plan_id&#39;: u&#39;m00_for_newOperator&#39;, u&#39;input_rows&#39;: [{u&#39;values&#39;: [[u&#39;2018-11-28 12:22:59.862776&#39;, True, [11], u&#39;f1 e4&#39;, [u&#39;01 02&#39;], u&#39;hello&#39;, 3.14, 1, u&#39;2018-11-28&#39;]], u&#39;fields&#39;: [{u&#39;col_name&#39;: u&#39;e&#39;, u&#39;data_type&#39;: u&#39;string&#39;}, {u&#39;col_name&#39;: u&#39;f&#39;, u&#39;data_type&#39;: u&#39;boolean&#39;}, {u&#39;col_name&#39;: u&#39;g&#39;, u&#39;data_type&#39;: u&#39;array&lt;double&gt;[1]&#39;}, {u&#39;col_name&#39;: u&#39;h&#39;, u&#39;data_type&#39;: u&#39;binary&#39;}, {u&#39;col_name&#39;: u&#39;i&#39;, u&#39;data_type&#39;: u&#39;array&lt;binary&gt;[1]&#39;}, {u&#39;col_name&#39;: u&#39;a&#39;, u&#39;data_type&#39;: u&#39;string&#39;}, {u&#39;col_name&#39;: u&#39;b&#39;, u&#39;data_type&#39;: u&#39;double&#39;}, {u&#39;col_name&#39;: u&#39;c&#39;, u&#39;data_type&#39;: u&#39;long&#39;}, {u&#39;col_name&#39;: u&#39;d&#39;, u&#39;data_type&#39;: u&#39;string&#39;}], u&#39;table_name&#39;: u&#39;id_t3&#39;}], u&#39;func_name&#39;: [u&#39;map1&#39;], u&#39;fast_mode&#39;: False, u&#39;operator_resource&#39;: [u&#39;test_map1&#39;], u&#39;module_name&#39;: u&#39;test_map1&#39;, u&#39;user_id&#39;: u&#39;zhangshan&#39;, u&#39;operator_type&#39;: u&#39;map&#39;}# python 3.5&gt;&gt;&gt; json.loads(s)&gt;&gt;&gt; {&#39;user_id&#39;: &#39;zhangshan&#39;, &#39;plan_id&#39;: &#39;m00_for_newOperator&#39;, &#39;func_name&#39;: [&#39;map1&#39;], &#39;input_rows&#39;: [{&#39;fields&#39;: [{&#39;data_type&#39;: &#39;string&#39;, &#39;col_name&#39;: &#39;e&#39;}, {&#39;data_type&#39;: &#39;boolean&#39;, &#39;col_name&#39;: &#39;f&#39;}, {&#39;data_type&#39;: &#39;array&lt;double&gt;[1]&#39;, &#39;col_name&#39;: &#39;g&#39;}, {&#39;data_type&#39;: &#39;binary&#39;, &#39;col_name&#39;: &#39;h&#39;}, {&#39;data_type&#39;: &#39;array&lt;binary&gt;[1]&#39;, &#39;col_name&#39;: &#39;i&#39;}, {&#39;data_type&#39;: &#39;string&#39;, &#39;col_name&#39;: &#39;a&#39;}, {&#39;data_type&#39;: &#39;double&#39;, &#39;col_name&#39;: &#39;b&#39;}, {&#39;data_type&#39;: &#39;long&#39;, &#39;col_name&#39;: &#39;c&#39;}, {&#39;data_type&#39;: &#39;string&#39;, &#39;col_name&#39;: &#39;d&#39;}], &#39;values&#39;: [[&#39;2018-11-28 12:22:59.862776&#39;, True, [11], &#39;f1 e4&#39;, [&#39;01 02&#39;], &#39;hello&#39;, 3.14, 1, &#39;2018-11-28&#39;]], &#39;table_name&#39;: &#39;id_t3&#39;}], &#39;module_name&#39;: &#39;test_map1&#39;, &#39;operator_resource&#39;: [&#39;test_map1&#39;], &#39;fast_mode&#39;: False, &#39;operator_type&#39;: &#39;map&#39;}</code></pre><p>做的项目同时支持python2和3的话，需要注意该问题。</p><h2 id="求树的深度"><a href="#求树的深度" class="headerlink" title="求树的深度"></a>求树的深度</h2><p>Case: 需要获取用户输入的多维数组的维度。该问题，即求解树的深度：</p><pre><code class="javascript">// 获取数组的维度getArrayDim: function getDeepth(array) {    function sum(arr, flag) {      return arr.reduce(function (total, item) {        var totalDeepth ;        if (Array.isArray(item)) {          totalDeepth = sum(item, flag + 1);        }        return totalDeepth &gt; total ? totalDeepth : total;      }, flag)    }    return sum(array, 1);}</code></pre><h2 id="递归遍历多维数组"><a href="#递归遍历多维数组" class="headerlink" title="递归遍历多维数组"></a>递归遍历多维数组</h2><p>Case: 用户输入一个多维数组，该数组的值类型已被固定（long, int, string…）。</p><pre><code class="javascript">// 判断array内部元素的值类型checkArrayValueType: function checkArrayValue(type, realType, obj) {    var res = { status: 0, msg: &quot;&quot;} ;    for (var a in obj) {      if (obj[a] instanceof Array) {        res = checkArrayValue(type,obj[a]); //递归遍历      } else {        if(typeof(obj[a]) === type){          continue        }        res.status = 1;        res.msg = &#39;数据项：&#39; + obj[a] +&#39;, 不是&#39; +realType + &#39;类型; &#39;;        return res;      }    }    return res;},</code></pre><h2 id="检测Binary-hex"><a href="#检测Binary-hex" class="headerlink" title="检测Binary(hex)"></a>检测Binary(hex)</h2><p>Case: 用户输入一段字符串，需要知道是否是hex型的binary。用户输入要求为：按空格分割为2位, 不能以0x开头，如：’f1 01 e4’, ‘01 02 03’。</p><pre><code class="javascript">// check for binary, 需要其他地方做split和trimcheckBinary: function isBinary(item) {    var validChars = &quot;0123456789abcdef&quot;;    var isBinary = true;    var char;    for (var i = 0; i &lt; item.length &amp;&amp; isBinary == true; i++){      char = item.toLowerCase().charAt(i);      if (validChars.indexOf(char) == -1) {        isBinary = false;      }    }    return isBinary;},</code></pre><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>记录了一些demo和实际开发的case, 有时间继续更。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在做一个叫计算平台的项目，主要是将机器学习、深度学习和大数据结合在一起，对标该领域中，阿里的计算平台、第四范式的先知平台等知名项目。目前，我们实现了基于Spark的可视化ETL功能，还有很多事情要做。&lt;/p&gt;
&lt;p&gt;在这个项目中，用到了Vue.js的技术栈。为了加快项目进度，笔者还是在全沾的道路上越走越远，所以特地记录一路上的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="前端" scheme="http://guzhenping.com/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
      <category term="前端开发" scheme="http://guzhenping.com/tags/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/"/>
    
      <category term="Vue.js" scheme="http://guzhenping.com/tags/Vue-js/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发学习--Redis</title>
    <link href="http://guzhenping.com/2018/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0--Redis/"/>
    <id>http://guzhenping.com/2018/11/02/大数据开发学习--Redis/</id>
    <published>2018-11-02T09:04:49.000Z</published>
    <updated>2018-12-01T06:50:31.928Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-前言"><a href="#一-前言" class="headerlink" title="一 前言"></a>一 前言</h2><p>经常使用redis, 特地进行总结。</p><a id="more"></a><h2 id="二-基础"><a href="#二-基础" class="headerlink" title="二 基础"></a>二 基础</h2><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>下载安装包，或者：</p><pre><code>$ wget http://download.redis.io/releases/redis-3.2.0.tar.gz$ tar xzf redis-3.2.0.tar.gz$ cd redis-3.2.0$ make</code></pre><p>就是make命令~中间过程，报什么错搞定什么即可。</p><p>Ubuntu下比较简单，就是apt-get install redis-server</p><p>默认安装位置： 利用whereis redis 去找redis.conf ，需要修改是否后台运行daemonize （为yes）等属性</p><p>完成后，自动启动，可用命令：ps -aux| grep redis</p><p>redis是依赖，因为我们用python，不得不装一个wrapper——redis-py:<br><code>sudo pip install redis</code></p><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><p>要让Redis-server在后台运行！</p><p>1.简单的启动：</p><p>进到redis目录下的src文件夹下，输入：<br>redis-server</p><p>2.后台启动：</p><p>先去找redis.conf文件，修改daemonize属性，从no变为yes<br>进到redis目录下的src文件夹下，<br>输入：<br>redis-server &amp;</p><p>3.通过配置文件启动：</p><p>需要配置启动文件，在Redis工程目录下有个redis.conf文件，修改：</p><pre><code>#修改daemonize为yes，即默认以后台程序方式运行（还记得前面手动使用&amp;号强制后台运行吗）。daemonize yes#可修改默认监听端口，别改了，万一你忘了port 6379#修改生成默认日志文件位置logfile  &quot;/home/futeng/logs/redis.log&quot;#配置持久化文件存放位置dir /home/futeng/data/redisData</code></pre><p>配置完以后，启动：<br>还是 src目录下，<code>redis-server ./redis.conf</code></p><p>if 改了端口，使用redis-cli命令连接时，需要带上端口，比如：<br>redis-cli -p xxxx  [再次强调：仍在src目录下]</p><p>4.使用redis启动脚本，设置开机自启<br>在生产环境中使用这种方式。</p><p>5.启动无密码验证的</p><p><code>redis-server --protected-mode no</code></p><p>设置连接数</p><p><code>redis-server --protected-mode no --maxclients 100000</code></p><h4 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h4><p>还是进到redis目录下的src文件夹下，输入：<br>redis-cli</p><p>在弹出的交互界面测试下：</p><pre><code>redis&gt; set foo barOKredis&gt; get foo&quot;bar&quot;</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-前言&quot;&gt;&lt;a href=&quot;#一-前言&quot; class=&quot;headerlink&quot; title=&quot;一 前言&quot;&gt;&lt;/a&gt;一 前言&lt;/h2&gt;&lt;p&gt;经常使用redis, 特地进行总结。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Redis" scheme="http://guzhenping.com/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redash开发指南</title>
    <link href="http://guzhenping.com/2018/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0--Redash/"/>
    <id>http://guzhenping.com/2018/11/02/大数据开发学习--Redash/</id>
    <published>2018-11-02T09:04:48.000Z</published>
    <updated>2019-02-14T03:09:38.984Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-Redash介绍"><a href="#一-Redash介绍" class="headerlink" title="一 Redash介绍"></a>一 Redash介绍</h2><p>Redash是一款融合多数据源的可视化查询工具，用于Ad-hoc查询再好不过。除了官方支持的数据源，还可以通过复用代码开发支持Kylin、Clickhouse、TiDB、Palo、Druid等。</p><p>写给刚刚接触Redash的同学看，大佬请绕路哈。<br><a id="more"></a></p><h2 id="二-测试环境"><a href="#二-测试环境" class="headerlink" title="二 测试环境"></a>二 测试环境</h2><h4 id="官方提供的测试环境启动方式"><a href="#官方提供的测试环境启动方式" class="headerlink" title="官方提供的测试环境启动方式"></a>官方提供的测试环境启动方式</h4><p>redash开发文档 ： <a href="https://redash.io/help-onpremise/" target="_blank" rel="noopener">https://redash.io/help-onpremise/</a></p><h4 id="自主搭建"><a href="#自主搭建" class="headerlink" title="自主搭建"></a>自主搭建</h4><p>安装虚拟环境管理工具anaconda, 创建虚拟环境redash:<code>conda create -n redash python=2.7</code>,激活该环境:<code>source activate redash</code>。</p><p>创建虚拟环境（也可以不创建），激活后。利用pip安装redash所需要的包。</p><pre><code>pip install -r requirements.txt      # 程序基础依赖pip install -r requirements_all_ds.txt    # 所有数据库的依赖</code></pre><p>在上述安装过程中大概率会出现安装错误(遇到过10多回)。请将出错的包单独安装。</p><p>安装npm，在主目录执行：</p><pre><code>npm install    # 安装node模块npm run build    # 编译前端的东西</code></pre><p>启动服务器: </p><pre><code>bin/run ./manage.py runserver --debugger --reload</code></pre><p>启动celery的woker和调度器:</p><pre><code>./bin/run celery worker --app=redash.worker --beat -Qscheduled_queries,queries,celery -c2</code></pre><p>介绍下单独启动的方式：分开启动woker和调度器，调度器启动:</p><pre><code>bin/run celery worker --app=redash.worker -c4 -Qscheduled_queries --maxtasksperchild=10 -Ofair</code></pre><p>worker启动:</p><pre><code>bin/run celery worker --app=redash.worker --beat -c8 -Qqueries,celery --maxtasksperchild=10 -Ofair</code></pre><p><strong>根据机器机器实际情况，调节-c后的参数，用来指定启动多少个进程数量。</strong></p><p>此处就不放PostgreSQL（Redash暂时只支持PG）和redis的环境准备了。搭好基础环境后，修改redash/settings/<strong>init</strong>.py文件的配置。</p><h4 id="启动方式"><a href="#启动方式" class="headerlink" title="启动方式"></a>启动方式</h4><p>除了上面的启动方式，还推荐使用guncoin的启动方式。</p><p>启动服务器：</p><pre><code># 前台启动gunicorn -b 127.0.0.1:5000 --name redash -w 4 --max-requests 1000 redash.wsgi:app# 后台启动nohup /home/hadoop/anaconda3/envs/redash/bin/python /home/hadoop/anaconda3/envs/redash/bin/gunicorn -b 0.0.0.0:5000 --name redash -w 4 --max-requests 1000 redash.wsgi:app &gt;&gt; redash.log &amp;</code></pre><p>没有gunicorn命令的，需要装Python包。</p><p>启动调度器进程：</p><pre><code># 前台启动bin/run celery worker --app=redash.worker -c4 -Qscheduled_queries --maxtasksperchild=10 -Ofair# 后台启动nohup /home/hadoop/anaconda3/envs/redash/bin/celery worker --app=redash.worker -c4 -Qscheduled_queries --maxtasksperchild=10 -Ofair &gt;&gt; schedular.log &amp;</code></pre><p>启动worker进程：</p><pre><code># 前台启动bin/run celery worker --app=redash.worker --beat -c8 -Qqueries,celery --maxtasksperchild=10 -Ofair  # 后台启动nohup /home/hadoop/anaconda3/envs/redash/bin/celery worker --app=redash.worker --beat -c8 -Qqueries,celery --maxtasksperchild=10 -Ofair  &gt;&gt; worker.log &amp;</code></pre><p>也可以用nginx做下代理，配置文件：</p><pre><code>worker_processes  4;events {    worker_connections  1024;}http {    include       mime.types;    default_type  application/octet-stream;    sendfile        on;    keepalive_timeout  3600;    upstream rd_servers {         server 127.0.0.1:5000;    }    server {          server_tokens off;          listen 5123 default;          access_log /var/log/nginx/rd.access.log;          gzip on;          gzip_types *;          gzip_proxied any;          location / {              proxy_set_header Host $http_host;              proxy_set_header X-Real-IP $remote_addr;              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;              proxy_set_header X-Forwarded-Proto $scheme;              proxy_pass       http://rd_servers;          }    }}</code></pre><p>启动NGINX:</p><pre><code>cd /usr/local/nginx/sbinsudo ./nginx</code></pre><h2 id="三-Redash-VS-Superset"><a href="#三-Redash-VS-Superset" class="headerlink" title="三 Redash VS Superset"></a>三 Redash VS Superset</h2><p>关于Redash：<a href="https://redash.io/；" target="_blank" rel="noopener">https://redash.io/；</a></p><p>与Superset的区别与联系：<a href="https://www.zhihu.com/question/60369195/answer/258298127。" target="_blank" rel="noopener">https://www.zhihu.com/question/60369195/answer/258298127。</a></p><p>这个答案里面有对比两边代码的描述，二次开发上，Redash可能更胜一筹。</p><h2 id="四-结语"><a href="#四-结语" class="headerlink" title="四 结语"></a>四 结语</h2><p>关于Redash的环境就写到这，后续继续增加Redash相关的技术文章。如有问题讨论，欢迎来我的小圈子~</p><p><img src="static/xmq.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-Redash介绍&quot;&gt;&lt;a href=&quot;#一-Redash介绍&quot; class=&quot;headerlink&quot; title=&quot;一 Redash介绍&quot;&gt;&lt;/a&gt;一 Redash介绍&lt;/h2&gt;&lt;p&gt;Redash是一款融合多数据源的可视化查询工具，用于Ad-hoc查询再好不过。除了官方支持的数据源，还可以通过复用代码开发支持Kylin、Clickhouse、TiDB、Palo、Druid等。&lt;/p&gt;
&lt;p&gt;写给刚刚接触Redash的同学看，大佬请绕路哈。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Redash" scheme="http://guzhenping.com/tags/Redash/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发学习（Flink）</title>
    <link href="http://guzhenping.com/2018/10/29/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0--Flink/"/>
    <id>http://guzhenping.com/2018/10/29/大数据开发学习--Flink/</id>
    <published>2018-10-29T09:04:48.000Z</published>
    <updated>2018-11-30T02:46:37.477Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Flink在实时流处理领域越来越热，以阿里为首的企业正在投入更多的资源。在实际工作中也遇到了流处理的场景，特此学习一下。</p><a id="more"></a><h2 id="Demo制作"><a href="#Demo制作" class="headerlink" title="Demo制作"></a>Demo制作</h2><p>先看一个demo:</p><p>WordCount.java</p><pre><code class="java">package com.gzp.batch;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.util.Collector;public class WordCount {    public static void main(String[] args) throws Exception {        final ParameterTool params = ParameterTool.fromArgs(args);        final ExecutionEnvironment env = ExecutionEnvironment.createCollectionsEnvironment();        DataSet&lt;String&gt; text = WordCountData.getDefaultTextLineDataSet(env);        DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; counts =                // split up the lines in pairs (2-tuples) containing: (word,1)                text.flatMap(new Tokenizer())                    .groupBy(0)                    .sum(1);        counts.print();    }    public static final class Tokenizer implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; {        @Override        public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) {            // emit the pairs            for (String token : value.toLowerCase().split(&quot;\\W+&quot;)) {                if (token.length() &gt; 0) {                    out.collect(new Tuple2&lt;&gt;(token, 1));                }            }        }    }}</code></pre><p>WordCountData.java</p><pre><code>package com.gzp.batch;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;public class WordCountData {    public static final String[] WORDS = new String[] {            &quot;To be, or not to be,--that is the question:--&quot;,            &quot;Whether &#39;tis nobler in the mind to suffer&quot;,            &quot;The slings and arrows of outrageous fortune&quot;,            &quot;Or to take arms against a sea of troubles,&quot;,            &quot;And by opposing end them?--To die,--to sleep,--&quot;,            &quot;No more; and by a sleep to say we end&quot;,            &quot;The heartache, and the thousand natural shocks&quot;,            &quot;That flesh is heir to,--&#39;tis a consummation&quot;,            &quot;Devoutly to be wish&#39;d. To die,--to sleep;--&quot;,            &quot;To sleep! perchance to dream:--ay, there&#39;s the rub;&quot;,            &quot;For in that sleep of death what dreams may come,&quot;,            &quot;When we have shuffled off this mortal coil,&quot;,            &quot;Must give us pause: there&#39;s the respect&quot;,            &quot;That makes calamity of so long life;&quot;,            &quot;For who would bear the whips and scorns of time,&quot;,            &quot;The oppressor&#39;s wrong, the proud man&#39;s contumely,&quot;,            &quot;The pangs of despis&#39;d love, the law&#39;s delay,&quot;,            &quot;The insolence of office, and the spurns&quot;,            &quot;That patient merit of the unworthy takes,&quot;,            &quot;When he himself might his quietus make&quot;,            &quot;With a bare bodkin? who would these fardels bear,&quot;,            &quot;To grunt and sweat under a weary life,&quot;,            &quot;But that the dread of something after death,--&quot;,            &quot;The undiscover&#39;d country, from whose bourn&quot;,            &quot;No traveller returns,--puzzles the will,&quot;,            &quot;And makes us rather bear those ills we have&quot;,            &quot;Than fly to others that we know not of?&quot;,            &quot;Thus conscience does make cowards of us all;&quot;,            &quot;And thus the native hue of resolution&quot;,            &quot;Is sicklied o&#39;er with the pale cast of thought;&quot;,            &quot;And enterprises of great pith and moment,&quot;,            &quot;With this regard, their currents turn awry,&quot;,            &quot;And lose the name of action.--Soft you now!&quot;,            &quot;The fair Ophelia!--Nymph, in thy orisons&quot;,            &quot;Be all my sins remember&#39;d.&quot;    };    public static DataSet&lt;String&gt; getDefaultTextLineDataSet(ExecutionEnvironment env) {        return env.fromElements(WORDS);    }}</code></pre><p>上述代码核心是将一段文本按单词切割，统计词频。flatMap()调用udf将文本切割并生成结构化数据，按单词分组后再sum。</p><p>不得不说，java写的flink task代码太长….</p><h2 id="Core-Concepts"><a href="#Core-Concepts" class="headerlink" title="Core Concepts"></a>Core Concepts</h2><p>####</p><h2 id="Flink-Case"><a href="#Flink-Case" class="headerlink" title="Flink Case"></a>Flink Case</h2><h4 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h4><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/cli.html" target="_blank" rel="noopener">使用简介</a></p><h2 id="参考资料、"><a href="#参考资料、" class="headerlink" title="参考资料、"></a>参考资料、</h2><ul><li>核心概念翻译</li></ul><p><a href="https://blog.csdn.net/xiaoping2017/article/details/79846158" target="_blank" rel="noopener">Flink架构、原理与部署测试详解</a></p><ul><li>简单的Scala Demo应用： </li></ul><p><a href="https://github.com/liguohua-bigdata/simple-flink/blob/master/book/stream/customSource/customSourceScala.md" target="_blank" rel="noopener">flink demo</a></p><ul><li>关于动态表: </li></ul><p><a href="http://www.10tiao.com/html/157/201707/2653162664/1.html" target="_blank" rel="noopener">在数据流中使用SQL查询：Apache Flink中的动态表的持续查询</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Flink在实时流处理领域越来越热，以阿里为首的企业正在投入更多的资源。在实际工作中也遇到了流处理的场景，特此学习一下。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="实时流" scheme="http://guzhenping.com/tags/%E5%AE%9E%E6%97%B6%E6%B5%81/"/>
    
      <category term="Flink" scheme="http://guzhenping.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Kylin应用篇</title>
    <link href="http://guzhenping.com/2018/10/26/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0--Kylin%E5%BA%94%E7%94%A8/"/>
    <id>http://guzhenping.com/2018/10/26/大数据开发学习--Kylin应用/</id>
    <published>2018-10-26T09:04:48.000Z</published>
    <updated>2018-11-30T02:56:21.818Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Kylin是一款处理海量数据，提供SQL和多维度分析的OLAP工具。Kylin用于处理hadoop/spark场景下大量数据的预聚合，用户可自定义数据模型用于解决超过100亿+条记录的查询。</p><a id="more"></a><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="Not-Support"><a href="#Not-Support" class="headerlink" title="Not Support"></a>Not Support</h3><ul><li><p>建表或构建模型时，请勿使用中文列名。</p></li><li><p>不支持临时创建新列的统计</p></li></ul><p>原表有两个字段a和b，通过concat进行拼接，然后做count()或者count(distinct *)。Kylin并不支持上述做法，因为无法命中相关的cube。</p><p>不能统计下述例子：</p><p>count(distinct concat(cast(a as varchar),b))</p><pre><code>select count(distinct concat(cast(a as varchar),b))from table_awhere dt = &#39;2018-05-01&#39;</code></pre><h3 id="Cube命中"><a href="#Cube命中" class="headerlink" title="Cube命中"></a>Cube命中</h3><p>所有的SQL需要命中相关Cube才可以使用。如果不是使用姿势问题，请联系管理员构建新的Cube。</p><ul><li>在join时，需要用事实表join维度表，负责容易出现：</li></ul><pre><code>No realization found for OLAPContext, CUBE_NOT_READY, CUBE_NOT_READY, CUBE_NOT_READY, MODEL_UNMATCHED_JOIN, MODEL_UNMATCHED_JOIN</code></pre><ul><li>在写子查询时，不能将事实表写在查询中，Cube可能无法命中。</li></ul><h2 id="调度脚本"><a href="#调度脚本" class="headerlink" title="调度脚本"></a>调度脚本</h2><pre><code class="python">import datetime, requestsauth_str = &quot;Basic YWRtaW46S1lMSU6=&quot;url_str = &quot;http://xxxxx.com:7070/kylin/api&quot;def auth():    &quot;&quot;&quot;    用户认证    :return:    &quot;&quot;&quot;    url = &quot;{url_str}/user/authentication&quot;.format(url_str=url_str)    payload = &quot;=&quot;    headers = {        &#39;Content-Type&#39;: &quot;application/x-www-form-urlencoded&quot;,        &#39;Authorization&#39;: auth_str,        &#39;Cache-Control&#39;: &quot;no-cache&quot;        }    response = requests.request(&quot;POST&quot;, url, data=payload, headers=headers)    print(response.text)def get_cube():    &quot;&quot;&quot;    获取cube信息    :return:    &quot;&quot;&quot;    url = &quot;{url_str}/cubes&quot;.format(url_str=url_str)    querystring = {&quot;cubeName&quot;: &quot;test_join&quot;}    headers = {        &#39;Cache-Control&#39;: &quot;no-cache&quot;,        &#39;Authorization&#39;: auth_str        }    response = requests.request(&quot;GET&quot;, url, headers=headers, params=querystring)    print(response.json())def build_cube(cube_name, start_date, end_date, build_type):    &quot;&quot;&quot;    构建指定cube    startTime 和 endTime 应该是utc时间。    buildType 可以是 BUILD 、 MERGE 或 REFRESH。        - BUILD 用于构建一个新的segment，        - REFRESH 用于刷新一个已有的segment，        - MERGE 用于合并多个已有的segment生成一个较大的segment    :return:    &quot;&quot;&quot;    url = &quot;{url_str}/cubes/{cube_name}/rebuild&quot;.format(cube_name=cube_name, url_str=url_str)    start_stamp = int(datetime.datetime.strptime(start_date, &#39;%Y-%m-%d %H:%M:%S&#39;).timestamp() * 1000)    end_stamp = int(datetime.datetime.strptime(end_date, &#39;%Y-%m-%d %H:%M:%S&#39;).timestamp() * 1000)    payload = &quot;{\&quot;startTime\&quot;: %d, \&quot;endTime\&quot;: %d, \&quot;buildType\&quot;: \&quot;%s\&quot;}&quot; % (start_stamp, end_stamp, build_type)    headers = {        &#39;Content-Type&#39;: &quot;application/json&quot;,        &#39;Authorization&#39;: auth_str,        &#39;Cache-Control&#39;: &quot;no-cache&quot;        }    response = requests.request(&quot;PUT&quot;, url, data=payload, headers=headers)    print(response.json())if __name__ == &#39;__main__&#39;:    cube_name = &quot;test_api&quot;    start_date = &#39;2018-05-01 04:00:00&#39;    end_date = &#39;2018-05-01 08:00:00&#39;    build_type = &#39;BUILD&#39;    build_cube(cube_name, start_date, end_date, build_type)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;Kylin是一款处理海量数据，提供SQL和多维度分析的OLAP工具。Kylin用于处理hadoop/spark场景下大量数据的预聚合，用户可自定义数据模型用于解决超过100亿+条记录的查询。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Kylin" scheme="http://guzhenping.com/tags/Kylin/"/>
    
  </entry>
  
  <entry>
    <title>Clickhouse 高级SQL</title>
    <link href="http://guzhenping.com/2018/07/27/Clickhouse%20%E9%AB%98%E7%BA%A7SQL/"/>
    <id>http://guzhenping.com/2018/07/27/Clickhouse 高级SQL/</id>
    <published>2018-07-27T09:04:48.000Z</published>
    <updated>2018-11-30T02:43:37.103Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>测试数据集：</p><table><thead><tr><th>ca</th><th>cb</th><th>cc</th></tr></thead><tbody><tr><td>A</td><td>W</td><td>1</td></tr><tr><td>A</td><td>W</td><td>2</td></tr><tr><td>B</td><td>X</td><td>1</td></tr><tr><td>B</td><td>Z</td><td>2</td></tr><tr><td>B</td><td>Z</td><td>4</td></tr></tbody></table><h3 id="按最大-最小值-TOP1去重"><a href="#按最大-最小值-TOP1去重" class="headerlink" title="按最大/最小值/TOP1去重"></a>按最大/最小值/TOP1去重</h3><p>按ca和cb取cc最小值取值：</p><pre><code>select     ca,     cb,     min(cc)from tablegroup by ca,     cb</code></pre><table><thead><tr><th>column_A</th><th>column_B</th><th>column_C</th></tr></thead><tbody><tr><td>A</td><td>W</td><td>1</td></tr><tr><td>B</td><td>X</td><td>1</td></tr><tr><td>B</td><td>Z</td><td>2</td></tr></tbody></table><p>取最大，则使用max()函数。取一组值的TOP1也是同理。</p><h3 id="按列合并多行（多-gt-少）"><a href="#按列合并多行（多-gt-少）" class="headerlink" title="按列合并多行（多-&gt;少）"></a>按列合并多行（多-&gt;少）</h3><pre><code>select     ca,     cb,     groupUniqArray(cc)from tablegroup by ca, cb</code></pre><table><thead><tr><th>column_A</th><th>column_B</th><th>column_C</th></tr></thead><tbody><tr><td>A</td><td>W</td><td>[2,1]</td></tr><tr><td>B</td><td>X</td><td>[1]</td></tr><tr><td>B</td><td>Z</td><td>[4,2]</td></tr></tbody></table><p>这是一种分组的概念，将相同的数据放在一起，需要被统计的数据放在array数据类型中。统计array，可以使用length()，获取数组长度，相当于分组count</p><p>此外，groupArray也可以满足需求。</p><h3 id="分组排序后取TopN"><a href="#分组排序后取TopN" class="headerlink" title="分组排序后取TopN"></a>分组排序后取TopN</h3><pre><code>SELECT ca,       groupArray(1)(cc) FROM  ( SELECT *   FROM table   ORDER BY ca,            cb,            cc )GROUP BY ca</code></pre><table><thead><tr><th>column_A</th><th>column_B</th><th>column_C</th></tr></thead><tbody><tr><td>A</td><td>W</td><td>[1]</td></tr><tr><td>B</td><td>X</td><td>[1]</td></tr><tr><td>B</td><td>Z</td><td>[2]</td></tr></tbody></table><p>以上是按cc列的值升序去top1。通过order by x改变顺序，再用groupArry(N)()函数处理获取top值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;测试数据集：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ca&lt;/th&gt;
&lt;th&gt;cb&lt;/th&gt;
&lt;th&gt;cc&lt;/th&gt;
&lt;/
      
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Clickhouse" scheme="http://guzhenping.com/tags/Clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>Redash Model源码分析</title>
    <link href="http://guzhenping.com/2018/07/26/Redash%20Model%E6%95%B4%E7%90%86%E5%88%86%E6%9E%90/"/>
    <id>http://guzhenping.com/2018/07/26/Redash Model整理分析/</id>
    <published>2018-07-26T09:04:48.000Z</published>
    <updated>2018-11-30T02:43:09.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>掌握Redash执行原理，对于深度的二次开发至关重要。</p><a id="more"></a><h2 id="query功能"><a href="#query功能" class="headerlink" title="query功能"></a>query功能</h2><p>SQL查询是Redash的核心功能之一。通常情况下，用户在前端会生成如下参数：</p><ul><li>query.data_source：下列列表中的数据源，必须选</li><li>parameter_values：用户自定义的参数值，可无</li><li>query.query_text：用户自定义的SQL文本，必须写</li><li>query.id：前端自动生成的查询id，系统生成</li></ul><p>在上述条件具备后，将会调用Redash API： <code>/api/queries/</code>进行任务提交。</p><p>所有任务将会被redash.handlers.query_result.run_query()方法接受处理。此处将进行参数校验，语法解析，任务传输等方面的处理。</p><p>多数的查询场景，先将SQL文本hash后，去缓存队列和后台数据库（Redash的Postgre）进行寻找，找不到再发给celery让worker执行该SQL。</p><p>负责具体执行：redash.models.QueryResult()类实例 和redash.tasks.queries.enqueue_query()方法。前者：负责查现有的结果；后者：立即执行相关SQL语句。</p><p>在enqueue_query()方法里，先连Redis,将任务加入队列，持续监听查询结果。</p><p>核心代码如下：</p><pre><code>    while try_count &lt; 5:        try_count += 1        pipe = redis_connection.pipeline()        try:            pipe.watch(_job_lock_id(query_hash, data_source.id))            job_id = pipe.get(_job_lock_id(query_hash, data_source.id))            if job_id:                logging.info(&quot;[%s] Found existing job: %s&quot;, query_hash, job_id)                job = QueryTask(job_id=job_id)                if job.ready():                    logging.info(&quot;[%s] job found is ready (%s), removing lock&quot;, query_hash, job.celery_status)                    redis_connection.delete(_job_lock_id(query_hash, data_source.id))                    job = None            if not job:                pipe.multi()                time_limit = None                if scheduled_query:                    queue_name = data_source.scheduled_queue_name                    scheduled_query_id = scheduled_query.id                else:                    queue_name = data_source.queue_name                    scheduled_query_id = None                    time_limit = settings.ADHOC_QUERY_TIME_LIMIT                result = execute_query.apply_async(args=(query, data_source.id, metadata, user_id, scheduled_query_id),                                                   queue=queue_name,                                                   time_limit=time_limit)                job = QueryTask(async_result=result)                tracker = QueryTaskTracker.create(                    result.id, &#39;created&#39;, query_hash, data_source.id,                    scheduled_query is not None, metadata)                tracker.save(connection=pipe)                logging.info(&quot;[%s] Created new job: %s&quot;, query_hash, job.id)                pipe.set(_job_lock_id(query_hash, data_source.id), job.id, settings.JOB_EXPIRY_TIME)                pipe.execute()            break        except redis.WatchError:            continue</code></pre><p>以上是Query功能的分析。主要组件：</p><ul><li>celery分布式框架</li><li>redis缓存和任务队列</li><li>postgre后台DB。</li></ul><p>相关功能的设计比较中规中矩，报错异常的检测较少，后续值得优化。</p><h2 id="Model层"><a href="#Model层" class="headerlink" title="Model层"></a>Model层</h2><p>model层是Redash的设计核心，每个类对应后台数据库一张表和一个功能。</p><ul><li>access_permission, 权限表</li><li>alembic_version,版本号         </li><li>alert_subscriptions，报警描述</li><li>alerts，报警列表         </li><li>api_keys, api key管理</li><li>changes，升级改动</li><li>dashboards，报表存储</li><li>data_source_groups,每个组对应的数据源</li><li>data_sources，所有数据源</li><li>events，后台日志</li><li>groups，所有分组列表</li><li>notification_destinations，报警的模板和目的地</li><li>organizations， 组织         </li><li>queries，所有queris         </li><li>query_results，query的结果，另一种缓存</li><li>query_snippets，SQL的评论         </li><li>users, 用户已列表</li><li>visualizations，可视化图表存储</li><li>widgets，可视化控件</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;掌握Redash执行原理，对于深度的二次开发至关重要。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="可视化" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="Redash" scheme="http://guzhenping.com/tags/Redash/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发学习（Kudu）</title>
    <link href="http://guzhenping.com/2018/05/25/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0--Kudu/"/>
    <id>http://guzhenping.com/2018/05/25/大数据开发学习--Kudu/</id>
    <published>2018-05-25T09:04:48.000Z</published>
    <updated>2018-11-30T02:57:14.186Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>kudu，作为OLAP工具十分强劲。本文记录了笔者对其学习和使用的过程。</p><p>注明：这篇文章是研究Kudu在OLAP的场景，不准备讨论其他Case。</p><h2 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h2><p>最新版安装：<a href="https://kudu.apache.org/docs/installation.html" target="_blank" rel="noopener">Installing Apache Kudu</a></p><p>推荐使用VM进行快速体验，<a href="https://kudu.apache.org/docs/quickstart.html" target="_blank" rel="noopener">Apache Kudu Quickstart</a>。</p><p>当需要使用kudu client功能时，需要安装：kudu-client、kudu-client-devel这两个C++的库。</p><h2 id="常规使用"><a href="#常规使用" class="headerlink" title="常规使用"></a>常规使用</h2><p>教学Demo参见：<a href="https://kudu.apache.org/docs/quickstart.html" target="_blank" rel="noopener">Apache Kudu Quickstart</a></p><p>以上内容是对静态数据的使用，如果是一条实时的数据流，则采用kudu API的方式。需要去做定制开发。</p><h2 id="架构体系"><a href="#架构体系" class="headerlink" title="架构体系"></a>架构体系</h2><p>kudu架构：<br><img src="static/kudu/kudu.png" alt=""></p><p>关于运行原理的文章，推荐：<a href="http://www.nosqlnotes.com/technotes/kudu-design/" target="_blank" rel="noopener">Kudu设计原理初探</a></p><h2 id="OLAP服务"><a href="#OLAP服务" class="headerlink" title="OLAP服务"></a>OLAP服务</h2><p>作为OLAP服务，ETL环节的处理至关重要。以下列举了小米公司的场景：<br><img src="static/kudu/kudu-etl1.jpg" alt=""></p><p>对比：<br><img src="static/kudu/kudu-etl2.jpg" alt=""></p><p>可以看出，除了日志数据外，线上业务数据都可以实时同步到kudu里。基于kudu对外提供OLAP服务，数据的实时性非常可观。</p><p>在小米，采用impala作为查询（计算）引擎，但是网上也有presto on kudu的组件可供选型（<a href="https://github.com/MartinWeindel/presto-kudu" target="_blank" rel="noopener">传送门</a>）。</p><h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>微店：</p><ul><li><a href="https://datascience.weidian-inc.com/kudu_impala/" target="_blank" rel="noopener">Kudu+Impala介绍</a><ul><li><a href="https://datascience.weidian-inc.com/kudu_schema_design/" target="_blank" rel="noopener">Kudu的Schema表结构设计</a></li></ul></li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>运行原理：<a href="https://blog.csdn.net/cdxxx5708/article/details/79074763" target="_blank" rel="noopener">kudu内部机制</a></p><p>关于Kudu的介绍： <a href="https://kudu.apache.org/docs/#_kudu_impala_integration_features" target="_blank" rel="noopener">Introducing Apache Kudu</a> </p><p>基于Kudu搭建OLAP工具：<a href="https://myslide.cn/slides/3584?vertical=1" target="_blank" rel="noopener">小米：使用Kudu搭建OLAP服务</a></p><p>基于Kudu的实际应用：<a href="http://www.infoq.com/cn/articles/spark-streaming-kudu-impala" target="_blank" rel="noopener">使用Spark Streaming + Kudu + Impala构建一个预测引擎</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;kudu，作为OLAP工具十分强劲。本文记录了笔者对其学习和使用的过程。&lt;/p&gt;
&lt;p&gt;注明：这篇文章是研究Kudu在OLAP的场景，不准备
      
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Kudu" scheme="http://guzhenping.com/tags/Kudu/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发学习（Clickhouse）</title>
    <link href="http://guzhenping.com/2018/05/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0--Clickhouse/"/>
    <id>http://guzhenping.com/2018/05/24/大数据开发学习--Clickhouse/</id>
    <published>2018-05-24T09:04:48.000Z</published>
    <updated>2018-11-30T02:10:54.811Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>1年前，用过Greenplum，这是第一次接触MPP结构的OLAP系统。今天市面上常见的MPP架构工具，还有Clickhouse和Palo等。(当然，SQL on Hadoop体系的Presto和Impala也算是MPP结构，只是数据存储方面没有自己的东西，都是依赖hdfs,mysql等。)</p><p>工作需要，对Clickhouse进行学习。<br><a id="more"></a></p><h2 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h2><p>Ubuntu上比较好安装，但是一般公司用的服务器都是Centos。这里只讨论在Centos 7的安装方式，centos 6请在Altinity公司提供的下载界面中自行寻找。</p><h3 id="安装命令"><a href="#安装命令" class="headerlink" title="安装命令"></a>安装命令</h3><pre><code>如果是装54362版本的包，其余所有依赖都需要是一致的。准备源依赖，由Altinity公司提供：curl -s https://packagecloud.io/install/repositories/Altinity/clickhouse/script.rpm.sh | sudo bashserver-common:sudo yum install clickhouse-server-common-1.1.54362-1.el7.x86_64sudo yum install clickhouse-server-common-1.1.54380-1.el7.x86_64server:sudo yum install clickhouse-server-1.1.54362-1.el7.x86_64sudo yum install clickhouse-server-1.1.54380-1.el7.x86_64client：sudo yum install clickhouse-client-1.1.54362-1.el7.x86_64sudo yum install clickhouse-client-1.1.54380-1.el7.x86_64</code></pre><p>关于Altinity公司的其他版本，可访问<a href="https://packagecloud.io/Altinity/clickhouse" target="_blank" rel="noopener">这里下载</a>。</p><p>以上安装如有疑问，可以使用下方安装方式：</p><ul><li><a href="https://github.com/red-soft-ru/clickhouse-rpm" target="_blank" rel="noopener">https://github.com/red-soft-ru/clickhouse-rpm</a></li></ul><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><pre><code>server端：sudo /etc/rc.d/init.d/clickhouse-server startclient端：clickhouse-client</code></pre><h2 id="使用经验"><a href="#使用经验" class="headerlink" title="使用经验"></a>使用经验</h2><h3 id="常用SQL"><a href="#常用SQL" class="headerlink" title="常用SQL"></a>常用SQL</h3><pre><code>-- 查看集群情况select * from system.clusters;-- 查看分区情况select     partition,    name,    rowsfrom system.parts;</code></pre><h3 id="MergeTree"><a href="#MergeTree" class="headerlink" title="MergeTree"></a>MergeTree</h3><p>选择engine时，尽量用merge tree.MergeTree引擎支持以主键和日期作为索引，提供实时更新数据的可能性。这是Clickhouse中最先进的表引擎。</p><p>数据表将数据分割为小的索引块进行处理。每个索引块之间依照主键排序。每个索引块记录了指定的开始日期和结束日期。插入数据时，MergeTree会对数据进行排序，以保证存储在索引块中的数据有序。当写入新数据时，会放在新的文件夹下，索引块之间的合并过程会在系统后台定期自动执行。MergeTree引擎会选择几个相邻的索引块进行合并，然后对二者合并排序。</p><p><img src="static/clickhouse/mergetree1.png" alt=""></p><p><img src="static/clickhouse/mergetree2.png" alt=""></p><p><img src="static/clickhouse/mergetree3.png" alt=""></p><p><img src="static/clickhouse/mergetree4.png" alt=""></p><p><img src="static/clickhouse/mergetree5.png" alt=""></p><p><img src="static/clickhouse/mergetree6.png" alt=""></p><p><img src="static/clickhouse/mergetree7.png" alt=""></p><h3 id="Distributed"><a href="#Distributed" class="headerlink" title="Distributed"></a>Distributed</h3><p>相当于数据库的视图，并不存储数据，而是用来做分布式的写入和查询，与其他引擎结合使用。</p><p>一种集群拓扑结构：<br><img src="static/clickhouse/distributed.jpg" alt=""></p><h2 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题"></a>实际问题</h2><ol><li>关键字大小写敏感，对强转支持不太好。</li><li>特性上不支持事务，不支持update/delete。</li><li>关于分布式<br><img src="static/clickhouse/question_clickhouse1.png" alt=""></li><li>在分布式表上执行count()时，发现结果不一致。</li></ol><p>暂无解决办法。问题:<a href="https://github.com/yandex/ClickHouse/issues/1443" target="_blank" rel="noopener">https://github.com/yandex/ClickHouse/issues/1443</a></p><ol start="5"><li>查询时，内存超出限制：</li></ol><pre><code>Progress: 4.84 million rows, 42.70 MB (45.34 million rows/s., 399.59 MB/s.)  87%Received exception from server:Code: 241. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Memory limit (for query) exceeded: would use 12.15 GiB (attempt to allocate chunk of 4294967296 bytes), maximum: 9.31 GiB.0 rows in set. Elapsed: 48.023 sec. Processed 4.84 million rows, 42.70 MB (100.89 thousand rows/s., 889.21 KB/s.)Progress: 4.89 million rows, 43.81 MB (23.23 thousand rows/s., 208.09 KB/s.)  88%Received exception from server:Code: 241. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Memory limit (for query) exceeded: would use 80.15 GiB (attempt to allocate chunk of 17179869184 bytes), maximum: 74.51 GiB.3668208133 rows in set. Elapsed: 235.536 sec. Processed 4.89 million rows, 43.81 MB (20.77 thousand rows/s., 186.00 KB/s.)Received exception from server (version 1.1.54362):Code: 241. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Memory limit (for query) exceeded: would use 96.14 GiB (attempt to allocate chunk of 17179869184 bytes), maximum: 93.13 GiB.8816716667 rows in set. Elapsed: 428.543 sec. Processed 4.89 million rows, 43.81 MB (11.41 thousand rows/s., 102.23 KB/s.)</code></pre><p>当处理的数据集超过设定的阈值以后，会触发限制，并返回已经处理好的结果。</p><p>解决办法：<br>修改max_memory_usage的值（user.xml）</p><p>不同的节点可以处理设置不同。在高并发分流的时候，尽量把query分摊到各个机器上，否则会将某一节点资源耗尽。</p><p>这个问题导致Clickhouse的高并发特性很差。在前端暴露端口时，不能单独暴露一个host,需要做成有命名空间的方式，或者有个query平衡器的机制。</p><p>解决办法：<br>就上述问题，发现一个公司采用如下架构：</p><p><img src="static/clickhouse/clickhouse_rebalance.jpg" alt=""></p><p>即：域名轮询。</p><h2 id="SQL-实战"><a href="#SQL-实战" class="headerlink" title="SQL 实战"></a>SQL 实战</h2><p>第一步建表：</p><pre><code>create table test_analysis (created_at DateTime, dt Date,  user String,  page_id String ) ENGINE=MergeTree(dt, (user, dt), 8192);</code></pre><p>插入测试数据：</p><pre><code>insert into table test_analysis Format Values(&#39;2018-4-24 18:45&#39;,&#39;2018-4-24&#39;,&#39;A&#39;,&#39;首页&#39;),(&#39;2018-4-24 18:46&#39;,&#39;2018-4-24&#39;,&#39;A&#39;,&#39;购物车&#39;),(&#39;2018-4-24 18:45&#39;,&#39;2018-4-24,&#39;B&#39;,&#39;首页&#39;),(&#39;2018-4-24 18:48&#39;,&#39;2018-4-24&#39;,&#39;B&#39;,&#39;商品详情&#39;),(&#39;2018-4-24 18:49&#39;,&#39;2018-4-24&#39;,&#39;B&#39;,&#39;购物车&#39;),(&#39;2018-4-24 18:46&#39;,&#39;2018-4-24&#39;,&#39;C&#39;,&#39;商品详情&#39;);</code></pre><p>第二步，建立模型：</p><pre><code>SELECT `user`,       created_at,       page_id,       gap1/60 AS &quot;与第一个动作的间隔时间&quot;,       if(gap1 == 0, 0, runningDifference(gap1)/60) AS &quot;与上一个动作的间隔时间&quot;FROM  (SELECT `user`,          created_at,          fist_created_at,          page_id,          created_at-fist_created_at AS gap1   FROM test_analysis ANY   LEFT JOIN     (SELECT `user` ,             min(created_at) AS fist_created_at      FROM test_analysis      GROUP BY `user`) using(`user`)) AS t</code></pre><p>获得的结果：</p><p><a href="http://www.clickhouse.com.cn/topic/5adf0d0a9d28dfde2ddc5fb2" target="_blank" rel="noopener">clickhouse 实战</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><h3 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h3><p><a href="https://clickhouse.yandex/docs/en/single/#introduction" target="_blank" rel="noopener">What is ClickHouse</a></p><p>官方写的体系结构文章：<a href="https://clickhouse.yandex/docs/en/development/architecture/" target="_blank" rel="noopener">Overview of ClickHouse architecture</a></p><p>翻译:<a href="http://www.clickhouse.com.cn/topic/5a3df7b02141c2917483557c" target="_blank" rel="noopener">ClickHouse 内部架构介绍</a></p><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p><a href="https://kuaibao.qq.com/s/20180409G06IIM00?refer=spider" target="_blank" rel="noopener">ClickHouse相关配置剖析</a></p><p><a href="http://note.abeffect.com/note/articles/2017/12/18/1513590469620.html" target="_blank" rel="noopener">ClickHouse的分布式引擎</a></p><h3 id="用户权限"><a href="#用户权限" class="headerlink" title="用户权限"></a>用户权限</h3><p><a href="https://www.jianshu.com/p/e339336e7bb9" target="_blank" rel="noopener">ClickHouse 用户名密码设置</a></p><p><a href="http://www.cnblogs.com/gomysql/p/6708796.html" target="_blank" rel="noopener">ClickHouse之访问权限控制</a></p><h3 id="引擎介绍"><a href="#引擎介绍" class="headerlink" title="引擎介绍"></a>引擎介绍</h3><p><a href="https://www.jianshu.com/p/48dbf2db2765" target="_blank" rel="noopener">ClickHouse MergeTree引擎介绍</a></p><p><a href="http://www.clickhouse.com.cn/topic/5a3e768d2141c2917483557e" target="_blank" rel="noopener">ClickHouse Distribute 引擎深度解读</a></p><p><a href="http://www.linkedkeeper.com/detail/blog.action?bid=1117" target="_blank" rel="noopener">实时大数据分析引擎ClickHouse介绍</a></p><h3 id="数据同步"><a href="#数据同步" class="headerlink" title="数据同步"></a>数据同步</h3><p>kafka-&gt;Clickhouse：<a href="http://jackpgao.github.io/2017/12/27/ClickHouse-with-Hangout/" target="_blank" rel="noopener">Hangout with ClickHouse</a></p><p>mysql-&gt;Clickhouse：<a href="http://jackpgao.github.io/2018/02/04/ClickHouse-Use-MySQL-Data/" target="_blank" rel="noopener">使用ClickHouse一键接管MySQL数据分析</a></p><h3 id="杂文"><a href="#杂文" class="headerlink" title="杂文"></a>杂文</h3><p>奶clickhouse的文章：<a href="https://zhuanlan.zhihu.com/p/33371816" target="_blank" rel="noopener">ClickHouse Beijing Meetup-数据分析领域的黑马-ClickHouse-新浪-高鹏</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;1年前，用过Greenplum，这是第一次接触MPP结构的OLAP系统。今天市面上常见的MPP架构工具，还有Clickhouse和Palo等。(当然，SQL on Hadoop体系的Presto和Impala也算是MPP结构，只是数据存储方面没有自己的东西，都是依赖hdfs,mysql等。)&lt;/p&gt;
&lt;p&gt;工作需要，对Clickhouse进行学习。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Clickhouse" scheme="http://guzhenping.com/tags/Clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库学习（概念篇）</title>
    <link href="http://guzhenping.com/2018/05/24/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AD%A6%E4%B9%A0--%E6%A6%82%E5%BF%B5%E7%AF%87/"/>
    <id>http://guzhenping.com/2018/05/24/数据仓库学习--概念篇/</id>
    <published>2018-05-24T09:04:48.000Z</published>
    <updated>2018-12-18T03:03:06.769Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>写点数据仓库（DW）的一些常用知识,架构等方面。加深基本功，保持进阶的心。</p><a id="more"></a><h2 id="典型架构"><a href="#典型架构" class="headerlink" title="典型架构"></a>典型架构</h2><p><img src="static/ScreenShot2017-01-13at11.09.58.png" alt="数据仓库系统架构"></p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul><li>数据仓库</li></ul><blockquote><p>数据仓库是一个面向主题的、集成的、非易失的（nonvolatile）、随时间变化的（time-variant）用来支持管理人员决策的数据集合。</p></blockquote><blockquote><p>—-William H.Inmon</p></blockquote><p>以上也是数据仓库区别于业务系统的4个特征。</p><ul><li>OLAP</li></ul><p>联机分析处理（On-line Analytical Processing），该概念由数据库创始人E. F. Codd于1993年提出。OLAP理事会（OLAP Council）进一步完善：OLAP是一种软件技术，它使分析人员、经理和执行官能够迅速、一致、交互的从各方面观察信息，以达到深入理解数据的目的。</p><ul><li>OLTP</li></ul><p>联机事务处理（On-line Transaction Processing）</p><ul><li>ODS </li></ul><p>操作数据存储（Operational Data Store, 简称ODS）。在数据仓库系统中，ODS存储了原始数据经过集成统一后的数据。</p><ul><li>DW</li></ul><p>数据仓库（Data Warehouse，简称DW）。在数据仓库系统中，DW数据库存储了整个企业的所有历史数据，是狭义上的数据仓库。DW数据库需要满足企业数据分析的各种需求，以及各个部门建设数据集市的需求，通常存储企业的基础数据和通用数据。</p><ul><li>数据集市(Data Market)</li></ul><p>数据集市是指针对特定部门和主题的小型数据仓库，数据从DW中获取。</p><ul><li>ETL</li></ul><p>ETL（Extract，Transform，Load）表示抽取、转换和装载，数据从多个同构或异构的数据源抽取出来，经过自定义的转换操作，最终装载进入目标表的过程叫做一次ETL。ETL是数据进入ODS、DW、Data Market的主要方式。</p><h2 id="建模方法"><a href="#建模方法" class="headerlink" title="建模方法"></a>建模方法</h2><ul><li>维度建模法（Dimensional Modeling）</li></ul><p>Ralph Kimball主张。</p><blockquote><p>维度建模以分析决策的需求为出发点构建模型，一般有较好的大规模复杂查询的响应性能，更直接面向业务，典型的代表是我们比较熟知的星形模型，以及在一些特殊场景下适用的雪花模型。</p></blockquote><ul><li>实体-关系建模（Entity-Relationship Medeling）</li></ul><p>实体-关系建模也叫做第三范式建模（Third Normal Form, 3NF）,是William H.Inmon主张的一种数据仓库建模方法。</p><blockquote><p>用实体加关系描述的数据模型描述企业业务架构，在范式理论上符合3NF，其是站在企业角度进行面向主题的抽象，而不是针对某个具体业务流程的，它更多是面向数据的整合和一致性治理，正如Inmon所希望达到的“single version of the truth”。</p><p>From: <a href="http://lxw1234.com/archives/2018/01/890.htm" target="_blank" rel="noopener">http://lxw1234.com/archives/2018/01/890.htm</a></p></blockquote><h2 id="数据与建模"><a href="#数据与建模" class="headerlink" title="数据与建模"></a>数据与建模</h2><p><a href="https://www.zhihu.com/question/19955124" target="_blank" rel="noopener">OLAP中roll-up和drill-down和slicing？</a></p><p><img src="static/olap.jpg" alt=""><br>真正好的建模，应该契合这些功能转换。见过Tableau做的很好，但不算是OLAP系统自带的功能。</p><h2 id="数仓进化史"><a href="#数仓进化史" class="headerlink" title="数仓进化史"></a>数仓进化史</h2><p>比较推荐这篇文章：<a href="http://lxw1234.com/archives/2016/03/624.htm" target="_blank" rel="noopener">从数据仓库到大数据，数据平台这25年是怎样进化的？</a></p><p>在架构方面，偏理论一点的文章：<a href="http://www.clickhouse.com.cn/topic/5ae13a599d28dfde2ddc5fc5#5aea82689d28dfde2ddc6014" target="_blank" rel="noopener">大数据分析的下一代架构–IOTA</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://segmentfault.com/a/1190000002672666" target="_blank" rel="noopener">hadoop HDFS常用文件操作命令</a></li><li><a href="http://book.51cto.com/art/201409/452359.htm" target="_blank" rel="noopener">HDFS 文件操作命令</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;写点数据仓库（DW）的一些常用知识,架构等方面。加深基本功，保持进阶的心。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="数据仓库" scheme="http://guzhenping.com/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Redash权限管理</title>
    <link href="http://guzhenping.com/2018/05/21/Redash%20%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/"/>
    <id>http://guzhenping.com/2018/05/21/Redash 权限管理/</id>
    <published>2018-05-21T09:04:48.000Z</published>
    <updated>2018-11-30T02:42:24.499Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Reash是一个数据查询平台，必定会涉及权限管理。主要由3个概念：组（group）,用户（user）， 数据源（data source）。</p><p>group在最上层，一个group对应多个user 和 data source。反之也可行，但不利于权限的管理。</p><p>权限功能主要基于组（group）和所属数据源(data source)来控制。一个用户（user）必须属于一个或多个组，当新用户进入时，该用户默认在“default组”。在新增一个数据源时，该数据源默认归属于”default组”。</p><a id="more"></a><h2 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h2><p>数据源有两个权限设置：</p><ul><li>Full access，即：该组的用户可操作被保存的查询（可以改SQL源码），创建一个新的查询</li><li>View Only access，即：该组的用户只能阅读被保存的查询（看不到SQL源码）及其结果</li></ul><p>对于不需要写SQL取结果的用户群来说，应该与需要写SQL取结果的用户群体区分在不同组（group）中。比如：基础架构查看报表组，基础架构制作报表组。</p><p>如果想对一个组的用户做table级别的查询限制，官方提供的方案：</p><blockquote><p>The idea is to leverage your database’s security model and hence create a user with access to the tables/columns you want to give access to. Create a data source that’s using this user and then associate it with a group of users who need this level of access.</p></blockquote><p>翻译过来：在建立数据源时，配置一个带有权限控制的数据库用户即可。比如，连接mysql时，用一个只能查询测试db/table的用户名进行连接。把这个数据源赋给某个组，然后该组的所有用户，只能看到这个数据源里的测试db/table。</p><h2 id="组"><a href="#组" class="headerlink" title="组"></a>组</h2><p>组可设置的权限有：</p><ul><li>admin/super_admin,管理员/超级管理员，可用所用功能</li><li>create_dashboard,创建dashboard</li><li>create_query,创建SQL查询</li><li>edit_dashboard,编辑自己/别人的dashboard</li><li>edit_query,编辑自己/别人的SQL查询</li><li>view_query,查看已经存在的SQL</li><li>view_source,查看SQL源码</li><li>execute_query,执行SQL</li><li>list_users,看到所有用户</li><li>schedule_query,设置定时刷新</li><li>list_dashboards,看到所用的dashboard</li><li>list_alerts,看到所有的提醒任务</li><li>list_data_sources,看到所有的数据源</li></ul><p>将以上权限赋给不同的组，每个组的用户就可以实现不同的功能。Redash不强调对用户做太多的权限控制，因为一个用户必须要归属于一个组。所以，对组做现在即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Reash是一个数据查询平台，必定会涉及权限管理。主要由3个概念：组（group）,用户（user）， 数据源（data source）。&lt;/p&gt;
&lt;p&gt;group在最上层，一个group对应多个user 和 data source。反之也可行，但不利于权限的管理。&lt;/p&gt;
&lt;p&gt;权限功能主要基于组（group）和所属数据源(data source)来控制。一个用户（user）必须属于一个或多个组，当新用户进入时，该用户默认在“default组”。在新增一个数据源时，该数据源默认归属于”default组”。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="可视化" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="Redash" scheme="http://guzhenping.com/tags/Redash/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发学习（Kylin）</title>
    <link href="http://guzhenping.com/2018/01/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0--Kylin/"/>
    <id>http://guzhenping.com/2018/01/24/大数据开发学习--Kylin/</id>
    <published>2018-01-24T09:04:48.000Z</published>
    <updated>2018-11-30T02:55:33.282Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-Kylin介绍"><a href="#一-Kylin介绍" class="headerlink" title="一 Kylin介绍"></a>一 Kylin介绍</h2><p>kylin是一款OLAP工具，背靠Hadoop，HBase，Spark，Kafka等大山，提供神奇体验。</p><a id="more"></a><h2 id="Kylin-安装与启动"><a href="#Kylin-安装与启动" class="headerlink" title="Kylin 安装与启动"></a>Kylin 安装与启动</h2><p>除了常规环境，还需要hadoop的historyjobserver启动。</p><p>具体参见官网，启动较为容易。</p><h2 id="度量计算"><a href="#度量计算" class="headerlink" title="度量计算"></a>度量计算</h2><p>目前提供9种计算方法。</p><ol><li><p>sum</p></li><li><p>min </p></li></ol><ul><li>max</li><li>count</li><li>count_distinct</li><li>top_n</li><li>raw</li><li>extended_column</li><li>percentile</li></ul><h2 id="建模心得"><a href="#建模心得" class="headerlink" title="建模心得"></a>建模心得</h2><p>在通过星形模型建立事实表+维度表的过程中，操作比较复杂。但是，通过view的方式，就比较简单。</p><p>view建模的方式，所有结果都在一张表里，只需要对该表进行维度和度量的划分即可。</p><h3 id="建模结果"><a href="#建模结果" class="headerlink" title="建模结果"></a>建模结果</h3><p>测试结果1：</p><p><img src="static/kylin建模2.png" alt=""></p><p>测试结果2：<br><img src="static/kylin建模.png" alt=""></p><h2 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题"></a>实际问题</h2><p>问题1：在同一个project里的Insight，可以看到所用cube的维度，但是不能共用，会报：No model found for rel</p><p>问题2：kylin不支持复杂的列，比如map,array类型。问题参见：<br><a href="https://mail-archives.apache.org/mod_mbox/kylin-dev/201512.mbox/%3C565D565C.3060205@jd.com%3E" target="_blank" rel="noopener">https://mail-archives.apache.org/mod_mbox/kylin-dev/201512.mbox/%3C565D565C.3060205@jd.com%3E</a></p><p>解决方法：创建该表的视图，剔除复杂列</p><p>问题3：kylin建model时，想做增量构建，在指定某列时间作为partition时，该列内容应该满足‘yyyy-MM-dd HH:mm:ss’的样子，不要使用unix_timestamp的类型。否则，构建后cube大小为零。 </p><p>问题4：kylin建cube所用的字段最好不要采用kylin 关键字，例如:year, month, day, hour等。否则写SQL时，不太友好。例如：</p><pre><code>想查一段 简单的pv和uv SQL，select platform, year, month, day ,count(*) as pv count(distinct guid) as uv from kylin_tracking_view group by platform, year, month, day 在Kylin的查询界面中，应当这么写:select platform,    &quot;YEAR&quot;,    &quot;MONTH&quot;,    &quot;DAY&quot;,    count(*) as pv,    count(distinct guid) as uvfrom kylin_tracking_view group by platform,    &quot;YEAR&quot;,    &quot;MONTH&quot;,    &quot;DAY&quot;可以看出，关键词必须全部大写，且被双引号(必须是双引号，单引是自定义常量)包住。建议提前规范好数据源，免得造成巨大的返工。</code></pre><p>问题5：build维度过不去<br>具体报错： [BadQueryDetector] service.BadQueryDetector:160 : System free memory less than 100 MB. 0 queries running</p><p>暂无解决办法。</p><p>这个问题可能是，kylin维度较多，把regionserver搞成僵死，进而导致的。</p><p>问题6： kylin 不支持中文列名。kylin在创建中间表时，会使用中文+英文的方式做拼接，这个过程会报错。</p><p>问题7：kylin 2.2.0 版本用户相关比较难搞。默认账户 ADMIN有问题，暂无好的解决办法。<br>一种解决方式：升级到 2.3.0版本，修复了230个bug，对用户更加友好。</p><p>问题8：kylin 2.3以后采用spark 2.1.2版本，需要进行相关的配置。<br>解决方式：<a href="https://kylin.apache.org/docs23/tutorial/cube_spark.html" target="_blank" rel="noopener">https://kylin.apache.org/docs23/tutorial/cube_spark.html</a></p><h2 id="Kylin的一些问题"><a href="#Kylin的一些问题" class="headerlink" title="Kylin的一些问题"></a>Kylin的一些问题</h2><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p>关于Kylin的架构和原理，有图可供参考：<a href="http://blog.csdn.net/lvguichen88/article/details/53054745" target="_blank" rel="noopener">Kylin 的架构和原理</a></p><p>Kylin比较详细的介绍：<a href="http://tech.meiyou.com/?p=97" target="_blank" rel="noopener">Kylin对大数据量的多维分析</a></p><p>关于Kylin Cube构建原理，落地到HBase的过程: <a href="https://blog.bcmeng.com/post/kylin-cube.html" target="_blank" rel="noopener">Apache Kylin Cube 构建原理</a></p><p>关于Kylin SQL 语法：<a href="http://calcite.apache.org/docs/reference.html" target="_blank" rel="noopener">SQL language</a></p><p>关于Kylin的关键字：<br><a href="https://github.com/apache/kylin/blob/4d50b26972bb7bbaff852172990e0f189f987673/core-metadata/src/main/java/org/apache/kylin/source/adhocquery/HivePushDownConverter.java" target="_blank" rel="noopener">关键字源码</a></p><p>关于一次正常查询的运行原理：<a href="https://zhuanlan.zhihu.com/p/30613434" target="_blank" rel="noopener">Kylin进阶之路</a></p><p>Kylin使用calcite做sql解析，可以参考calcite的语法文档：<a href="https://calcite.apache.org/" target="_blank" rel="noopener">https://calcite.apache.org/</a></p><h3 id="维度问题"><a href="#维度问题" class="headerlink" title="维度问题"></a>维度问题</h3><p>关于维度的聚合组中各个含义，请参考<br><a href="https://kylin.apache.org/blog/2016/02/18/new-aggregation-group/" target="_blank" rel="noopener">https://kylin.apache.org/blog/2016/02/18/new-aggregation-group/</a></p><p>Kylin Mandatory Dimension(必要维度)：<a href="https://mp.weixin.qq.com/s?__biz=MzAwODE3ODU5MA==&amp;mid=2653077943&amp;idx=1&amp;sn=007d2ba345d0e25ec12807aa47f9913d&amp;chksm=80a4bf46b7d33650465d33e20dac7edc09a7ad9308d77de6a501685c8ae00cba661c1d612074&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">【技术帖】Apache Kylin高级设置： 必要维度 （Mandatory Dimension）原理解析</a></p><p>Kylin Hierarchy Dimension(层级维度)：<a href="https://mp.weixin.qq.com/s?__biz=MzAwODE3ODU5MA==&amp;mid=2653077929&amp;idx=1&amp;sn=c76ed1fbb745945a077d9ca99f159a4d&amp;chksm=80a4bf58b7d3364e0346ad9c433d4e32c57d45f41b361ae653c64c7fcebab21238793d2f66cb&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">【技术帖】Apache Kylin 高级设置：层级维度（Hierarchy Dimension）原理解析</a></p><p>Kylin Joint Dimension(联合维度)：<a href="https://mp.weixin.qq.com/s?__biz=MzAwODE3ODU5MA==&amp;mid=2653077926&amp;idx=1&amp;sn=a0037628bd102ec8e607d67204cbfa7c&amp;chksm=80a4bf57b7d336419896c9e801a51f08ead2f7727d0d0ec0f9e3b7799ae3c302ebea54f93cc0&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">【技术帖】Apache Kylin 高级设置：联合维度（Joint Dimension）原理解析</a></p><p>Kylin Aggregation Group(聚合组)：<a href="https://mp.weixin.qq.com/s?__biz=MzAwODE3ODU5MA==&amp;mid=2653077921&amp;idx=1&amp;sn=89ae88bc63e71098166b74df7106c7bf&amp;chksm=80a4bf50b7d3364692903aac3e901d09a516a8ff635e690e1e22b1d96abb4b2925c98cdace82&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">【技术帖】Apache Kylin 高级设置：聚合组（Aggregation Group）原理解析</a></p><h3 id="Server端"><a href="#Server端" class="headerlink" title="Server端"></a>Server端</h3><p>配置负载均衡：<a href="https://mp.weixin.qq.com/s?__biz=MzAwODE3ODU5MA==&amp;mid=2653077900&amp;idx=1&amp;sn=5bcadd8e5bf1f65c7226344b2f05320e&amp;chksm=80a4bf7db7d3366ba1901296110d51fccd0f41957003d90771db2c238161c41ee5ec555dc334&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">【技术贴】如何部署Apache Kylin集群实现负载均衡？</a></p><h3 id="进阶优化"><a href="#进阶优化" class="headerlink" title="进阶优化"></a>进阶优化</h3><p>Kylin cube算法：<a href="http://www.infoq.com/cn/articles/apache-kylin-algorithm" target="_blank" rel="noopener">Apache Kylin的快速数据立方体算法——概述</a></p><p>Kylin cube介绍：<a href="http://blog.csdn.net/yu616568/article/details/50570536" target="_blank" rel="noopener">Kylin使用之创建Cube和高级设置</a></p><p>别人的Cube优化案例：<a href="http://www.jianshu.com/p/1e82e5dddae2" target="_blank" rel="noopener">Apache Kylin cube优化指南</a></p><p>优化案例：<a href="https://zhuanlan.zhihu.com/p/29084008" target="_blank" rel="noopener">Apache Kylin 深入Cube和查询优化 提升Cube</a></p><h3 id="API开发"><a href="#API开发" class="headerlink" title="API开发"></a>API开发</h3><p>脚本触发增量更新：<a href="http://blog.csdn.net/aaronhadoop/article/details/52806486" target="_blank" rel="noopener">Kylin定时增量build</a></p><p>Kylin+superset 可视化方案案例：<a href="http://zhuanlan.zhihu.com/p/26628057" target="_blank" rel="noopener">Kylin初体验总结</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>很不错的博文：</p><ul><li><a href="http://www.jianshu.com/p/6eadb77d091c" target="_blank" rel="noopener">Apache Kylin 框架介绍</a></li><li><a href="http://www.infoq.com/cn/articles/kylin-apache-in-meituan-olap-scenarios-practice" target="_blank" rel="noopener">Apache Kylin在美团数十亿数据OLAP场景下的实践</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-Kylin介绍&quot;&gt;&lt;a href=&quot;#一-Kylin介绍&quot; class=&quot;headerlink&quot; title=&quot;一 Kylin介绍&quot;&gt;&lt;/a&gt;一 Kylin介绍&lt;/h2&gt;&lt;p&gt;kylin是一款OLAP工具，背靠Hadoop，HBase，Spark，Kafka等大山，提供神奇体验。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据开发" scheme="http://guzhenping.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/"/>
    
      <category term="OLAP" scheme="http://guzhenping.com/tags/OLAP/"/>
    
      <category term="Kylin" scheme="http://guzhenping.com/tags/Kylin/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库学习(维度建模)</title>
    <link href="http://guzhenping.com/2017/07/11/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AD%A6%E4%B9%A0(%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1)/"/>
    <id>http://guzhenping.com/2017/07/11/数据仓库学习(维度建模)/</id>
    <published>2017-07-11T09:04:48.000Z</published>
    <updated>2018-12-14T05:09:54.312Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前做了一个关于数仓的talk, 给小白用户做了一些建模的举例，特地放在这里。都是比较常见模型。</p><a id="more"></a><h2 id="一-雪花模型"><a href="#一-雪花模型" class="headerlink" title="一 雪花模型"></a>一 雪花模型</h2><p><img src="static/运营活动雪花模型.png" alt=""></p><h2 id="二-星形模型"><a href="#二-星形模型" class="headerlink" title="二 星形模型"></a>二 星形模型</h2><p><img src="static/运营活动星形模型.png" alt=""></p><h2 id="三-雪花-vs-星形"><a href="#三-雪花-vs-星形" class="headerlink" title="三 雪花 vs 星形"></a>三 雪花 vs 星形</h2><p>可参考这篇：<br><a href="http://blog.csdn.net/nisjlvhudy/article/details/7889422" target="_blank" rel="noopener">星型模型和雪花型模型比较</a></p><h2 id="四-参考资料"><a href="#四-参考资料" class="headerlink" title="四 参考资料"></a>四 参考资料</h2><ul><li><a href="https://www.jianshu.com/p/17baa9f96ca7" target="_blank" rel="noopener">漫谈数据仓库之维度建模</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;之前做了一个关于数仓的talk, 给小白用户做了一些建模的举例，特地放在这里。都是比较常见模型。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="数据仓库" scheme="http://guzhenping.com/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Python学习（迭代器&amp;生成器）</title>
    <link href="http://guzhenping.com/2017/05/22/Python%E5%AD%A6%E4%B9%A0--%E8%BF%AD%E4%BB%A3%E5%99%A8&amp;%E7%94%9F%E6%88%90%E5%99%A8/"/>
    <id>http://guzhenping.com/2017/05/22/Python学习--迭代器&amp;生成器/</id>
    <published>2017-05-22T09:04:48.000Z</published>
    <updated>2018-11-30T02:42:52.947Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一直对Python的生成器、yield相关的东西比较晕。也终于到了这一天，算算总账，一口气理解这些东西。</p><p>这篇文章的背景是对yield/生成器/协程知识的总结，用于自我提升。请静下心来读，走马观花就不会有自己的思考。</p><p>懂分享的人，一定会快乐。笔者的内心期待着大家一起进步。所以，一起来分享自己的理解，留下你的评论吧。</p><a id="more"></a><h2 id="行文介绍"><a href="#行文介绍" class="headerlink" title="行文介绍"></a>行文介绍</h2><p>因为这是一整块知识，请不要拆散理解（好处是更易融会贯通）。所以，从头到尾的必备知识：</p><ul><li>可迭代对象（iterable）</li><li>迭代器（iterator）</li><li>迭代（iterator）</li><li>yield表达式</li><li>生成器（generators）</li></ul><p>本文会分别介绍上面的知识，最后介绍协程。剧透一下，协程和生成器很相似，所以学习协程，必备生成器相关的知识。</p><h2 id="啥是iterable？"><a href="#啥是iterable？" class="headerlink" title="啥是iterable？"></a>啥是iterable？</h2><p>Python中任意的对象，只要它定义了可以返回一个迭代器的<strong>iter</strong>方法，或者定义了可以支持下标索引的<strong>getitem</strong>方法，那么它就是一个可迭代对象。</p><p>简单说，可迭代对象就是能提供迭代器的任意对象。</p><h2 id="啥是iterator？"><a href="#啥是iterator？" class="headerlink" title="啥是iterator？"></a>啥是iterator？</h2><p>任意对象，只要定义了__iter__方法和next(Python2) 或者<strong>next</strong>（Python3）方法，它就是一个迭代器。__iter__()返回迭代器对象本身；next()或者__next__()返回容器的下一个元素，在结尾时引发StopIteration异常退出。</p><p>对于可迭代对象，可以使用内建函数iter()来获取它的迭代器对象。</p><pre><code># python3中&gt;&gt;&gt; test = [1,2,3] &gt;&gt;&gt; item = iter(test)    # 获取迭代器对象&gt;&gt;&gt; print(item)    # 打印该对象的类型、地址&lt;list_iterator object at 0x10231ab38&gt;&gt;&gt;&gt; item.__next__()    # 获取第一个元素1&gt;&gt;&gt; item.__next__()    # 获取第二个元素2&gt;&gt;&gt; item.__next__()    # 获取第三个元素3&gt;&gt;&gt; item.__next__()    # 获取StopIterationTraceback (most recent call last):  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration</code></pre><p>可以对string和dict做上面的事情，看看效果。扩展一点：在for循环中，for语法会自动调用迭代器的<strong>next</strong>()或next()，并能在遇到StopIteration时正常退出循环。</p><p>作为强化，给出一个自定义iterator的实例：</p><pre><code>class MyIteratorFab():    &quot;&quot;&quot;Python3实现生成斐波那契数Fibonacci       输出最后一个数据不大于max    &quot;&quot;&quot;    def __init__(self, max):        self.max = max        self.a = 0        self.b = 1    def __iter__(self):        return self    def __next__(self):        &quot;&quot;&quot; 返回容器中下一个元素,没有(符合标准的)元素后, 抛出StopIteration            python2中请将__next__替换成next        &quot;&quot;&quot;        if self.b &lt;= self.max:            r = self.b    # 用于记录倒数第二个b的值,该值才是小于self.max的            self.a, self.b = self.b, self.a + self.b            return r    # 不可返回self.b, 该值会比self.max大一点点        else:            raise StopIteration()    # 这条代码很重要,删除这不能正常退出for循环# 测试test = MyIteratorFab(100)for item in test:    print(item)############################################## 附一段生成fibonacci数的函数，同上述自定义迭代器是# 同一功能.#############################################def fib_func2(max):    &quot;&quot;&quot;一个普通的的生产fibonacci数的函数       输出最后一个数据不大于max    &quot;&quot;&quot;    a, b = 0, 1    while b &lt;= max:        print(b)        a, b = b, a+b</code></pre><p>最后，在<a href="http://python.jobbole.com/81881/" target="_blank" rel="noopener">《Python迭代器和生成器》</a>一文中提到过：对于一个可迭代对象，如果它本身又是一个迭代器对象，那么没办法支持多次迭代。感兴趣可戳过去阅读。</p><p>文中给出该问题的解法是，对迭代器对象类再包一个可迭代对象，实现多次迭代。拿上述MyIteratorFab()自定义迭代器举例：</p><pre><code># 问题：fab = MyIteratorFab(10)print([item for item in fab])    # output: [1, 1, 2, 3, 5, 8]print([item for item in fab])    # output: []# 解决方法：class BetterFab():    &quot;&quot;&quot;实现生成斐波那契数Fibonacci       输出最后一个数据不大于max       可多次迭代    &quot;&quot;&quot;    def __init__(self, max):        self.max = max    def __iter__(self):        return MyIteratorFab(self.max)# 测试fab2 = BetterFab(10)print([item for item in fab2])    # output: [1, 1, 2, 3, 5, 8]print([item for item in fab2])    # output: [1, 1, 2, 3, 5, 8]# 暴力的解法，但不推荐：print([item for item in MyIteratorFab(10)])    # output: [1, 1, 2, 3, 5, 8]print([item for item in MyIteratorFab(10)])    # output: [1, 1, 2, 3, 5, 8]</code></pre><p>到此，iterator就算是说完了。这是三个以itera*开头的概念中最核心的一个概念。当然，它也是生成器（generator）的基础。</p><h2 id="啥是iteration？"><a href="#啥是iteration？" class="headerlink" title="啥是iteration？"></a>啥是iteration？</h2><p>提醒一下，这是一个名词。用简单的话讲，它就是从某个地方（比如一个列表）取出一个元素的过程。当我们使用一个循环来遍历某个东西时，这个过程本身就叫迭代。</p><h2 id="如何理解yield表达式？"><a href="#如何理解yield表达式？" class="headerlink" title="如何理解yield表达式？"></a>如何理解yield表达式？</h2><p>下面的内容摘自Python3官方文档翻译：</p><blockquote><pre><code>yield_atom ::=  &quot;(&quot; yield_expression &quot;)&quot;yield_expression ::=  &quot;yield&quot; [expression_list | &quot;from&quot; expression]</code></pre><p>yield表达式仅在定义生成器函数时使用，因此只能用在函数定义的主体中。在函数体中使用yield表达式会使该函数成为生成器。</p></blockquote><blockquote><p>当生成器函数被调用时，它返回一个称为生成器的迭代器。然后，生成器控制生成器函数的执行。当生成器的一个方法被调用时，执行开始。此时，执行进行到第一个yield表达式，在那里它被再次挂起，将expression_list的值返回给生成器的调用者。挂起，我们的意思是保留所有局部状态，包括局部变量的当前绑定，指令指针，内部计算栈和任何异常处理的状态。当通过调用其中一个生成器的方法来恢复执行时，函数可以像yield表达式只是另一个外部调用一样继续进行。恢复后的yield表达式的值取决于恢复执行的方法。如果使用__next__()（通常通过for或next()内置函数），则结果为None。否则，如果使用send()，则结果将是传递到该方法的值。</p><p>摘自：<a href="http://python.usyiyi.cn/translate/python_352/reference/expressions.html#yieldexpr" target="_blank" rel="noopener">http://python.usyiyi.cn/translate/python_352/reference/expressions.html#yieldexpr</a></p></blockquote><p>yield 是一个类似 return 的关键字，只是这个函数返回的是个生成器。</p><p>另外，官网也提到yield其实和Coroutine类似：</p><blockquote><p>所有这些使生成器函数与协程非常相似；它们产生多次，它们具有多个入口点并且它们的执行可以被挂起。唯一的区别是生成器函数不能控制在它yield后继续执行的位置；控制总是转移到生成器的调用者。</p></blockquote><h2 id="啥是generators？"><a href="#啥是generators？" class="headerlink" title="啥是generators？"></a>啥是generators？</h2><p>下面的内容摘自Python3官方文档翻译：</p><blockquote><pre><code>generator_expression ::=  &quot;(&quot; expression comp_for &quot;)&quot;</code></pre><p>生成器表达式产生一个新的生成器对象。它的语法与推导式的语法相同，除了它被括在括号中而不是括号或花括号中。</p><p>当生成器对象调用__next__()方法时，生成器表达式中使用的变量将被懒惰地计算（以与正常生成器相同的方式）。但是，最左边的for子句会立即被求值，所以它产生的错误可以在生成器表达式代码中的任何其它可能的错误之前发现。后续for子句无法立即计算，因为它们可能取决于之前的for循环。例如：(x*y for x in range(10) for y in bar(x))。</p><p>摘自：<a href="http://python.usyiyi.cn/translate/python_352/reference/expressions.html#yieldexpr" target="_blank" rel="noopener">http://python.usyiyi.cn/translate/python_352/reference/expressions.html#yieldexpr</a></p></blockquote><p>生成器也是一种迭代器，但是你只能对其迭代一次。这是因为它们并没有把所有的值存在内存中，而是在运行时生成值。你通过遍历来使用它们，要么用一个“for”循环，要么将它们传递给任意可以进行迭代的函数和结构。大多数时候生成器是以函数来实现的。然而，它们并不返回一个值，而是yield(暂且译作“生出”)一个值。</p><p>生成器最佳应用场景是：你不想同一时间将所有计算出来的大量结果集分配到内存当中，特别是结果集里还包含循环。</p><h2 id="协程（Coroutine）"><a href="#协程（Coroutine）" class="headerlink" title="协程（Coroutine）"></a>协程（Coroutine）</h2><p>常见于合作式多任务，迭代器，无限列表，管道。</p><p>协程是基于单个线程的，Python的线程是基于单核的并发实现。</p><blockquote><p>并发对应计算机中充分利用单核（一个CPU）实现（看起来）多个任务同时执行。实现并发编程可以用多进程、多线程、异步、协程。</p></blockquote><p>从上面这段引用可以看出：为啥很多人讨论协程与其他并发编程方式的异同优劣？但是今天我们不聊并发，所以关注该话题的童鞋请自己Google文章。</p><p>深挖Coroutine的本质:</p><blockquote><p>allowing multiple entry points for suspending and resuming execution at certain locations.</p></blockquote><p>允许多个入口对程序进行挂起、继续执行等操作。</p><p>这和生成器很相似，但有区别：</p><ul><li>生成器是数据的生产者</li><li>协程则是数据的消费者</li></ul><p>协程会消费掉发送给它的值。</p><p>协程常用的方法：<br>next()<br>send()<br>close()</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>有件事情必须要说明白：</p><p>我是宇宙中微不足道的一粒沙。即使作为沙粒，我想浸在湛蓝的海水中，也想沐浴在灿烂的阳光下，更想陪着孩童们构建沙滩城堡。自然清楚自己是一粒沙，但还是想要这样的生活。</p><p>所以，我愿意写这些微不足道的东西，来分享自己的价值。尽管，我也微不足道。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://blog.rainy.im/2016/04/07/python-thread-and-coroutine/" target="_blank" rel="noopener">Python 线程与协程</a></li><li><a href="http://pyzh.readthedocs.io/en/latest/the-python-yield-keyword-explained.html" target="_blank" rel="noopener">Python关键字yield的解释</a></li><li><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/" target="_blank" rel="noopener">Python yield 使用浅析</a></li><li><a href="http://www.cnblogs.com/rio2607/p/4440122.html" target="_blank" rel="noopener">Python高级编程之生成器(Generator)与coroutine(一):Generator</a></li><li><a href="http://www.cnblogs.com/rio2607/p/4456332.html" target="_blank" rel="noopener">Python高级编程之生成器(Generator)与coroutine(二):coroutine介绍</a></li><li><a href="http://www.cnblogs.com/rio2607/p/4472456.html" target="_blank" rel="noopener">Python高级编程之生成器(Generator)与coroutine(三):coroutine与pipeline(管道)和Dataflow(数据流_</a></li><li><a href="http://www.cnblogs.com/rio2607/p/4570353.html" target="_blank" rel="noopener">Python高级编程之生成器(Generator)与coroutine(四):一个简单的多任务系统</a></li><li><a href="http://python.jobbole.com/81881/" target="_blank" rel="noopener">Python迭代器和生成器</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;一直对Python的生成器、yield相关的东西比较晕。也终于到了这一天，算算总账，一口气理解这些东西。&lt;/p&gt;
&lt;p&gt;这篇文章的背景是对yield/生成器/协程知识的总结，用于自我提升。请静下心来读，走马观花就不会有自己的思考。&lt;/p&gt;
&lt;p&gt;懂分享的人，一定会快乐。笔者的内心期待着大家一起进步。所以，一起来分享自己的理解，留下你的评论吧。&lt;/p&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://guzhenping.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="Python" scheme="http://guzhenping.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>HA成功升级的总结</title>
    <link href="http://guzhenping.com/2017/02/02/HA%E6%88%90%E5%8A%9F%E5%8D%87%E7%BA%A7%E7%9A%84%E6%80%BB%E7%BB%93/"/>
    <id>http://guzhenping.com/2017/02/02/HA成功升级的总结/</id>
    <published>2017-02-02T09:04:48.000Z</published>
    <updated>2018-11-30T02:38:07.484Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>从升级到现在，一共过了1个半月，到昨天（20170511）总算踏实了。踩的那些坑，真的教会了笔者咋做人。升了，笔者并不后悔。当然脸被打了这么多下，也高兴不起来。</p><p>升级过程的所有问题，发生在当事人无法注意到、无法理解的地方。然而，只要时间充足，精力充沛，严谨严格，肯定能克服。</p><p>必须申明：本文是基于个体实际经验所写，难免片面，可能不具备参考价值。其次，本文是对一个已经存在数年有大数量的集群所做的一些升级，情况较为特殊。<br><a id="more"></a></p><h2 id="升级准备"><a href="#升级准备" class="headerlink" title="升级准备"></a>升级准备</h2><p>这里就不多说了，参见<a href="https://www.tapd.cn/20096511/markdown_wikis/#1120096511001000954" target="_blank" rel="noopener">《Hadoop Namenode HA升级》</a>。</p><p>总结一句，准备和成功概率成正比，有付出，才有收获。</p><h2 id="问题1：异构的配置文件"><a href="#问题1：异构的配置文件" class="headerlink" title="问题1：异构的配置文件"></a>问题1：异构的配置文件</h2><p>对于hadoop集群来说，很多人都觉得hadoop配置文件是一样的。错了，真的错了。hadoop集群支持异构模式。所以，配置可以不同。</p><p>在本次升级过程中，配置上，遇到的问题：个别机器的盘符不同于大部分机器，个别机器的datanode数据所在盘没有填写在<code>&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</code>下。</p><p>对于linux盘符不太熟悉的观众请戳：<a href="http://ilinuxkernel.com/?p=958" target="_blank" rel="noopener">《Linux硬盘盘符分配》</a></p><p>例如，有以下两种（及以上）配置：</p><pre><code>&lt;property&gt;       &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;       &lt;value&gt;/home/deploy/hadoopdata/datanode/,/mnt/sdb/hadoopdata/datanode/&lt;/value&gt;&lt;/property&gt;</code></pre><pre><code>&lt;property&gt;       &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;       &lt;value&gt;/home/deploy/hadoopdata/datanode/,/mnt/sdc/hadoopdata/datanode/,/mnt/sdd/hadoopdata/datanode/&lt;/value&gt;&lt;/property&gt;</code></pre><p>这种错误的配置，会导致namenode在启动过程中无法找到block，处于安全模式无法退出。对于集群停机不能超过5小时的公司/团体来说，只能强制立刻安全模式。此时，将会产生坏块。</p><p>对于hadoop集群来说，必须清除坏块。换句话说，因为配置的失误，会导致datanode丢数据。这种丢法，和没有挂上的那个/些盘有关，而且一丢就是丢一个盘，后果严重。</p><p>当然hadoop的块备份是大于等于2，如果只是一个盘，对于集群来说就相当于没有丢。反之，则是随机丢失块数据。</p><h2 id="问题2：Hadoop堆内存"><a href="#问题2：Hadoop堆内存" class="headerlink" title="问题2：Hadoop堆内存"></a>问题2：Hadoop堆内存</h2><p>先说结论：对于HA来说，两个namenode（active/standby）的堆内存应该要比hadoop集群metadata大至少1倍。这个结论并没有理论根据，但确实实践所得的最可靠数据。假设一个运行5年的集群有20G metadata数据，则namenode(a/s)需要40G以上。</p><p>为啥这么说？因为，namenode主备自动切换时(即主namenode异常，备namenode启动)，standby namenode需要将metadata读到进程堆内存中。堆内存不住，NameNode进程会报GC堆已满。GC堆相关问题请戳：<a href="http://www.cnblogs.com/dingyingsi/p/3760447.html" target="_blank" rel="noopener">《深入理解JVM—JVM内存模型》</a></p><p>理解主备热切，大致有两个要点，本文不多说。只点一下，第一，单节点Namenode启动过程发生了哪些事情；第二，HA集群namenode启动中发生了哪些事情。主要就是围绕FSImage和EditsLog展开。请戳下方链接：</p><ul><li><a href="http://blog.csdn.net/cnhk1225/article/details/50786785" target="_blank" rel="noopener">《NameNode启动过程详细剖析 NameNode中几个关键的数据结构 FSImage》</a></li><li><a href="http://blog.csdn.net/dabokele/article/details/51686257" target="_blank" rel="noopener">《Hadoop-2.X HA模式下的FSImage和EditsLog合并过程》</a></li></ul><p>对于拥有20G左右metadata的集群，standby需要10-20分钟内就可以启动(堆内存够大)。如果热切时，standby namenode的内存就16G,那么半个小时差不多只能走到25%左右。</p><p>堆内存大点很易理解，数据多嘛。而且，网上有很多hadoop在提交作业时报jvm堆内存不足的问题，可以参考。</p><p>笔者的小伙伴在解决standby namenode启动过慢这个问题时，通过修改安装目录的<code>../etc/hadoop/hadoop-env.sh</code>文件中的HADOOP_HEAPSIZE这个参数。一开始,该参数同集群metadata数据大小无异，第二次改成约1.5倍，第三次改成约2倍。</p><p>HADOOP_HEAPSIZE的值会成为本地（不是每台机器共用的，可以不同）的JVM的堆大小。本地的各个Java守护进程都会共享这个堆，此时进程NameNoe能满足快速启动的条件。虽然会有其他java进程也用，但是HA模式下的namenode没有过多的java进程（其实就是DFSZKFailoverController）。</p><h2 id="问题3：NameNode-RPC调用方式"><a href="#问题3：NameNode-RPC调用方式" class="headerlink" title="问题3：NameNode RPC调用方式"></a>问题3：NameNode RPC调用方式</h2><p>未升级前，连接hadoop集群通过<code>&lt;name&gt;fs.defaultFS&lt;/name&gt;</code>下的hdfs://IP：PORT来连。升级后，需要采用命名空间的方式：</p><pre><code>&lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://mainhadoop&lt;/value&gt;&lt;/property&gt;</code></pre><p>即：hdfs://命名空间。连接上命名空间后，zookeeper集群会自动分配程序连接到当前active的namenode。</p><p>对此，受影响较大的是一些执行脚本和已存在的hive表。执行脚本一般写死，只能一个一个修改。</p><p>对于hive表，需要进入到元数据所在数据库，修改数据Location指向。如果hive是以mysql作元数据存储，则需连上mysql，修改SDS和DBS两张表的数据。将“hdfs://ip:port/XXXXXXXXXX”改成新的hadoop命名空间。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;从升级到现在，一共过了1个半月，到昨天（20170511）总算踏实了。踩的那些坑，真的教会了笔者咋做人。升了，笔者并不后悔。当然脸被打了这么多下，也高兴不起来。&lt;/p&gt;
&lt;p&gt;升级过程的所有问题，发生在当事人无法注意到、无法理解的地方。然而，只要时间充足，精力充沛，严谨严格，肯定能克服。&lt;/p&gt;
&lt;p&gt;必须申明：本文是基于个体实际经验所写，难免片面，可能不具备参考价值。其次，本文是对一个已经存在数年有大数量的集群所做的一些升级，情况较为特殊。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://guzhenping.com/tags/Hadoop/"/>
    
      <category term="集群运维" scheme="http://guzhenping.com/tags/%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 2.7.2 HA升级方案</title>
    <link href="http://guzhenping.com/2017/01/02/HA%E5%8D%87%E7%BA%A7%E8%BF%87%E7%A8%8B/"/>
    <id>http://guzhenping.com/2017/01/02/HA升级过程/</id>
    <published>2017-01-02T09:04:48.000Z</published>
    <updated>2018-11-30T02:39:17.615Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本次升级主要对Hadoop的core-site.xml和hdfs-site.xml文件进行修改，暂时不涉及其他配置。</p><p>这次升级过程，大致是三步：备份数据文件，修改配置文件，启动集群。如果升级中发现异常，启动回滚方案。</p><h2 id="备份数据"><a href="#备份数据" class="headerlink" title="备份数据"></a>备份数据</h2><h3 id="备份配置文件"><a href="#备份配置文件" class="headerlink" title="备份配置文件"></a>备份配置文件</h3><p>需要执行fabric脚本，在每台机器上进行备份。将core-site.xml和hdfs-site.xml分别cp为core-site.xml.ha.back和hdfs-site.xml.ha.back</p><p>以下是本次要修改的core-site.xml和hdfs-site.xml配置文件原内容。</p><ul><li>core-site.xml</li></ul><pre><code>&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;fs.defaultFS&lt;/name&gt;                &lt;value&gt;hdfs://sha2hdpn01:9000&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;io.file.buffer.size&lt;/name&gt;                &lt;value&gt;131072&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;                &lt;value&gt;/home/deploy/hadoopdata/tmp&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hadoop.proxyuser.deploy.groups&lt;/name&gt;                &lt;value&gt;*&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hadoop.proxyuser.deploy.hosts&lt;/name&gt;                &lt;value&gt;*&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;              &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;              &lt;value&gt;*&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;              &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;              &lt;value&gt;*&lt;/value&gt;        &lt;/property&gt;         &lt;property&gt;                &lt;name&gt;fs.trash.interval&lt;/name&gt;                &lt;value&gt;10080&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                   &lt;name&gt;topology.script.file.name&lt;/name&gt;                &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/topology.sh&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;net.topology.script.file.name&lt;/name&gt;                &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/topology.sh&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><ul><li>hdfs-site.xml</li></ul><pre><code>&lt;configuration&gt;    &lt;property&gt;           &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;           &lt;value&gt;sha2hdpn02:50090&lt;/value&gt;    &lt;/property&gt;        &lt;property&gt;           &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;           &lt;value&gt;sha2hdpn01:50070&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;           &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;           &lt;value&gt;/home/deploy/hadoopdata/namenode&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;           &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;           &lt;value&gt;/home/deploy/hadoopdata/datanode/,/mnt/sdb/hadoopdata/datanode/&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;           &lt;name&gt;dfs.replication&lt;/name&gt;           &lt;value&gt;2&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;        &lt;value&gt;/home/deploy/hadoopdata/checkpoint&lt;/value&gt;        &lt;final&gt;true&lt;/final&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;/name&gt;        &lt;value&gt;/home/deploy/hadoopdata/checkpoint&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;        &lt;value&gt;600&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt;        &lt;value&gt;107374182400&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.fsdataset.volume.choosing.policy&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;        &lt;value&gt;10485760&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;        &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/exclude&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;        &lt;value&gt;52428800&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.max.transfer.threads&lt;/name&gt;        &lt;value&gt;8192&lt;/value&gt;        &lt;description&gt;            Specifies the maximum number of threads to use for transferring data            in and out of the DN. default is 4096,###Modified###        &lt;/description&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.datanode.socket.write.timeout&lt;/name&gt;         &lt;value&gt;480000&lt;/value&gt;    &lt;/property&gt;   &lt;property&gt;         &lt;name&gt;dfs.client.socket-timeout&lt;/name&gt;         &lt;value&gt;300000&lt;/value&gt;   &lt;/property&gt;   &lt;property&gt;         &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;         &lt;value&gt;8192&lt;/value&gt;   &lt;/property&gt;   &lt;property&gt;         &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;         &lt;value&gt;80&lt;/value&gt;   &lt;/property&gt;&lt;/configuration&gt;</code></pre><h3 id="备份namenode数据"><a href="#备份namenode数据" class="headerlink" title="备份namenode数据"></a>备份namenode数据</h3><p>需要关闭集群后，在sha2hdpn01上备份namenode的元数据,位置：<code>/home/deploy/hadoopdata</code>,大小：25G。</p><h3 id="备份secondary-namenode的数据"><a href="#备份secondary-namenode的数据" class="headerlink" title="备份secondary namenode的数据"></a>备份secondary namenode的数据</h3><p>需要关闭集群后，在sha2hdpn02上备份snn的元数据，位置：<code>/home/deploy/hadoopdata</code>, 大小：774G。</p><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><h3 id="修改core-stie-xml"><a href="#修改core-stie-xml" class="headerlink" title="修改core-stie.xml"></a>修改core-stie.xml</h3><pre><code>&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;fs.defaultFS&lt;/name&gt;                &lt;value&gt;hdfs://hacluster&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;io.file.buffer.size&lt;/name&gt;                &lt;value&gt;131072&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;                &lt;value&gt;/home/deploy/hadoopdata/tmp&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;            &lt;value&gt;sha2hb01:2181,sha2hb02:2181,sha2hb03:2181&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hadoop.proxyuser.deploy.groups&lt;/name&gt;                &lt;value&gt;*&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hadoop.proxyuser.deploy.hosts&lt;/name&gt;                &lt;value&gt;*&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;              &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;              &lt;value&gt;*&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;              &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;              &lt;value&gt;*&lt;/value&gt;        &lt;/property&gt;         &lt;property&gt;                &lt;name&gt;fs.trash.interval&lt;/name&gt;                &lt;value&gt;10080&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                   &lt;name&gt;topology.script.file.name&lt;/name&gt;                &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/topology.sh&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;net.topology.script.file.name&lt;/name&gt;                &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/topology.sh&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><h3 id="修改hdfs-site-xml"><a href="#修改hdfs-site-xml" class="headerlink" title="修改hdfs-site.xml"></a>修改hdfs-site.xml</h3><p>先删除secondary namenode配置，在添加和ha配置相关的内容。</p><pre><code>&lt;configuration&gt;    &lt;property&gt;           &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;           &lt;value&gt;/home/deploy/hadoopdata/namenode&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;           &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;           &lt;value&gt;/home/deploy/hadoopdata/datanode/,/mnt/sdb/hadoopdata/datanode/&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;           &lt;name&gt;dfs.replication&lt;/name&gt;           &lt;value&gt;2&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.nameservices&lt;/name&gt;        &lt;value&gt;hacluster&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.namenodes.hacluster&lt;/name&gt;        &lt;value&gt;nn1,nn2&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.hacluster.nn1&lt;/name&gt;        &lt;value&gt;sha2hdpn01:9000&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.hacluster.nn2&lt;/name&gt;        &lt;value&gt;sha2hdpn02:9000&lt;/value&gt;    &lt;/property&gt;     &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.hacluster.nn1&lt;/name&gt;        &lt;value&gt;sha2hdpn01:50070&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.hacluster.nn2&lt;/name&gt;        &lt;value&gt;sha2hdpn02:50070&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;        &lt;value&gt;qjournal://sha2hdpw46:8485; sha2hdpw47:8485;sha2hdpw48:8485/hacluster&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;        &lt;value&gt;/home/deploy/hadoopdata/journaldata&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.client.failover.proxy.provider.hacluster&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;      &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;      &lt;value&gt;            sshfence            shell(/bin/true)      &lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;      &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;      &lt;value&gt;/home/deploy/.ssh/id_rsa&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;        &lt;value&gt;30000&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt;        &lt;value&gt;107374182400&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.fsdataset.volume.choosing.policy&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;        &lt;value&gt;10485760&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;        &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/exclude&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;        &lt;value&gt;52428800&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.max.transfer.threads&lt;/name&gt;        &lt;value&gt;8192&lt;/value&gt;        &lt;description&gt;            Specifies the maximum number of threads to use for transferring data            in and out of the DN. default is 4096,###Modified###        &lt;/description&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.datanode.socket.write.timeout&lt;/name&gt;         &lt;value&gt;480000&lt;/value&gt;    &lt;/property&gt;   &lt;property&gt;         &lt;name&gt;dfs.client.socket-timeout&lt;/name&gt;         &lt;value&gt;300000&lt;/value&gt;   &lt;/property&gt;   &lt;property&gt;         &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;         &lt;value&gt;8192&lt;/value&gt;   &lt;/property&gt;   &lt;property&gt;         &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;         &lt;value&gt;80&lt;/value&gt;   &lt;/property&gt;&lt;/configuration&gt;</code></pre><h2 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h2><p>先启动zookeeper集群，并确定状态；</p><p>再启动journalnode集群，用于nn间数据同步（注意文件夹存储的位置和权限）；</p><p>在主节点上启动namenode；</p><p>在副节点上，同步namenode的元数据：<code>hdfs namenode -bootstandby</code>（metadata为7.3G, 注意磁盘空间及时长）；</p><p>在副节点上启动namenode;</p><p>在任意节点上，启动dfs: <code>sh start-dfs.sh</code>;</p><p>启动其他的。</p><h2 id="回滚方案"><a href="#回滚方案" class="headerlink" title="回滚方案"></a>回滚方案</h2><p>此处升级，主要是对core-site.xml和hdfs-site.xml文件进行修改，所以回滚方案的主要逻辑就是恢复这两份文件。</p><p>利用fabric写脚本，先每台机器上备份（cp命令），如需回滚，先关闭集群，再把该文件替换回来（mv命令）即可。</p><p>附脚本rollback_hadoop.py：</p><pre><code class="python">from fabric.api import run,sudo,roles,env,cd,executeenv.roledefs = {    &#39;all_node&#39;: [&#39;sha2hdpn01&#39;,&#39;sha2hdpn02&#39;,&#39;sha2hdpw01&#39;,&#39;sha2hdpw02&#39;,&#39;sha2hdpw03&#39;,&#39;sha2hdpw04&#39;,&#39;sha2hdpw05&#39;,&#39;sha2hdpw06&#39;,&#39;sha2hdpw07&#39;,&#39;sha2hdpw08&#39;,&#39;sha2hdpw09&#39;,&#39;sha2hdpw10&#39;,&#39;sha2hdpw11&#39;,&#39;sha2hdpw12&#39;,&#39;sha2hdpw13&#39;,&#39;sha2hdpw14&#39;,&#39;sha2hdpw15&#39;,&#39;sha2hdpw16&#39;,&#39;sha2hdpw17&#39;,&#39;sha2hdpw18&#39;,&#39;sha2hdpw19&#39;,&#39;sha2hdpw20&#39;,&#39;sha2hdpw21&#39;,&#39;sha2hdpw22&#39;,&#39;sha2hdpw23&#39;,&#39;sha2hdpw24&#39;,&#39;sha2hdpw25&#39;,&#39;sha2hdpw26&#39;,&#39;sha2hdpw27&#39;,&#39;sha2hdpw28&#39;,&#39;sha2hdpw29&#39;,&#39;sha2hdpw30&#39;,&#39;sha2hdpw31&#39;,&#39;sha2hdpw32&#39;,&#39;sha2hdpw33&#39;,&#39;sha2hdpw34&#39;,&#39;sha2hdpw35&#39;,&#39;sha2hdpw36&#39;,&#39;sha2hdpw37&#39;,&#39;sha2hdpw38&#39;,&#39;sha2hdpw39&#39;,&#39;sha2hdpw40&#39;,&#39;sha2hdpw41&#39;,&#39;sha2hdpw42&#39;,&#39;sha2hdpw43&#39;,&#39;sha2hdpw44&#39;,&#39;sha2hdpw45&#39;,&#39;sha2hdpw46&#39;,&#39;sha2hdpw47&#39;,&#39;sha2hdpw48&#39;],    &#39;namenode&#39;: [&#39;sha2hdpn01&#39;],    &#39;test_node&#39;: [&#39;sha2hdpw48&#39;]}env.user = &#39;deploy&#39;env.password = &#39;XXXXXX&#39; # yourself# env.shell = &#39;/bin/sh -c&#39;@roles(&#39;all_node&#39;)def showfile():    # run(&#39;ll /usr/local/hadoop-default/etc/hadoop&#39;)i    with cd(&#39;/usr/local/hadoop-default/etc/hadoop&#39;):        run(&quot;ls core-site.xml hdfs-site.xml&quot;)@roles(&#39;namenode&#39;)def start_hadoop():    print(&#39;start hadoop cluster...&#39;)    # run(&#39;sh /usr/local/hadoop-default/sbin/start-all.sh&#39;)    print(&#39;done...&#39;)@roles(&#39;all_node&#39;)#@roles(&#39;test_node&#39;)def backup():    print(&#39;start backup...&#39;)    with cd(&#39;/usr/local/hadoop-default/etc/hadoop&#39;):        run(&#39;cp core-site.xml core-site.xml.ha.back&#39;)        run(&#39;cp hdfs-site.xml hdfs-site.xml.ha.back&#39;)    print(&#39;done...&#39;)@roles(&#39;all_node&#39;)#@roles(&#39;test_node&#39;)def rollback():    print(&#39;start roolback...&#39;)    with cd(&#39;/usr/local/hadoop-default/etc/hadoop&#39;):        run(&#39;mv core-site.xml.ha.back core-site.xml&#39;)        run(&#39;mv hdfs-site.xml.ha.back hdfs-site.xml&#39;)    print(&#39;done...&#39;)def deploy():    execute(showfile)def run_backup():    execute(backup)def run_rollback():    execute(rollback)    execute(start_hadoop)</code></pre><h2 id="测试方案"><a href="#测试方案" class="headerlink" title="测试方案"></a>测试方案</h2><p>重跑airflow中金融/流量的报表任务，用于提交job，查看是否能够完整跑完。这些任务中有使用了hdfs及yarn的操作，如果成功，说明hadoop ha集群可以用。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>烧香拜佛，希望成功！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本次升级主要对Hadoop的core-site.xml和hdfs-site.xml文件进行修改，暂时不涉及其他配置。&lt;/p&gt;
&lt;p&gt;这次升级
      
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://guzhenping.com/tags/Hadoop/"/>
    
      <category term="集群运维" scheme="http://guzhenping.com/tags/%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习指南(HA配置)</title>
    <link href="http://guzhenping.com/2017/01/02/Hadoop%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97--HA%E9%85%8D%E7%BD%AE/"/>
    <id>http://guzhenping.com/2017/01/02/Hadoop学习指南--HA配置/</id>
    <published>2017-01-02T09:04:48.000Z</published>
    <updated>2018-11-30T02:50:33.953Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇介绍Hadoop的一些常用知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。</p><p>HA模式，主要是将namenode及resourcemanager都变成主备两个。这里先不讨论resourcemanager，主要针对namenode。</p><p>将namenode变成可主备自动切换的，主要是通过zookeeper集群对namenode的健康状态进行监控，然后选举一个健康的namenode做active(主)的，另一个成为standby(备)。因此，保证zookeeper集群的配置是正确且不易挂掉，是HA的基石。同时，注意HA升级过程中的相关进程的启动步骤即可完成。</p><a id="more"></a><p>官网有两种配置：NFS和QJM，两者区别参见:<a href="http://joshuasabrina.iteye.com/blog/1858448" target="_blank" rel="noopener">《HDFS v2 HA方案对比》</a></p><p>本文以NFS为例进行讨论。</p><p>首先声明环境：</p><ul><li>CentOS 6.5 64位</li><li>Hadoop 2.7.1</li><li>Java 1.8</li></ul><p>所有操作均基于拥有第一代hadoop集群的环境之上。同时满足Centos 6.5系统内网络互通、机器免密码登陆、防火墙关闭。重要的是/etc/hosts文件如下：</p><pre><code>192.168.20.2    hadoop1192.168.20.3    hadoop2192.168.20.4    hadoop3192.168.20.5    hadoop4192.168.20.6    hadoop5</code></pre><p>另外，温馨提示：在hadoop的各种文件配置中，最好不要出现空格。例如：</p><pre><code class="![](j)">&lt;property&gt;    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;    &lt;value&gt;hadoop2:2181,  hadoop3:2181,  hadoop4:2181&lt;/value&gt;&lt;/property&gt;</code></pre><p>hadoop将无法找到<code>hadoop3</code>和<code>hadoop4</code>这种IP。如果手贱多敲，那么将会浪费很多时间。</p><h2 id="进程功能介绍"><a href="#进程功能介绍" class="headerlink" title="进程功能介绍"></a>进程功能介绍</h2><p>zk:维护共享锁保证只有一个active的namenode</p><p>journalnode：在两个nn间同步元数据</p><h2 id="配置zookeeper集群"><a href="#配置zookeeper集群" class="headerlink" title="配置zookeeper集群"></a>配置zookeeper集群</h2><p>该集群最少需要3台机器（用于选举）。下面详述配置过程。</p><p>假设1： 3台机器是这样：</p><table><thead><tr><th>IP</th><th>标识</th></tr></thead><tbody><tr><td>192.168.20.3</td><td>hadoop2</td></tr><tr><td>192.168.20.4</td><td>hadoop3</td></tr><tr><td>192.168.20.5</td><td>hadoop4</td></tr></tbody></table><p>假设2： 下载完的zookeeper源码包位于：/home/deploy/zookeeper-3.4.9。</p><p>假设3：java环境为1.8版本，位于：/home/deploy/jdk1.8.0_111。</p><p>第一步，配置zoo.cfg:<br><code>vi /home/deploy/zookeeper-3.4.9/conf/zoo.cfg</code></p><p>修改配置:</p><pre><code># The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/home/deploy/zookeeper-3.4.9/data# logs dirdataLogDir=/home/deploy/zookeeper-3.4.9/logs# the port at which the clients will connectclientPort=2181server.1=hadoop3:2888:3888server.2=hadoop4:2888:3888server.3=hadoop2:2888:3888</code></pre><p>第二步，配置myid文件</p><p>在zoo.cfg中dataDir路径：dataDir=/home/deploy/zookeeper-3.4.9/data下，新建myid文件。<code>vi myid</code><br>在hadoop3的机器上，该文件内容为1，<br>hadoop4机器上，该文件内容为2，<br>hadoop5机器上，该文件内容为3。内容应保持同zoo.cfg中的server.x的x值相同</p><p>第三步，开/关zookeeper集群：</p><p>开：<code>sh /home/deploy/zookeeper-3.4.9/bin/zkServer.sh start</code></p><p>关：<code>sh /home/deploy/zookeeper-3.4.9/bin/zkServer.sh stop</code></p><p>请在将三台机器全部开启后，查看状态：<code>zkServer.sh status</code>。</p><h2 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h2><p>在原文件上进行添加:</p><pre><code>&lt;!-- 指定hdfs的nameservice为h01，需与dfs.nameservices一致 --&gt;&lt;property&gt;     &lt;name&gt;fs.defaultFS&lt;/name&gt;     &lt;value&gt;hdfs://mycluster&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt;    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;    &lt;value&gt;hadoop2:2181,hadoop3:2181,hadoop4:2181&lt;/value&gt;&lt;/property&gt;</code></pre><h2 id="配置hdfs-site-xml"><a href="#配置hdfs-site-xml" class="headerlink" title="配置hdfs-site.xml"></a>配置hdfs-site.xml</h2><p>这里有两步，第一步是删除关于secondary namenode的配置，第二步是添加HA的配置。</p><p>删：</p><pre><code>&lt;!-- 以下3个 property 的配置，是非HA模式下的，即一个集群只有一个namenode，在这里不可使用 --&gt;  &lt;property&gt;    &lt;name&gt;dfs.http.address&lt;/name&gt;    &lt;value&gt;h01.vm.com:50070&lt;/value&gt;    &lt;description&gt;Secondary get fsimage and edits via dfs.http.address&lt;/description&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.secondary.http.address&lt;/name&gt;    &lt;value&gt;h02.vm.com:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;    &lt;value&gt;/home/vagrant/VMBigData/hadoop/data/namesecondary&lt;/value&gt;&lt;/property&gt; </code></pre><p>添：</p><pre><code>&lt;!-- 命名空间的逻辑名称 --&gt; &lt;property&gt;    &lt;name&gt;dfs.nameservices&lt;/name&gt;    &lt;value&gt;mycluster&lt;/value&gt;&lt;/property&gt;&lt;!-- 命名空间中所有NameNode的唯一标示。该标识指示DataNode集群中有哪些NameNode --&gt;  &lt;property&gt;    &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;    &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;    &lt;value&gt;hadoop1:9091&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;    &lt;value&gt;hadoop2:9091&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;    &lt;value&gt;hadoop1:9092&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;    &lt;value&gt;hadoop2:9092&lt;/value&gt;&lt;/property&gt;&lt;!-- JournalNode URLs，ActiveNameNode 会将 Edit Log 写入这些 JournalNode 所配置的本地目录即 dfs.journalnode.edits.dir --&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;    &lt;value&gt;qjournal://hadoop2:8485;hadoop3:8485;hadoop4:8485/mycluster&lt;/value&gt;&lt;/property&gt;&lt;!-- JournalNode 用于存放 editlog 和其他状态信息的目录 --&gt;&lt;property&gt;    &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;    &lt;value&gt;/home/deploy/hadoop-2.7.1/journaldata&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;!-- 一种关于 NameNode 的隔离机制(fencing) --&gt;  &lt;property&gt;    &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;    &lt;value&gt;        sshfence        shell(/bin/true)    &lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;    &lt;value&gt;/home/deploy/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;    &lt;value&gt;30000&lt;/value&gt;&lt;/property&gt;</code></pre><h2 id="修改slaves文件"><a href="#修改slaves文件" class="headerlink" title="修改slaves文件"></a>修改slaves文件</h2><p>主要是把作为namenode节点的ip从该文件中删除，视各自机器环境而定。</p><h2 id="启动HA集群"><a href="#启动HA集群" class="headerlink" title="启动HA集群"></a>启动HA集群</h2><p>启动顺序，非常讲究。。。</p><p>重要的一点：不容许使用 hdfs namenode -format的命令，此命令会删除原集群的所有数据。</p><p>第一步，先关所有的集群进程。</p><p>第二步，打开zookeeper集群：sh /home/deploy/zookeeper-3.4.9/bin/zkServer.sh start</p><p>第三步，打开journalnode进程，该进程在两个nn间同步元数据。在hadoop2\3\4上都执行：<br>sh hadoop-daemon.sh start journalnode</p><p>第四步，在原namenode节点上执行：sh hadoop-daemon.sh start namenode。<br>此操作会将namenode状态变成active。</p><p>第五步，在备（standby）节点执行同步namenode数据的命令：hdfs namenode -bootstrapStandby。<br>切记不要使用scp的方式同步元数据，会导致文件权限问题。</p><p>第六步，启动备namendoe：sh hadoop-daemon.sh start namenode</p><p>第七步，初始化zkfc。在主备两台上任意一台执行：hdfs zkfc -formatZK</p><p>第七步，启动zk(DFSZKFailoverController)，该进程维护共享锁保证只有一个active的namenode。分别在两台作为NN的节点上执行：sh hadoop-daemon.sh start zkfc</p><p>第八步，启动hdfs集群（就是打开所有的datanode进程）：sh start-dfs.sh</p><p>第九步，打开没有任何变化的yarn：sh start-yarn.sh。上面说了暂时不讨论resourcemanger的升级。</p><p>至此，hadoop集群双NN的升级就完成了。</p><h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><p>第一步， 打开两个NN的监控页：</p><p><img src="static/ScreenShot2017-02-16at11.12.24.png" alt=""></p><p><img src="static/ScreenShot2017-02-16at11.12.33.png" alt=""></p><p>第二步：kill hadoop1的namenode进程，查看hadoop2中namenode的状态由standby变为active：<br><img src="static/ScreenShot2017-02-16at11.14.55.png" alt=""><br>重复几次，主备仍能自切。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html#Hardware_resources" target="_blank" rel="noopener">官方文档: HDFS High Availability</a></li><li><a href="https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="noopener">官方文档: ResourceManager High Availability</a></li><li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="noopener">Hadoop NameNode 高可用 (High Availability) 实现解析</a></li><li><a href="http://blog.csdn.net/dingchenxixi/article/details/51131493" target="_blank" rel="noopener">Hadoop的HA机制(Zookeeper集群+Hadoop集群)配置记录</a></li><li><a href="http://www.jianshu.com/p/8a8fb958f11f" target="_blank" rel="noopener">hadoop HA高可用集群模式搭建指南</a>    </li><li><a href="http://blog.csdn.net/knowledgeaaa/article/details/23759099" target="_blank" rel="noopener">Journal Storage Directory not formatted</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本篇介绍Hadoop的一些常用知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。&lt;/p&gt;
&lt;p&gt;HA模式，主要是将namenode及resourcemanager都变成主备两个。这里先不讨论resourcemanager，主要针对namenode。&lt;/p&gt;
&lt;p&gt;将namenode变成可主备自动切换的，主要是通过zookeeper集群对namenode的健康状态进行监控，然后选举一个健康的namenode做active(主)的，另一个成为standby(备)。因此，保证zookeeper集群的配置是正确且不易挂掉，是HA的基石。同时，注意HA升级过程中的相关进程的启动步骤即可完成。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://guzhenping.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://guzhenping.com/tags/Hadoop/"/>
    
      <category term="集群运维" scheme="http://guzhenping.com/tags/%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
</feed>
