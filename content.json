{"meta":{"title":"谷震平的博客","subtitle":"写点一路的风景，都很普通，主要还是留给自己。","description":"写点一路的风景，都很普通，主要还是留给自己。","author":"谷震平[guzhenping, hehzyx@gmail.com]","url":"http://guzhenping.com"},"pages":[],"posts":[{"title":"前端开发学习笔记（一）","slug":"前端开发学习笔记（一）","date":"2018-11-29T09:04:48.000Z","updated":"2018-11-29T09:10:40.619Z","comments":true,"path":"2018/11/29/前端开发学习笔记（一）/","link":"","permalink":"http://guzhenping.com/2018/11/29/前端开发学习笔记（一）/","excerpt":"","text":"前言还是在全沾的道路上越走越远，特地记录下一路上的问题。 初识vue.js待续…","categories":[],"tags":[{"name":"前端开发","slug":"前端开发","permalink":"http://guzhenping.com/tags/前端开发/"}]},{"title":"","slug":"数据库学习（Postgre）","date":"2018-11-05T16:00:00.000Z","updated":"2018-11-28T06:32:44.234Z","comments":true,"path":"2018/11/06/数据库学习（Postgre）/","link":"","permalink":"http://guzhenping.com/2018/11/06/数据库学习（Postgre）/","excerpt":"","text":"数据库学习（Postgre） 前言sql基础 修改字段类型： 12345678-- 将string类型变成intLTER TABLE the_table ALTER COLUMN col_name TYPE integer USING (col_name::integer); -- 字段值没有空格ALTER TABLE the_table ALTER COLUMN col_name TYPE integer USING (trim(col_name)::integer); -- 字段值有空格ALTER TABLE onetruereport.report_jinrong_submit ALTER COLUMN createdtime TYPE integer USING (trim(createdtime)::integer); 转多列 1234SELECT unnest(string_to_array(&apos; xxx@xx.com, ccc@cc.com&apos;, &apos;,&apos;)) as email string_to_array：将字符串处理成array数组，unnest将array转成多列。","categories":[],"tags":[]},{"title":"大数据开发学习（Redis）","slug":"大数据开发学习（Redis）","date":"2018-11-02T09:04:48.000Z","updated":"2018-11-30T02:49:33.473Z","comments":true,"path":"2018/11/02/大数据开发学习（Redis）/","link":"","permalink":"http://guzhenping.com/2018/11/02/大数据开发学习（Redis）/","excerpt":"一 前言经常使用redis, 特地进行总结。","text":"一 前言经常使用redis, 特地进行总结。 二 基础安装下载安装包，或者： 1234$ wget http://download.redis.io/releases/redis-3.2.0.tar.gz$ tar xzf redis-3.2.0.tar.gz$ cd redis-3.2.0$ make 就是make命令~中间过程，报什么错搞定什么即可。 Ubuntu下比较简单，就是apt-get install redis-server 默认安装位置： 利用whereis redis 去找redis.conf ，需要修改是否后台运行daemonize （为yes）等属性 完成后，自动启动，可用命令：ps -aux| grep redis redis是依赖，因为我们用python，不得不装一个wrapper——redis-py:sudo pip install redis 启动要让Redis-server在后台运行！ 1.简单的启动： 进到redis目录下的src文件夹下，输入：redis-server 2.后台启动： 先去找redis.conf文件，修改daemonize属性，从no变为yes进到redis目录下的src文件夹下，输入：redis-server &amp; 3.通过配置文件启动： 需要配置启动文件，在Redis工程目录下有个redis.conf文件，修改： 1234567891011#修改daemonize为yes，即默认以后台程序方式运行（还记得前面手动使用&amp;号强制后台运行吗）。daemonize yes#可修改默认监听端口，别改了，万一你忘了port 6379#修改生成默认日志文件位置logfile &quot;/home/futeng/logs/redis.log&quot;#配置持久化文件存放位置dir /home/futeng/data/redisData 配置完以后，启动：还是 src目录下，redis-server ./redis.conf if 改了端口，使用redis-cli命令连接时，需要带上端口，比如：redis-cli -p xxxx [再次强调：仍在src目录下] 4.使用redis启动脚本，设置开机自启在生产环境中使用这种方式。 5.启动无密码验证的 redis-server --protected-mode no 设置连接数 redis-server --protected-mode no --maxclients 100000 客户端还是进到redis目录下的src文件夹下，输入：redis-cli 在弹出的交互界面测试下： 1234redis&gt; set foo barOKredis&gt; get foo&quot;bar&quot;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://guzhenping.com/tags/Redis/"}]},{"title":"Redash开发指南","slug":"大数据开发学习（Redash）","date":"2018-11-02T09:04:48.000Z","updated":"2018-11-30T02:48:21.177Z","comments":true,"path":"2018/11/02/大数据开发学习（Redash）/","link":"","permalink":"http://guzhenping.com/2018/11/02/大数据开发学习（Redash）/","excerpt":"一 Redash介绍Redash是一款融合多数据源的可视化查询工具，用于Ad-hoc查询再好不过。除了官方支持的数据源，还可以通过复用代码开发支持Kylin、Clickhouse、TiDB、Palo、Druid等。","text":"一 Redash介绍Redash是一款融合多数据源的可视化查询工具，用于Ad-hoc查询再好不过。除了官方支持的数据源，还可以通过复用代码开发支持Kylin、Clickhouse、TiDB、Palo、Druid等。 二 测试环境官方提供的测试环境启动方式redash开发文档 ： https://redash.io/help-onpremise/ 自主搭建安装虚拟环境管理工具anaconda, 创建虚拟环境redash:conda create -n redash python=2.7,激活该环境:source activate redash。 创建虚拟环境（也可以不创建），激活后。利用pip安装redash所需要的包。 12pip install -r requirements.txt # 程序基础依赖pip install -r requirements_all_ds.txt # 所有数据库的依赖 在上述安装过程中大概率会出现安装错误(遇到过10多回)。请将出错的包单独安装。 安装npm，在主目录执行： 12npm install # 安装node模块npm run build # 编译前端的东西 启动服务器:1bin/run ./manage.py runserver --debugger --reload 启动celery的woker和调度器:1./bin/run celery worker --app=redash.worker --beat -Qscheduled_queries,queries,celery -c2 也可以分开启动woker和调度器，调度器启动: 1bin/run celery worker --app=redash.worker -c4 -Qscheduled_queries --maxtasksperchild=10 -Ofair worker启动:1bin/run celery worker --app=redash.worker --beat -c8 -Qqueries,celery --maxtasksperchild=10 -Ofair 根据机器机器实际情况，调节-c后的参数，用来指定启动多少个进程数量。 启动方式除了上面的启动方式，还推荐使用guncoin的启动方式。 启动服务器： 12345# 前台启动gunicorn -b 127.0.0.1:5000 --name redash -w 4 --max-requests 1000 redash.wsgi:app# 后台启动nohup /home/hadoop/anaconda3/envs/redash/bin/python /home/hadoop/anaconda3/envs/redash/bin/gunicorn -b 0.0.0.0:5000 --name redash -w 4 --max-requests 1000 redash.wsgi:app &gt;&gt; redash.log &amp; 没有gunicorn命令的，需要装Python包。 启动调度器进程： 12345# 前台启动bin/run celery worker --app=redash.worker -c4 -Qscheduled_queries --maxtasksperchild=10 -Ofair# 后台启动nohup /home/hadoop/anaconda3/envs/redash/bin/celery worker --app=redash.worker -c4 -Qscheduled_queries --maxtasksperchild=10 -Ofair &gt;&gt; schedular.log &amp; 启动worker进程： 12345# 前台启动bin/run celery worker --app=redash.worker --beat -c8 -Qqueries,celery --maxtasksperchild=10 -Ofair # 后台启动nohup /home/hadoop/anaconda3/envs/redash/bin/celery worker --app=redash.worker --beat -c8 -Qqueries,celery --maxtasksperchild=10 -Ofair &gt;&gt; worker.log &amp; 也可以用nginx做下代理，配置文件： 1234567891011121314151617181920212223242526272829worker_processes 4;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 3600; upstream rd_servers &#123; server 127.0.0.1:5000; &#125; server &#123; server_tokens off; listen 5123 default; access_log /var/log/nginx/rd.access.log; gzip on; gzip_types *; gzip_proxied any; location / &#123; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://rd_servers; &#125; &#125;&#125; 启动NGINX: 12cd /usr/local/nginx/sbinsudo ./nginx 三 Redash VS Superset关于Redash：https://redash.io/； 与Superset的区别与联系：https://www.zhihu.com/question/60369195/answer/258298127。 Bughive 读 schemashow columns in table , 无法处理中文comment,desc table , 无法处理没有字段的表， 报错：Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.ClassNotFoundException Class org.apache.hive.hcatalog.data.JsonSerDe not found 参考","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"Redash","slug":"Redash","permalink":"http://guzhenping.com/tags/Redash/"}]},{"title":"","slug":"Redash Redis队列运维","date":"2018-11-01T16:00:00.000Z","updated":"2018-11-28T06:32:44.233Z","comments":true,"path":"2018/11/02/Redash Redis队列运维/","link":"","permalink":"http://guzhenping.com/2018/11/02/Redash Redis队列运维/","excerpt":"","text":"Redash 队列运维1234567891011121314151617181920212223242526272829303132333435363738394041# coding:utf-8import datetime, json, redisr = redis.StrictRedis(host='xx.xxx.xx.xx', port=123, db=1)list = ['query_task_trackers:waiting', 'query_task_trackers:in_progress']pipe = r.pipeline()for item in list: t = item ids = r.zrevrange(t, 0, -1) print(\"The queue \" + item + \" has \" + str(len(ids))) if len(ids) == 0: continue for id in ids: pipe.get(id) p = pipe.execute() # 可以调试看看p的数据结构，现阶段值会返回list，index=0 try: json_text = json.loads(p[0]) except Exception as e: pass timestamp = json_text['created_at'] date = datetime.datetime.fromtimestamp(timestamp) date_str = date.strftime(\"%Y-%m-%d %H:%M:%S\") print(date_str) # 选择自己合适的时间 flag_day = datetime.datetime.today() - datetime.timedelta(days=1) # flag_day = datetime.datetime.today() - datetime.timedelta(hours=12) print(flag_day) if flag_day &gt; date: print(\"可以删除...\") print(\"delete task: \" + str(id)) r.delete(id) r.zrem(t, id) print(\"deleted success...\") else: print(\"不需要删除...\")","categories":[],"tags":[]},{"title":"","slug":"Hadoop学习指南（Yarn篇）","date":"2018-10-30T16:00:00.000Z","updated":"2018-11-28T06:32:44.222Z","comments":true,"path":"2018/10/31/Hadoop学习指南（Yarn篇）/","link":"","permalink":"http://guzhenping.com/2018/10/31/Hadoop学习指南（Yarn篇）/","excerpt":"","text":"Hadoop学习指南(YARN篇) 前言从业2年多，也该总结下关于Yarn的东西了。 架构图： 核心概念 ResourceManager: 全局资源管理和任务调度 NodeManager: 单个节点的资源管理和监控 ApplicationMaster: 单个作业的资源管理和任务监控 Container: 资源申请的单位和任务运行的容器 参考资料 极客学院 Yarn资料","categories":[],"tags":[]},{"title":"大数据开发学习（Flink）","slug":"大数据开发学习（Flink）","date":"2018-10-29T09:04:48.000Z","updated":"2018-11-30T02:46:37.477Z","comments":true,"path":"2018/10/29/大数据开发学习（Flink）/","link":"","permalink":"http://guzhenping.com/2018/10/29/大数据开发学习（Flink）/","excerpt":"前言Flink在实时流处理领域越来越热，以阿里为首的企业正在投入更多的资源。在实际工作中也遇到了流处理的场景，特此学习一下。","text":"前言Flink在实时流处理领域越来越热，以阿里为首的企业正在投入更多的资源。在实际工作中也遇到了流处理的场景，特此学习一下。 Demo制作先看一个demo: WordCount.java 123456789101112131415161718192021222324252627282930313233package com.gzp.batch;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.util.Collector;public class WordCount &#123; public static void main(String[] args) throws Exception &#123; final ParameterTool params = ParameterTool.fromArgs(args); final ExecutionEnvironment env = ExecutionEnvironment.createCollectionsEnvironment(); DataSet&lt;String&gt; text = WordCountData.getDefaultTextLineDataSet(env); DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; counts = // split up the lines in pairs (2-tuples) containing: (word,1) text.flatMap(new Tokenizer()) .groupBy(0) .sum(1); counts.print(); &#125; public static final class Tokenizer implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) &#123; // emit the pairs for (String token : value.toLowerCase().split(\"\\\\W+\")) &#123; if (token.length() &gt; 0) &#123; out.collect(new Tuple2&lt;&gt;(token, 1)); &#125; &#125; &#125; &#125;&#125; WordCountData.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.gzp.batch;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;public class WordCountData &#123; public static final String[] WORDS = new String[] &#123; &quot;To be, or not to be,--that is the question:--&quot;, &quot;Whether &apos;tis nobler in the mind to suffer&quot;, &quot;The slings and arrows of outrageous fortune&quot;, &quot;Or to take arms against a sea of troubles,&quot;, &quot;And by opposing end them?--To die,--to sleep,--&quot;, &quot;No more; and by a sleep to say we end&quot;, &quot;The heartache, and the thousand natural shocks&quot;, &quot;That flesh is heir to,--&apos;tis a consummation&quot;, &quot;Devoutly to be wish&apos;d. To die,--to sleep;--&quot;, &quot;To sleep! perchance to dream:--ay, there&apos;s the rub;&quot;, &quot;For in that sleep of death what dreams may come,&quot;, &quot;When we have shuffled off this mortal coil,&quot;, &quot;Must give us pause: there&apos;s the respect&quot;, &quot;That makes calamity of so long life;&quot;, &quot;For who would bear the whips and scorns of time,&quot;, &quot;The oppressor&apos;s wrong, the proud man&apos;s contumely,&quot;, &quot;The pangs of despis&apos;d love, the law&apos;s delay,&quot;, &quot;The insolence of office, and the spurns&quot;, &quot;That patient merit of the unworthy takes,&quot;, &quot;When he himself might his quietus make&quot;, &quot;With a bare bodkin? who would these fardels bear,&quot;, &quot;To grunt and sweat under a weary life,&quot;, &quot;But that the dread of something after death,--&quot;, &quot;The undiscover&apos;d country, from whose bourn&quot;, &quot;No traveller returns,--puzzles the will,&quot;, &quot;And makes us rather bear those ills we have&quot;, &quot;Than fly to others that we know not of?&quot;, &quot;Thus conscience does make cowards of us all;&quot;, &quot;And thus the native hue of resolution&quot;, &quot;Is sicklied o&apos;er with the pale cast of thought;&quot;, &quot;And enterprises of great pith and moment,&quot;, &quot;With this regard, their currents turn awry,&quot;, &quot;And lose the name of action.--Soft you now!&quot;, &quot;The fair Ophelia!--Nymph, in thy orisons&quot;, &quot;Be all my sins remember&apos;d.&quot; &#125;; public static DataSet&lt;String&gt; getDefaultTextLineDataSet(ExecutionEnvironment env) &#123; return env.fromElements(WORDS); &#125;&#125; 上述代码核心是将一段文本按单词切割，统计词频。flatMap()调用udf将文本切割并生成结构化数据，按单词分组后再sum。 不得不说，java写的flink task代码太长…. Core Concepts#### Flink Case命令行使用简介 参考资料、 核心概念翻译 Flink架构、原理与部署测试详解 简单的Scala Demo应用： flink demo 关于动态表: 在数据流中使用SQL查询：Apache Flink中的动态表的持续查询","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"实时流","slug":"实时流","permalink":"http://guzhenping.com/tags/实时流/"},{"name":"Flink","slug":"Flink","permalink":"http://guzhenping.com/tags/Flink/"}]},{"title":"Kylin应用篇","slug":"大数据开发学习（Kylin应用）","date":"2018-10-26T09:04:48.000Z","updated":"2018-11-30T02:56:21.818Z","comments":true,"path":"2018/10/26/大数据开发学习（Kylin应用）/","link":"","permalink":"http://guzhenping.com/2018/10/26/大数据开发学习（Kylin应用）/","excerpt":"简介Kylin是一款处理海量数据，提供SQL和多维度分析的OLAP工具。Kylin用于处理hadoop/spark场景下大量数据的预聚合，用户可自定义数据模型用于解决超过100亿+条记录的查询。","text":"简介Kylin是一款处理海量数据，提供SQL和多维度分析的OLAP工具。Kylin用于处理hadoop/spark场景下大量数据的预聚合，用户可自定义数据模型用于解决超过100亿+条记录的查询。 常见问题Not Support 建表或构建模型时，请勿使用中文列名。 不支持临时创建新列的统计 原表有两个字段a和b，通过concat进行拼接，然后做count()或者count(distinct *)。Kylin并不支持上述做法，因为无法命中相关的cube。 不能统计下述例子： count(distinct concat(cast(a as varchar),b)) 123select count(distinct concat(cast(a as varchar),b))from table_awhere dt = &apos;2018-05-01&apos; Cube命中所有的SQL需要命中相关Cube才可以使用。如果不是使用姿势问题，请联系管理员构建新的Cube。 在join时，需要用事实表join维度表，负责容易出现： 1No realization found for OLAPContext, CUBE_NOT_READY, CUBE_NOT_READY, CUBE_NOT_READY, MODEL_UNMATCHED_JOIN, MODEL_UNMATCHED_JOIN 在写子查询时，不能将事实表写在查询中，Cube可能无法命中。 调度脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import datetime, requestsauth_str = \"Basic YWRtaW46S1lMSU6=\"url_str = \"http://xxxxx.com:7070/kylin/api\"def auth(): \"\"\" 用户认证 :return: \"\"\" url = \"&#123;url_str&#125;/user/authentication\".format(url_str=url_str) payload = \"=\" headers = &#123; 'Content-Type': \"application/x-www-form-urlencoded\", 'Authorization': auth_str, 'Cache-Control': \"no-cache\" &#125; response = requests.request(\"POST\", url, data=payload, headers=headers) print(response.text)def get_cube(): \"\"\" 获取cube信息 :return: \"\"\" url = \"&#123;url_str&#125;/cubes\".format(url_str=url_str) querystring = &#123;\"cubeName\": \"test_join\"&#125; headers = &#123; 'Cache-Control': \"no-cache\", 'Authorization': auth_str &#125; response = requests.request(\"GET\", url, headers=headers, params=querystring) print(response.json())def build_cube(cube_name, start_date, end_date, build_type): \"\"\" 构建指定cube startTime 和 endTime 应该是utc时间。 buildType 可以是 BUILD 、 MERGE 或 REFRESH。 - BUILD 用于构建一个新的segment， - REFRESH 用于刷新一个已有的segment， - MERGE 用于合并多个已有的segment生成一个较大的segment :return: \"\"\" url = \"&#123;url_str&#125;/cubes/&#123;cube_name&#125;/rebuild\".format(cube_name=cube_name, url_str=url_str) start_stamp = int(datetime.datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').timestamp() * 1000) end_stamp = int(datetime.datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S').timestamp() * 1000) payload = \"&#123;\\\"startTime\\\": %d, \\\"endTime\\\": %d, \\\"buildType\\\": \\\"%s\\\"&#125;\" % (start_stamp, end_stamp, build_type) headers = &#123; 'Content-Type': \"application/json\", 'Authorization': auth_str, 'Cache-Control': \"no-cache\" &#125; response = requests.request(\"PUT\", url, data=payload, headers=headers) print(response.json())if __name__ == '__main__': cube_name = \"test_api\" start_date = '2018-05-01 04:00:00' end_date = '2018-05-01 08:00:00' build_type = 'BUILD' build_cube(cube_name, start_date, end_date, build_type)","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"大数据开发","slug":"大数据开发","permalink":"http://guzhenping.com/tags/大数据开发/"},{"name":"OLAP","slug":"OLAP","permalink":"http://guzhenping.com/tags/OLAP/"},{"name":"Kylin","slug":"Kylin","permalink":"http://guzhenping.com/tags/Kylin/"}]},{"title":"","slug":"README","date":"2018-08-13T16:00:00.000Z","updated":"2018-11-28T06:32:44.200Z","comments":true,"path":"2018/08/14/README/","link":"","permalink":"http://guzhenping.com/2018/08/14/README/","excerpt":"","text":"导读 这里是谷震平个人的博客文章，描述了关于大数据技术的一些实践结果和理论知识。包括，集群搭建、大数据工具学习、运维、学术资料整理。请记住这个域名：http://guzhenping.com。 笔者作为一名数据仓库工程师，经历过两个数据仓库系统，一大一小。大的让笔者学到了很多，小的是笔者自己从0到1攒的。 图1 基于hadoop的大数仓 图2 基于PG 10的小数仓 正在开启第三段职业生涯。 文章列表 集群搭建与运维 2016-11 集群搭建指南（上卷） 2016-11 集群搭建指南（中卷） 2016-11 集群搭建指南（下卷） 2016-12 Hadoop学习指南（集群运维篇）.md) 2017-01 Hadoop学习指南（HA配置） 2017-01 HA升级过程 2017-02 HA成功升级的总结 2017-10 大数据开发学习（Fabric） 数据仓库 2016-07 数据仓库学习（概念篇） 2017-05 数据仓库学习（ETL） 2018-01 数据仓库学习（维度建模） Hadoop生态圈 2016-11 Hadoop学习指南（HDFS篇） 2016-11 大数据开发学习（Hive） 2016-11 大数据开发学习（ZooKeeper） 2017-12 Hadoop学习指南（HBase篇） OLAP/数据查询与可视化 2017-12 大数据开发学习（Redash） 2018-01 大数据开发学习（Kylin） 2018-01 Redash使用指南 2018-01 Redash权限分配 2018-04 大数据开发学习（Clickhouse） 区块链 2018-07 区块链简史 2018-07 区块链分类 工作关系，学习到一点区块链的知识。特地开了一个公众号进行写作，欢迎扫码关注： window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-89345053-2');","categories":[],"tags":[]},{"title":"Clickhouse 高级SQL","slug":"Clickhouse 高级SQL","date":"2018-07-27T09:04:48.000Z","updated":"2018-11-30T02:43:37.103Z","comments":true,"path":"2018/07/27/Clickhouse 高级SQL/","link":"","permalink":"http://guzhenping.com/2018/07/27/Clickhouse 高级SQL/","excerpt":"","text":"前言测试数据集： ca cb cc A W 1 A W 2 B X 1 B Z 2 B Z 4 按最大/最小值/TOP1去重按ca和cb取cc最小值取值： 1234567select ca, cb, min(cc)from tablegroup by ca, cb column_A column_B column_C A W 1 B X 1 B Z 2 取最大，则使用max()函数。取一组值的TOP1也是同理。 按列合并多行（多-&gt;少）123456select ca, cb, groupUniqArray(cc)from tablegroup by ca, cb column_A column_B column_C A W [2,1] B X [1] B Z [4,2] 这是一种分组的概念，将相同的数据放在一起，需要被统计的数据放在array数据类型中。统计array，可以使用length()，获取数组长度，相当于分组count 此外，groupArray也可以满足需求。 分组排序后取TopN12345678910SELECT ca, groupArray(1)(cc) FROM ( SELECT * FROM table ORDER BY ca, cb, cc )GROUP BY ca column_A column_B column_C A W [1] B X [1] B Z [2] 以上是按cc列的值升序去top1。通过order by x改变顺序，再用groupArry(N)()函数处理获取top值。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"OLAP","slug":"OLAP","permalink":"http://guzhenping.com/tags/OLAP/"},{"name":"Clickhouse","slug":"Clickhouse","permalink":"http://guzhenping.com/tags/Clickhouse/"}]},{"title":"Redash Model源码分析","slug":"Redash Model整理分析","date":"2018-07-26T09:04:48.000Z","updated":"2018-11-30T02:43:09.000Z","comments":true,"path":"2018/07/26/Redash Model整理分析/","link":"","permalink":"http://guzhenping.com/2018/07/26/Redash Model整理分析/","excerpt":"前言掌握Redash执行原理，对于深度的二次开发至关重要。","text":"前言掌握Redash执行原理，对于深度的二次开发至关重要。 query功能SQL查询是Redash的核心功能之一。通常情况下，用户在前端会生成如下参数： query.data_source：下列列表中的数据源，必须选 parameter_values：用户自定义的参数值，可无 query.query_text：用户自定义的SQL文本，必须写 query.id：前端自动生成的查询id，系统生成 在上述条件具备后，将会调用Redash API： /api/queries/进行任务提交。 所有任务将会被redash.handlers.query_result.run_query()方法接受处理。此处将进行参数校验，语法解析，任务传输等方面的处理。 多数的查询场景，先将SQL文本hash后，去缓存队列和后台数据库（Redash的Postgre）进行寻找，找不到再发给celery让worker执行该SQL。 负责具体执行：redash.models.QueryResult()类实例 和redash.tasks.queries.enqueue_query()方法。前者：负责查现有的结果；后者：立即执行相关SQL语句。 在enqueue_query()方法里，先连Redis,将任务加入队列，持续监听查询结果。 核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546while try_count &lt; 5: try_count += 1 pipe = redis_connection.pipeline() try: pipe.watch(_job_lock_id(query_hash, data_source.id)) job_id = pipe.get(_job_lock_id(query_hash, data_source.id)) if job_id: logging.info(&quot;[%s] Found existing job: %s&quot;, query_hash, job_id) job = QueryTask(job_id=job_id) if job.ready(): logging.info(&quot;[%s] job found is ready (%s), removing lock&quot;, query_hash, job.celery_status) redis_connection.delete(_job_lock_id(query_hash, data_source.id)) job = None if not job: pipe.multi() time_limit = None if scheduled_query: queue_name = data_source.scheduled_queue_name scheduled_query_id = scheduled_query.id else: queue_name = data_source.queue_name scheduled_query_id = None time_limit = settings.ADHOC_QUERY_TIME_LIMIT result = execute_query.apply_async(args=(query, data_source.id, metadata, user_id, scheduled_query_id), queue=queue_name, time_limit=time_limit) job = QueryTask(async_result=result) tracker = QueryTaskTracker.create( result.id, &apos;created&apos;, query_hash, data_source.id, scheduled_query is not None, metadata) tracker.save(connection=pipe) logging.info(&quot;[%s] Created new job: %s&quot;, query_hash, job.id) pipe.set(_job_lock_id(query_hash, data_source.id), job.id, settings.JOB_EXPIRY_TIME) pipe.execute() break except redis.WatchError: continue 以上是Query功能的分析。主要组件： celery分布式框架 redis缓存和任务队列 postgre后台DB。 相关功能的设计比较中规中矩，报错异常的检测较少，后续值得优化。 Model层model层是Redash的设计核心，每个类对应后台数据库一张表和一个功能。 access_permission, 权限表 alembic_version,版本号 alert_subscriptions，报警描述 alerts，报警列表 api_keys, api key管理 changes，升级改动 dashboards，报表存储 data_source_groups,每个组对应的数据源 data_sources，所有数据源 events，后台日志 groups，所有分组列表 notification_destinations，报警的模板和目的地 organizations， 组织 queries，所有queris query_results，query的结果，另一种缓存 query_snippets，SQL的评论 users, 用户已列表 visualizations，可视化图表存储 widgets，可视化控件","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"},{"name":"可视化","slug":"大数据/可视化","permalink":"http://guzhenping.com/categories/大数据/可视化/"}],"tags":[{"name":"大数据开发","slug":"大数据开发","permalink":"http://guzhenping.com/tags/大数据开发/"},{"name":"Redash","slug":"Redash","permalink":"http://guzhenping.com/tags/Redash/"}]},{"title":"大数据开发学习（Kudu）","slug":"大数据开发学习（Kudu）","date":"2018-05-25T09:04:48.000Z","updated":"2018-11-30T02:57:14.186Z","comments":true,"path":"2018/05/25/大数据开发学习（Kudu）/","link":"","permalink":"http://guzhenping.com/2018/05/25/大数据开发学习（Kudu）/","excerpt":"","text":"前言kudu，作为OLAP工具十分强劲。本文记录了笔者对其学习和使用的过程。 注明：这篇文章是研究Kudu在OLAP的场景，不准备讨论其他Case。 安装与启动最新版安装：Installing Apache Kudu 推荐使用VM进行快速体验，Apache Kudu Quickstart。 当需要使用kudu client功能时，需要安装：kudu-client、kudu-client-devel这两个C++的库。 常规使用教学Demo参见：Apache Kudu Quickstart 以上内容是对静态数据的使用，如果是一条实时的数据流，则采用kudu API的方式。需要去做定制开发。 架构体系kudu架构： 关于运行原理的文章，推荐：Kudu设计原理初探 OLAP服务作为OLAP服务，ETL环节的处理至关重要。以下列举了小米公司的场景： 对比： 可以看出，除了日志数据外，线上业务数据都可以实时同步到kudu里。基于kudu对外提供OLAP服务，数据的实时性非常可观。 在小米，采用impala作为查询（计算）引擎，但是网上也有presto on kudu的组件可供选型（传送门）。 使用场景微店： Kudu+Impala介绍 Kudu的Schema表结构设计 参考资料运行原理：kudu内部机制 关于Kudu的介绍： Introducing Apache Kudu 基于Kudu搭建OLAP工具：小米：使用Kudu搭建OLAP服务 基于Kudu的实际应用：使用Spark Streaming + Kudu + Impala构建一个预测引擎","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"大数据开发","slug":"大数据开发","permalink":"http://guzhenping.com/tags/大数据开发/"},{"name":"OLAP","slug":"OLAP","permalink":"http://guzhenping.com/tags/OLAP/"},{"name":"Kudu","slug":"Kudu","permalink":"http://guzhenping.com/tags/Kudu/"}]},{"title":"数据仓库学习（概念篇）","slug":"数据仓库学习（概念篇）","date":"2018-05-24T09:04:48.000Z","updated":"2018-11-30T02:53:03.735Z","comments":true,"path":"2018/05/24/数据仓库学习（概念篇）/","link":"","permalink":"http://guzhenping.com/2018/05/24/数据仓库学习（概念篇）/","excerpt":"前言写点数据仓库（DW）的一些常用知识,架构等方面。加深基本功，保持进阶的心。","text":"前言写点数据仓库（DW）的一些常用知识,架构等方面。加深基本功，保持进阶的心。 典型架构 基本概念 数据仓库 数据仓库是一个面向主题的、集成的、非易失的（nonvolatile）、随时间变化的（time-variant）用来支持管理人员决策的数据集合。 —-William H.Inmon 以上也是数据仓库区别于业务系统的4个特征。 OLAP 联机分析处理（On-line Analytical Processing），该概念由数据库创始人E. F. Codd于1993年提出。OLAP理事会（OLAP Council）进一步完善：OLAP是一种软件技术，它使分析人员、经理和执行官能够迅速、一致、交互的从各方面观察信息，以达到深入理解数据的目的。 OLTP 联机事务处理（On-line Transaction Processing） ODS 操作数据存储（Operational Data Store, 简称ODS）。在数据仓库系统中，ODS存储了原始数据经过集成统一后的数据。 DW 数据仓库（Data Warehouse，简称DW）。在数据仓库系统中，DW数据库存储了整个企业的所有历史数据，是狭义上的数据仓库。DW数据库需要满足企业数据分析的各种需求，以及各个部门建设数据集市的需求，通常存储企业的基础数据和通用数据。 数据集市(Data Market) 数据集市是指针对特定部门和主题的小型数据仓库，数据从DW中获取。 ETL ETL（Extract，Transform，Load）表示抽取、转换和装载，数据从多个同构或异构的数据源抽取出来，经过自定义的转换操作，最终装载进入目标表的过程叫做一次ETL。ETL是数据进入ODS、DW、Data Market的主要方式。 建模方法 维度建模法（Dimensional Modeling） Ralph Kimball主张， 实体-关系建模（Entity-Relationship Medeling） 实体-关系建模也叫做第三范式建模（Third Normal Form, 3NF）,是William H.Inmon主张的一种数据仓库建模方法。 数据与建模OLAP中roll-up和drill-down和slicing？ 真正好的建模，应该契合这些功能转换。见过Tableau做的很好，但不算是OLAP系统自带的功能。 数仓进化史比较推荐这篇文章：从数据仓库到大数据，数据平台这25年是怎样进化的？ 在架构方面，偏理论一点的文章：大数据分析的下一代架构–IOTA 参考资料 hadoop HDFS常用文件操作命令 HDFS 文件操作命令","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"http://guzhenping.com/tags/数据仓库/"}]},{"title":"大数据开发学习（Clickhouse）","slug":"大数据开发学习（Clickhouse）","date":"2018-05-24T09:04:48.000Z","updated":"2018-11-30T02:10:54.811Z","comments":true,"path":"2018/05/24/大数据开发学习（Clickhouse）/","link":"","permalink":"http://guzhenping.com/2018/05/24/大数据开发学习（Clickhouse）/","excerpt":"前言1年前，用过Greenplum，这是第一次接触MPP结构的OLAP系统。今天市面上常见的MPP架构工具，还有Clickhouse和Palo等。(当然，SQL on Hadoop体系的Presto和Impala也算是MPP结构，只是数据存储方面没有自己的东西，都是依赖hdfs,mysql等。) 工作需要，对Clickhouse进行学习。","text":"前言1年前，用过Greenplum，这是第一次接触MPP结构的OLAP系统。今天市面上常见的MPP架构工具，还有Clickhouse和Palo等。(当然，SQL on Hadoop体系的Presto和Impala也算是MPP结构，只是数据存储方面没有自己的东西，都是依赖hdfs,mysql等。) 工作需要，对Clickhouse进行学习。 安装与启动Ubuntu上比较好安装，但是一般公司用的服务器都是Centos。这里只讨论在Centos 7的安装方式，centos 6请在Altinity公司提供的下载界面中自行寻找。 安装命令12345678910111213141516如果是装54362版本的包，其余所有依赖都需要是一致的。准备源依赖，由Altinity公司提供：curl -s https://packagecloud.io/install/repositories/Altinity/clickhouse/script.rpm.sh | sudo bashserver-common:sudo yum install clickhouse-server-common-1.1.54362-1.el7.x86_64sudo yum install clickhouse-server-common-1.1.54380-1.el7.x86_64server:sudo yum install clickhouse-server-1.1.54362-1.el7.x86_64sudo yum install clickhouse-server-1.1.54380-1.el7.x86_64client：sudo yum install clickhouse-client-1.1.54362-1.el7.x86_64sudo yum install clickhouse-client-1.1.54380-1.el7.x86_64 关于Altinity公司的其他版本，可访问这里下载。 以上安装如有疑问，可以使用下方安装方式： https://github.com/red-soft-ru/clickhouse-rpm 启动12345server端：sudo /etc/rc.d/init.d/clickhouse-server startclient端：clickhouse-client 使用经验常用SQL123456789-- 查看集群情况select * from system.clusters;-- 查看分区情况select partition, name, rowsfrom system.parts; MergeTree选择engine时，尽量用merge tree.MergeTree引擎支持以主键和日期作为索引，提供实时更新数据的可能性。这是Clickhouse中最先进的表引擎。 数据表将数据分割为小的索引块进行处理。每个索引块之间依照主键排序。每个索引块记录了指定的开始日期和结束日期。插入数据时，MergeTree会对数据进行排序，以保证存储在索引块中的数据有序。当写入新数据时，会放在新的文件夹下，索引块之间的合并过程会在系统后台定期自动执行。MergeTree引擎会选择几个相邻的索引块进行合并，然后对二者合并排序。 Distributed相当于数据库的视图，并不存储数据，而是用来做分布式的写入和查询，与其他引擎结合使用。 一种集群拓扑结构： 实际问题 关键字大小写敏感，对强转支持不太好。 特性上不支持事务，不支持update/delete。 关于分布式 在分布式表上执行count()时，发现结果不一致。 暂无解决办法。问题:https://github.com/yandex/ClickHouse/issues/1443 查询时，内存超出限制： 123456789101112131415161718Progress: 4.84 million rows, 42.70 MB (45.34 million rows/s., 399.59 MB/s.) 87%Received exception from server:Code: 241. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Memory limit (for query) exceeded: would use 12.15 GiB (attempt to allocate chunk of 4294967296 bytes), maximum: 9.31 GiB.0 rows in set. Elapsed: 48.023 sec. Processed 4.84 million rows, 42.70 MB (100.89 thousand rows/s., 889.21 KB/s.)Progress: 4.89 million rows, 43.81 MB (23.23 thousand rows/s., 208.09 KB/s.) 88%Received exception from server:Code: 241. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Memory limit (for query) exceeded: would use 80.15 GiB (attempt to allocate chunk of 17179869184 bytes), maximum: 74.51 GiB.3668208133 rows in set. Elapsed: 235.536 sec. Processed 4.89 million rows, 43.81 MB (20.77 thousand rows/s., 186.00 KB/s.)Received exception from server (version 1.1.54362):Code: 241. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Memory limit (for query) exceeded: would use 96.14 GiB (attempt to allocate chunk of 17179869184 bytes), maximum: 93.13 GiB.8816716667 rows in set. Elapsed: 428.543 sec. Processed 4.89 million rows, 43.81 MB (11.41 thousand rows/s., 102.23 KB/s.) 当处理的数据集超过设定的阈值以后，会触发限制，并返回已经处理好的结果。 解决办法：修改max_memory_usage的值（user.xml） 不同的节点可以处理设置不同。在高并发分流的时候，尽量把query分摊到各个机器上，否则会将某一节点资源耗尽。 这个问题导致Clickhouse的高并发特性很差。在前端暴露端口时，不能单独暴露一个host,需要做成有命名空间的方式，或者有个query平衡器的机制。 解决办法：就上述问题，发现一个公司采用如下架构： 即：域名轮询。 SQL 实战第一步建表：1create table test_analysis (created_at DateTime, dt Date, user String, page_id String ) ENGINE=MergeTree(dt, (user, dt), 8192); 插入测试数据： 1234567insert into table test_analysis Format Values(&apos;2018-4-24 18:45&apos;,&apos;2018-4-24&apos;,&apos;A&apos;,&apos;首页&apos;),(&apos;2018-4-24 18:46&apos;,&apos;2018-4-24&apos;,&apos;A&apos;,&apos;购物车&apos;),(&apos;2018-4-24 18:45&apos;,&apos;2018-4-24,&apos;B&apos;,&apos;首页&apos;),(&apos;2018-4-24 18:48&apos;,&apos;2018-4-24&apos;,&apos;B&apos;,&apos;商品详情&apos;),(&apos;2018-4-24 18:49&apos;,&apos;2018-4-24&apos;,&apos;B&apos;,&apos;购物车&apos;),(&apos;2018-4-24 18:46&apos;,&apos;2018-4-24&apos;,&apos;C&apos;,&apos;商品详情&apos;); 第二步，建立模型： 1234567891011121314151617SELECT `user`, created_at, page_id, gap1/60 AS &quot;与第一个动作的间隔时间&quot;, if(gap1 == 0, 0, runningDifference(gap1)/60) AS &quot;与上一个动作的间隔时间&quot;FROM (SELECT `user`, created_at, fist_created_at, page_id, created_at-fist_created_at AS gap1 FROM test_analysis ANY LEFT JOIN (SELECT `user` , min(created_at) AS fist_created_at FROM test_analysis GROUP BY `user`) using(`user`)) AS t 获得的结果： clickhouse 实战 参考资料官方文档What is ClickHouse 官方写的体系结构文章：Overview of ClickHouse architecture 翻译:ClickHouse 内部架构介绍 配置文件ClickHouse相关配置剖析 ClickHouse的分布式引擎 用户权限ClickHouse 用户名密码设置 ClickHouse之访问权限控制 引擎介绍ClickHouse MergeTree引擎介绍 ClickHouse Distribute 引擎深度解读 实时大数据分析引擎ClickHouse介绍 数据同步kafka-&gt;Clickhouse：Hangout with ClickHouse mysql-&gt;Clickhouse：使用ClickHouse一键接管MySQL数据分析 杂文奶clickhouse的文章：ClickHouse Beijing Meetup-数据分析领域的黑马-ClickHouse-新浪-高鹏","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"OLAP","slug":"OLAP","permalink":"http://guzhenping.com/tags/OLAP/"},{"name":"Clickhouse","slug":"Clickhouse","permalink":"http://guzhenping.com/tags/Clickhouse/"}]},{"title":"Redash权限管理","slug":"Redash 权限管理","date":"2018-05-21T09:04:48.000Z","updated":"2018-11-30T02:42:24.499Z","comments":true,"path":"2018/05/21/Redash 权限管理/","link":"","permalink":"http://guzhenping.com/2018/05/21/Redash 权限管理/","excerpt":"前言Reash是一个数据查询平台，必定会涉及权限管理。主要由3个概念：组（group）,用户（user）， 数据源（data source）。 group在最上层，一个group对应多个user 和 data source。反之也可行，但不利于权限的管理。 权限功能主要基于组（group）和所属数据源(data source)来控制。一个用户（user）必须属于一个或多个组，当新用户进入时，该用户默认在“default组”。在新增一个数据源时，该数据源默认归属于”default组”。","text":"前言Reash是一个数据查询平台，必定会涉及权限管理。主要由3个概念：组（group）,用户（user）， 数据源（data source）。 group在最上层，一个group对应多个user 和 data source。反之也可行，但不利于权限的管理。 权限功能主要基于组（group）和所属数据源(data source)来控制。一个用户（user）必须属于一个或多个组，当新用户进入时，该用户默认在“default组”。在新增一个数据源时，该数据源默认归属于”default组”。 数据源数据源有两个权限设置： Full access，即：该组的用户可操作被保存的查询（可以改SQL源码），创建一个新的查询 View Only access，即：该组的用户只能阅读被保存的查询（看不到SQL源码）及其结果 对于不需要写SQL取结果的用户群来说，应该与需要写SQL取结果的用户群体区分在不同组（group）中。比如：基础架构查看报表组，基础架构制作报表组。 如果想对一个组的用户做table级别的查询限制，官方提供的方案： The idea is to leverage your database’s security model and hence create a user with access to the tables/columns you want to give access to. Create a data source that’s using this user and then associate it with a group of users who need this level of access. 翻译过来：在建立数据源时，配置一个带有权限控制的数据库用户即可。比如，连接mysql时，用一个只能查询测试db/table的用户名进行连接。把这个数据源赋给某个组，然后该组的所有用户，只能看到这个数据源里的测试db/table。 组组可设置的权限有： admin/super_admin,管理员/超级管理员，可用所用功能 create_dashboard,创建dashboard create_query,创建SQL查询 edit_dashboard,编辑自己/别人的dashboard edit_query,编辑自己/别人的SQL查询 view_query,查看已经存在的SQL view_source,查看SQL源码 execute_query,执行SQL list_users,看到所有用户 schedule_query,设置定时刷新 list_dashboards,看到所用的dashboard list_alerts,看到所有的提醒任务 list_data_sources,看到所有的数据源 将以上权限赋给不同的组，每个组的用户就可以实现不同的功能。Redash不强调对用户做太多的权限控制，因为一个用户必须要归属于一个组。所以，对组做现在即可。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"},{"name":"可视化","slug":"大数据/可视化","permalink":"http://guzhenping.com/categories/大数据/可视化/"}],"tags":[{"name":"大数据开发","slug":"大数据开发","permalink":"http://guzhenping.com/tags/大数据开发/"},{"name":"Redash","slug":"Redash","permalink":"http://guzhenping.com/tags/Redash/"}]},{"title":"","slug":"分布式算法设计","date":"2018-05-15T16:00:00.000Z","updated":"2018-11-28T06:32:44.102Z","comments":true,"path":"2018/05/16/分布式算法设计/","link":"","permalink":"http://guzhenping.com/2018/05/16/分布式算法设计/","excerpt":"","text":"分布式算法设计 前言在分布式存储领域有两种常见的算法：Paxos和Raft。因为CAP原理在，所以在分布式算法的选择上有很多的考量。 Paxos算法 Paxos 一致性哈希 Raft算法参考资料 Paxos在大型系统中常见的应用场景 分布式系统Paxos算法 从一到N，掘金区块——区块链行业研究报告 比特币：一种点对点的电子现金系统","categories":[],"tags":[]},{"title":"","slug":"OLAP工具的关键技术研究","date":"2018-05-15T16:00:00.000Z","updated":"2018-11-28T06:32:44.102Z","comments":true,"path":"2018/05/16/OLAP工具的关键技术研究/","link":"","permalink":"http://guzhenping.com/2018/05/16/OLAP工具的关键技术研究/","excerpt":"","text":"OLAP工具的关键技术研究 前言OLAP工具种类繁多，所用技术都有独到之处。这里一一研究。 计算这里总结和计算领域相关的技术点。 向量化PgSQL · 引擎介绍 · 向量化执行引擎简介 存储这里总结和存储相关的技术点。 BitmapBit即比特，是目前计算机系统里边数据的最小单位，8个bit即为一个Byte。一个bit的值，或者是0，或者是1；也就是说一个bit能存储的最多信息是2。 Bitmap可以理解为通过一个bit数组来存储特定数据的一种数据结构；由于bit是数据的最小单位，所以这种数据结构往往是非常节省存储空间。比如一个公司有8个员工，现在需要记录公司的考勤记录，传统的方案是记录下每天正常考勤的员工的ID列表，比如2012-01-01:[1,2,3,4,5,6,7,8]。假如员工ID采用byte数据类型，则保存每天的考勤记录需要N个byte，其中N是当天考勤的总人数。另一种方案则是构造一个8bit（01110011）的数组，将这8个员工跟员工号分别映射到这8个位置，如果当天正常考勤了，则将对应的这个位置置为1，否则置为0；这样可以每天采用恒定的1个byte即可保存当天的考勤记录。 综上所述，Bitmap节省大量的存储空间，因此可以被一次性加载到内存中。 Bitmap的秘密 参考资料","categories":[],"tags":[]},{"title":"","slug":"分布式核心设计","date":"2018-05-15T16:00:00.000Z","updated":"2018-11-28T06:32:44.102Z","comments":true,"path":"2018/05/16/分布式核心设计/","link":"","permalink":"http://guzhenping.com/2018/05/16/分布式核心设计/","excerpt":"","text":"分布式核心设计 前言本篇介绍分布式的一些核心设计知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。 常用算法 Paxos 一致性哈希 从区块链看分布式核心设计此处以Bitcoin为例进行相关说明。 参考资料 Paxos在大型系统中常见的应用场景 分布式系统Paxos算法 从一到N，掘金区块——区块链行业研究报告 比特币：一种点对点的电子现金系统","categories":[],"tags":[]},{"title":"","slug":"大数据开发学习（Mesa）","date":"2018-05-15T16:00:00.000Z","updated":"2018-11-28T06:32:44.103Z","comments":true,"path":"2018/05/16/大数据开发学习（Mesa）/","link":"","permalink":"http://guzhenping.com/2018/05/16/大数据开发学习（Mesa）/","excerpt":"","text":"前言Google Mesa是一种高性能、近实时、大规模数据分析仓储体系。 ABSTRACT Mesa is a highly scalable analytic data warehousing systemthat stores critical measurement data related to Google’sInternet advertising business. Mesa is designed to satisfya complex and challenging set of user and systems requirements,including near real-time data ingestion and queryability,as well as high availability, reliability, fault tolerance,and scalability for large data and query volumes. Specifi-cally, Mesa handles petabytes of data, processes millions ofrow updates per second, and serves billions of queries thatfetch trillions of rows per day. Mesa is geo-replicated acrossmultiple datacenters and provides consistent and repeatablequery answers at low latency, even when an entire datacenterfails. This paper presents the Mesa system and reportsthe performance and scale that it achieves. from ‘Mesa: Geo-Replicated, Near Real-Time, Scalable DataWarehousing’ Mesa设计出了主要是为了广告业务，而且也有解决跨数据中心的问题，在其他场景下可能不适用。 网评，Mesa最大的贡献在其数据模型上，这一点可以找找ORCFile、Parquet进行对比学习。 架构体系在单一数据中心（data center）部署下，主要由两个子系统构成了Mesa。 Update/Maintenance Subsystem Query Subsystem 对于跨数据中心的部署，架构体系和单DC差不都。 Update/Maintenance Subsystem主要职责包括: 加载update，并且按照存储模型保存到Mesa的物理存储上。 执行多级的compaction。 在线做schema change。 执行一些表的checksum检查。 Query Subsystem工作职责： Mesa’s query subsystem consists of query servers, illustratedin Figure 5. These servers receive user queries, lookup table metadata, determine the set of files storing therequired data, perform on-the-fly aggregation of this data,and convert the data from the Mesa internal format to theclient protocol format before sending the data back to theclient 跨DC部署架构图： Mesa的数据模型此处推荐阅读《浅谈从Google Mesa到百度PALO》关于mesa的存储部分。 暂时没有理解。需要与ORCFile和Parquet进行对比学习。 安装与启动暂时未掌握。 参考资料 Mesa: Geo-Replicated, Near Real-Time, Scalable DataWarehousing","categories":[],"tags":[]},{"title":"","slug":"大数据开发学习（Palo）","date":"2018-05-15T16:00:00.000Z","updated":"2018-11-28T06:32:44.110Z","comments":true,"path":"2018/05/16/大数据开发学习（Palo）/","link":"","permalink":"http://guzhenping.com/2018/05/16/大数据开发学习（Palo）/","excerpt":"","text":"Palo介绍Palo是2017年由百度开源的一款基于MPP架构的、对SQL友好的的数据仓储，主要由于报表和分析。 Palo is an MPP-based interactive SQL data warehousing for reporting and analysis. Palo mainly integrates the technology of Google Mesa and Apache Impala. PALO = Mesa的存储引擎 + Impala查询引擎 接触Palo主要是为了调研OLAP的工具。截止目前(2018-04-12 11:24:00)，Palo在github上的star数为658。 架构体系 ETL导入数据部分： 123456789-- 一个demo, 在mysql执行。 broker_name需要自己提前定义 LOAD LABEL test_palo.gzp_2 ( DATA INFILE(&quot;hdfs://nanenode1:x020/user/hadoop/xxx/test_happysql&quot;) INTO TABLE `test_happysql` COLUMNS TERMINATED BY &quot;,&quot; )WITH BROKER broker_test (&quot;username&quot;=&quot;haxxx&quot;, &quot;password&quot;=&quot;&quot;) 配置文件网络相关参数解释： 常用SQL12345-- 查看 BE 状态show proc &apos;/backends&apos;-- 查看 FE 状态show proc &apos;/frontends&apos; SQL 高级用法SQL高级用法 安装与启动暂时未掌握。 参考资料palo和mesa的关联较大，推荐阅读: 《浅谈从Google Mesa到百度PALO》","categories":[],"tags":[]},{"title":"","slug":"大数据开发学习（TiDB）","date":"2018-05-02T16:00:00.000Z","updated":"2018-11-28T06:32:44.087Z","comments":true,"path":"2018/05/03/大数据开发学习（TiDB）/","link":"","permalink":"http://guzhenping.com/2018/05/03/大数据开发学习（TiDB）/","excerpt":"","text":"前言TiDB集存储与计算之大成，势头很猛。因为工作需要，进行学习。 TiDB 开源分布式 NewSQL 关系型数据库 TiDB 是新一代开源分布式 NewSQL 数据库，模型受 Google Spanner / F1 论文的启发，实现了自动的水平伸缩，强一致性的分布式事务，基于 Raft 算法的多副本复制等重要 NewSQL 特性。TiDB 结合了 RDBMS 和 NoSQL 的优点，部署简单，在线弹性扩容和异步表结构变更不影响业务， 真正的异地多活及自动故障恢复保障数据安全，同时兼容 MySQL 协议，使迁移使用成本降到极低。 系统架构整体结构： 主要的三个组件的作用： TiDB Server TiDB Server 负责接收 SQL 请求，处理 SQL 相关的逻辑，并通过 PD 找到存储计算所需数据的 TiKV 地址，与 TiKV 交互获取数据，最终返回结果。 TiDB Server 是无状态的，其本身并不存储数据，只负责计算，可以无限水平扩展，可以通过负载均衡组件（如LVS、HAProxy 或 F5）对外提供统一的接入地址。 PD Server Placement Driver (简称 PD) 是整个集群的管理模块，其主要工作有三个： 一是存储集群的元信息（某个 Key 存储在哪个 TiKV 节点）；二是对 TiKV 集群进行调度和负载均衡（如数据的迁移、Raft group leader 的迁移等）；三是分配全局唯一且递增的事务 ID。 PD 是一个集群，需要部署奇数个节点，一般线上推荐至少部署 3 个节点。 TiKV Server TiKV Server 负责存储数据，从外部看 TiKV 是一个分布式的提供事务的 Key-Value 存储引擎。存储数据的基本单位是 Region，每个 Region 负责存储一个 Key Range （从 StartKey 到 EndKey 的左闭右开区间）的数据，每个 TiKV 节点会负责多个 Region 。TiKV 使用 Raft 协议做复制，保持数据的一致性和容灾。副本以 Region 为单位进行管理，不同节点上的多个 Region 构成一个 Raft Group，互为副本。数据在多个 TiKV 之间的负载均衡由 PD 调度，这里也是以 Region 为单位进行调度。 系统结构2 架构上的理解，推荐三篇官方文档： 说存储 说计算 谈调度 分布式机制 存储模型 TiKV 的选择是 Key-Value 模型，并且提供有序遍历方法。简单来讲，可以将 TiKV 看做一个巨大的 Map，其中 Key 和 Value都是原始的 Byte 数组，在这个 Map 中，Key 按照 Byte 数组总的原始二进制比特位比较顺序排列。 Region 将数据划分成 Region 后，我们将会做两件重要的事情： 以 Region 为单位，将数据分散在集群中所有的节点上，并且尽量保证每个节点上服务的 Region 数量差不多 以 Region 为单位做 Raft 的复制和成员管理 总结： 具备MVCC（多版本并发控制 multi-version concurroncy control）功能。 安装与启动暂时未掌握。 参考资料参数调优TiKV 性能参数调优","categories":[],"tags":[]},{"title":"","slug":"大数据开发学习（Durid）","date":"2018-04-09T16:00:00.000Z","updated":"2018-11-28T06:32:44.086Z","comments":true,"path":"2018/04/10/大数据开发学习（Durid）/","link":"","permalink":"http://guzhenping.com/2018/04/10/大数据开发学习（Durid）/","excerpt":"","text":"一 Druid介绍Druid和kylin很像，在工作中先接触了kylin以后，为了调研这个竞品，特地进行学习。 Druid is an open-source data store designed for sub-second queries on real-time and historical data. It is primarily used for business intelligence (OLAP) queries on event data. Druid provides low latency (real-time) data ingestion, flexible data exploration, and fast data aggregation. Existing Druid deployments have scaled to trillions of events and petabytes of data. Druid is most commonly used to power user-facing analytic applications. 看起来Druid Druid 安装与启动暂时未掌握。 度量计算和Kylin一样提供了常用的计算函数，但是不支持精确去重。 kylin目前提供9种计算方法。 sum min max count count_distinct top_n raw extended_column percentile 参考资料 Druid实时大数据分析-快速入门","categories":[],"tags":[]},{"title":"","slug":"大数据开发学习（Spark）","date":"2018-03-22T16:00:00.000Z","updated":"2018-11-28T06:32:44.085Z","comments":true,"path":"2018/03/23/大数据开发学习（Spark）/","link":"","permalink":"http://guzhenping.com/2018/03/23/大数据开发学习（Spark）/","excerpt":"","text":"大数据开发学习（Spark） 一 前言平时接触了一些spark的应用场景，持续保持学习。 二 安装一般都用on yarn的方式 三 实际问题Kylin在采用spark作为计算引擎时，需要进行参数配置。以下答案给了很好的启发性，特地记录： Note that yarn.nodemanager.resource.memory-mb is total memory that a single NodeManager can allocate across all containers on one node.In your case, since yarn.nodemanager.resource.memory-mb = 12G, if you add up the memory allocated to all YARN containers on any single node, it cannot exceed 12G.You have requested 11G (-executor-memory 11G) for each Spark Executor container. Though 11G is less than 12G, this still won’t work. Why ? Because you have to account for spark.yarn.executor.memoryOverhead, which is min(executorMemory * 0.10, 384) (by default, unless you override it).So, following math must hold true:spark.executor.memory + spark.yarn.executor.memoryOverhead &lt;= yarn.nodemanager.resource.memory-mbSee: https://spark.apache.org/docs/latest/running-on-yarn.html for latest documentation on spark.yarn.executor.memoryOverheadMoreover, spark.executor.instances is merely a request. Spark ApplicationMaster for your application will make a request to YARN ResourceManager for number of containers = spark.executor.instances. Request will be granted by ResourceManager on NodeManager node based on: Resource availability on the node. YARN scheduling has its own nuances - this is a good primer on how YARN FairScheduler works. Whether yarn.nodemanager.resource.memory-mb threshold has not been exceeded on the node: (number of spark containers running on the node (spark.executor.memory + spark.yarn.executor.memoryOverhead)) &lt;= yarn.nodemanager.resource.memory-mbIf the request is not granted, request will be queued and granted when above conditions are met from:https://stackoverflow.com/questions/29940711/apache-spark-setting-executor-instances-does-not-change-the-executors 四 参考资料","categories":[],"tags":[]},{"title":"大数据开发学习（Kylin）","slug":"大数据开发学习（Kylin）","date":"2018-01-24T09:04:48.000Z","updated":"2018-11-30T02:55:33.282Z","comments":true,"path":"2018/01/24/大数据开发学习（Kylin）/","link":"","permalink":"http://guzhenping.com/2018/01/24/大数据开发学习（Kylin）/","excerpt":"一 Kylin介绍kylin是一款OLAP工具，背靠Hadoop，HBase，Spark，Kafka等大山，提供神奇体验。","text":"一 Kylin介绍kylin是一款OLAP工具，背靠Hadoop，HBase，Spark，Kafka等大山，提供神奇体验。 Kylin 安装与启动除了常规环境，还需要hadoop的historyjobserver启动。 具体参见官网，启动较为容易。 度量计算目前提供9种计算方法。 sum min max count count_distinct top_n raw extended_column percentile 建模心得在通过星形模型建立事实表+维度表的过程中，操作比较复杂。但是，通过view的方式，就比较简单。 view建模的方式，所有结果都在一张表里，只需要对该表进行维度和度量的划分即可。 建模结果测试结果1： 测试结果2： 实际问题问题1：在同一个project里的Insight，可以看到所用cube的维度，但是不能共用，会报：No model found for rel 问题2：kylin不支持复杂的列，比如map,array类型。问题参见：https://mail-archives.apache.org/mod_mbox/kylin-dev/201512.mbox/%3C565D565C.3060205@jd.com%3E 解决方法：创建该表的视图，剔除复杂列 问题3：kylin建model时，想做增量构建，在指定某列时间作为partition时，该列内容应该满足‘yyyy-MM-dd HH:mm:ss’的样子，不要使用unix_timestamp的类型。否则，构建后cube大小为零。 问题4：kylin建cube所用的字段最好不要采用kylin 关键字，例如:year, month, day, hour等。否则写SQL时，不太友好。例如： 1234567891011121314151617181920想查一段 简单的pv和uv SQL，select platform, year, month, day ,count(*) as pv count(distinct guid) as uv from kylin_tracking_view group by platform, year, month, day 在Kylin的查询界面中，应当这么写:select platform, &quot;YEAR&quot;, &quot;MONTH&quot;, &quot;DAY&quot;, count(*) as pv, count(distinct guid) as uvfrom kylin_tracking_view group by platform, &quot;YEAR&quot;, &quot;MONTH&quot;, &quot;DAY&quot;可以看出，关键词必须全部大写，且被双引号(必须是双引号，单引是自定义常量)包住。建议提前规范好数据源，免得造成巨大的返工。 问题5：build维度过不去具体报错： [BadQueryDetector] service.BadQueryDetector:160 : System free memory less than 100 MB. 0 queries running 暂无解决办法。 这个问题可能是，kylin维度较多，把regionserver搞成僵死，进而导致的。 问题6： kylin 不支持中文列名。kylin在创建中间表时，会使用中文+英文的方式做拼接，这个过程会报错。 问题7：kylin 2.2.0 版本用户相关比较难搞。默认账户 ADMIN有问题，暂无好的解决办法。一种解决方式：升级到 2.3.0版本，修复了230个bug，对用户更加友好。 问题8：kylin 2.3以后采用spark 2.1.2版本，需要进行相关的配置。解决方式：https://kylin.apache.org/docs23/tutorial/cube_spark.html Kylin的一些问题基础关于Kylin的架构和原理，有图可供参考：Kylin 的架构和原理 Kylin比较详细的介绍：Kylin对大数据量的多维分析 关于Kylin Cube构建原理，落地到HBase的过程: Apache Kylin Cube 构建原理 关于Kylin SQL 语法：SQL language 关于Kylin的关键字：关键字源码 关于一次正常查询的运行原理：Kylin进阶之路 Kylin使用calcite做sql解析，可以参考calcite的语法文档：https://calcite.apache.org/ 维度问题关于维度的聚合组中各个含义，请参考https://kylin.apache.org/blog/2016/02/18/new-aggregation-group/ Kylin Mandatory Dimension(必要维度)：【技术帖】Apache Kylin高级设置： 必要维度 （Mandatory Dimension）原理解析 Kylin Hierarchy Dimension(层级维度)：【技术帖】Apache Kylin 高级设置：层级维度（Hierarchy Dimension）原理解析 Kylin Joint Dimension(联合维度)：【技术帖】Apache Kylin 高级设置：联合维度（Joint Dimension）原理解析 Kylin Aggregation Group(聚合组)：【技术帖】Apache Kylin 高级设置：聚合组（Aggregation Group）原理解析 Server端配置负载均衡：【技术贴】如何部署Apache Kylin集群实现负载均衡？ 进阶优化Kylin cube算法：Apache Kylin的快速数据立方体算法——概述 Kylin cube介绍：Kylin使用之创建Cube和高级设置 别人的Cube优化案例：Apache Kylin cube优化指南 优化案例：Apache Kylin 深入Cube和查询优化 提升Cube API开发脚本触发增量更新：Kylin定时增量build Kylin+superset 可视化方案案例：Kylin初体验总结 参考很不错的博文： Apache Kylin 框架介绍 Apache Kylin在美团数十亿数据OLAP场景下的实践","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"大数据开发","slug":"大数据开发","permalink":"http://guzhenping.com/tags/大数据开发/"},{"name":"OLAP","slug":"OLAP","permalink":"http://guzhenping.com/tags/OLAP/"},{"name":"Kylin","slug":"Kylin","permalink":"http://guzhenping.com/tags/Kylin/"}]},{"title":"","slug":"大数据开发学习（HBase）","date":"2018-01-16T16:00:00.000Z","updated":"2018-11-28T06:32:43.893Z","comments":true,"path":"2018/01/17/大数据开发学习（HBase）/","link":"","permalink":"http://guzhenping.com/2018/01/17/大数据开发学习（HBase）/","excerpt":"","text":"Hadoop学习指南（HBase篇） 一 HBase介绍 HMaster：主要负责监控集群、管理RegionServers的负责均衡等，可以用主-备形式部署多个Master。 HRegionServers：负责响应用户的I/O操作请求，客户端对HBase读写数据是与RegionServer交互。 Zookeeper：负责选举Master的主节点；服务注册；保存RegionServers的状态等。可以使用系统内建的zookeeper，也可以使用独立的zookeeper，只需要在配置文件中调整即可。 HDFS：真正的数据持久层，并非必须是HDFS文件系统，但搭配HDFS是最佳选择，也是目前应用最广泛的选择。 二 HBase namespace操作学习HBase常用操作之namespace 参考作者：阿羅链接：http://www.jianshu.com/p/6aeceb5d49cf來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[],"tags":[]},{"title":"","slug":"大数据开发学习（Hive）","date":"2018-01-15T16:00:00.000Z","updated":"2018-11-28T06:32:43.884Z","comments":true,"path":"2018/01/16/大数据开发学习（Hive）/","link":"","permalink":"http://guzhenping.com/2018/01/16/大数据开发学习（Hive）/","excerpt":"","text":"Hadoop学习指南（Hive篇） 一 Hive介绍 CLI(command line interface) JDBC/ODBC Thrift server web gui metastore：存储hive的元数据 driver(complier,optimizer和executor)：将Hive QL语句进行解析、编译优化，生成执行计划，然后调用底层的计算框架（诸如mapreduce,tez等）。 服务器端组件和客户端组件。 二 Hive安装此处演示的hive安装环境为： CentOS 6.5 64位 Hive 2.1.1 Java 1.8 三 关于HiveQLHiveQL虽然基于SQL，但是并不严格遵循SQL-92的标准。HiveQL提供SQL没有的扩展，包括： 四 Hive的使用 创建数据库 12-- 创建hello_world数据库create database hello_world; 查看所有数据库 1show databases; 查看所有表 1show tables; 创建内部表 123456789-- 创建hello_world_innercreate table hello_world_inner( id bigint, account string, name string, age int)row format delimited fields terminated by &apos;\\t&apos;; 创建分区表 1234567create table hello_world_parti( id bigint, name string)partitioned by (dt string, country string); 展示表分区 1show partition hello_world_parti; 创建外部表 12 注外部表和hive内的表区别： 1、在导入数据到外部表，数据并没有移动到自己的数据仓库目录下，也就是说外部表中的数据并不是由它自己来管理的！而表则不一样； 2、在删除表的时候，Hive将会把属于表的元数据和数据全部删掉；而删除外部表的时候，Hive仅仅删除外部表的元数据，数据是不会删除的！ 更改表名称 1alter table hello_world_parti to hello_world2_parti; 修改表结构 12-- 新增列alter table fact_jinrong_submit add columns(amount string, period string); 改变表的location 1alter table tracking2(表名) set location &apos;hdfs://ns2/user/hive/external/log/tracking&apos;（新的位置）; 对表中的某一列进行修改，包括列的名称/列的数据类型/列的位置/列的注释 123456789101112ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type[COMMENT col_comment] [FIRST|AFTER column_name]这个命令可以允许用户修改一个列的名称、数据类型、注释或者位置create table test_col_change (a int,b int, c int);修改列的名称，后面一定要加上数据类型：ALTER TABLE test_col_change CHANGE a a1 INT; 将 a 列的名字改为 a1.ALTER TABLE test_col_change CHANGE a a1 STRING AFTER b; 将 a 列的名字改为 a1，a 列的数据类型改为string，并将它放置在列 b 之后。新的表结构为： b int, a1 string, c int.ALTER TABLE test_col_change CHANGE b b1 INT FIRST; 会将 b 列的名字修改为b1, 并将它放在第一列。新表的结构为： b1 int, a string, c int.注意：对列的改变只会修改Hive 的元数据，而不会改变实际数据。用户应该确定保证元数据定义和实际数据结构的一致性。 创建视图 1create view xxx as select * from table A; 导入数据 1load data local inpath &apos;/home/deploy/user_info.txt&apos; into table user_info; 导出数据 12 四 导入数据的几种方式比如有一张测试表： 12345678910111213create table hello(id int,name string,message string)partitioned by (dt string)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &apos;\\t&apos;STORED AS TEXTFILE; 从本地文件系统中导入数据到hive表 1load data local inpath &apos;data.txt&apos; into table hello; 从HDFS上导入数据到hive表 从别的表中查询出相应的数据并导入到hive表中 创建表时从别的表查到数据并插入的所创建的表中 四 Hive Architecture 引自官网，务必仔细阅读： Figure 1 also shows how a typical query flows through the system. The UI calls the execute interface to the Driver (step 1 in Figure 1). The Driver creates a session handle for the query and sends the query to the compiler to generate an execution plan (step 2). The compiler gets the necessary metadata from the metastore (steps 3 and 4). This metadata is used to typecheck the expressions in the query tree as well as to prune partitions based on query predicates. The plan generated by the compiler (step 5) is a DAG of stages with each stage being either a map/reduce job, a metadata operation or an operation on HDFS. For map/reduce stages, the plan contains map operator trees (operator trees that are executed on the mappers) and a reduce operator tree (for operations that need reducers). The execution engine submits these stages to appropriate components (steps 6, 6.1, 6.2 and 6.3). In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree. Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce). The temporary files are used to provide data to subsequent map/reduce stages of the plan. For DML operations the final temporary file is moved to the table’s location. This scheme is used to ensure that dirty data is not read (file rename being an atomic operation in HDFS). For queries, the contents of the temporary file are read by the execution engine directly from HDFS as part of the fetch call from the Driver (steps 7, 8 and 9). 一个完成的hive查询过程如上所述。上次过程，也展示了hive的主要组件，以及同hadoop的接口。这些重要的组件是： UI：用户向系统提交查询及其他操作。 Driver：接受用户发来的查询等操作。 Compiler：解析查询，包括： Metastore：存储所有的 Execution Engine：执行 五 Hive Data ModelHive的数据被组织成： tables partitions buckets 六 Metastore这是hive 五 常见问题1. 多次初始化可能解法:在终端输入hive命令时，hive的CLI会自动加载一个配置文件： /home/deploy/apache-hive-2.1.1-bin/conf/hive-log4j2.properties 文件内容（可以略过）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements. See the NOTICE file# distributed with this work for additional information# regarding copyright ownership. The ASF licenses this file# to you under the Apache License, Version 2.0 (the# &quot;License&quot;); you may not use this file except in compliance# with the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.status = INFOname = HiveLog4j2packages = org.apache.hadoop.hive.ql.log# list of propertiesproperty.hive.log.level = INFOproperty.hive.root.logger = DRFAproperty.hive.log.dir = $&#123;sys:java.io.tmpdir&#125;/$&#123;sys:user.name&#125;property.hive.log.file = hive.logproperty.hive.perflogger.log.level = INFO# list of all appendersappenders = console, DRFA# console appenderappender.console.type = Consoleappender.console.name = consoleappender.console.target = SYSTEM_ERRappender.console.layout.type = PatternLayoutappender.console.layout.pattern = %d&#123;ISO8601&#125; %5p [%t] %c&#123;2&#125;: %m%n# daily rolling file appenderappender.DRFA.type = RollingRandomAccessFileappender.DRFA.name = DRFAappender.DRFA.fileName = $&#123;sys:hive.log.dir&#125;/$&#123;sys:hive.log.file&#125;# Use %pid in the filePattern to append &lt;process-id&gt;@&lt;host-name&gt; to the filename if you want separate log files for different CLI sessionappender.DRFA.filePattern = $&#123;sys:hive.log.dir&#125;/$&#123;sys:hive.log.file&#125;.%d&#123;yyyy-MM-dd&#125;appender.DRFA.layout.type = PatternLayoutappender.DRFA.layout.pattern = %d&#123;ISO8601&#125; %5p [%t] %c&#123;2&#125;: %m%nappender.DRFA.policies.type = Policiesappender.DRFA.policies.time.type = TimeBasedTriggeringPolicyappender.DRFA.policies.time.interval = 1appender.DRFA.policies.time.modulate = trueappender.DRFA.strategy.type = DefaultRolloverStrategyappender.DRFA.strategy.max = 30# list of all loggersloggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLoggerlogger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxnlogger.NIOServerCnxn.level = WARNlogger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIOlogger.ClientCnxnSocketNIO.level = WARNlogger.DataNucleus.name = DataNucleuslogger.DataNucleus.level = ERROR# Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements. See the NOTICE file# distributed with this work for additional information# regarding copyright ownership. The ASF licenses this file# to you under the Apache License, Version 2.0 (the# &quot;License&quot;); you may not use this file except in compliance# with the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License. 可以删除已存在的该文件，然后再次初始化一次形成新文件。 2. 指定mysql，无法初始化一般是用户权限问题，以及错误地指定了mysql的连接IP和端口。 3. Could not obtain block4. 修改分区字段类型常见的一个场景是Hive里面一个带分区的表，原来是int类型的字段，后来发现数据超过了int的最大值，要改成bigint。或者是bigint要改string或decimal。无论如何，对于带分区的表，要改列类型，有一个坑： 如果使用alter table t change column oldcol newcol bigint，即把int类型的oldcol改为bigint类型的newcol 这个时候，去读数据，应该还是NULL的。 这是因为每个分区Hive还会存一份元数据，于是两种解决方案： alter table t change column oldcol newcol bigint cascade; alter table t change column oldcol newcol bigint, alter table t partition(…) change column oldcol newcol bigint; 同时，在修改表字段类型时，会遇到各种限制： 特别是string转int，hive不让转。如果某个字段一定是int值类型，但是不小心存成了string，这个时候可以先取消hive内置的限制。在hive CLI中执行：set hive.metastore.disallow.incompatible.col.type.changes=false; 执行string to int的sql,例如：将string类型的unix时间戳（createdtime字段）转成int，则alter table fact_jinrong_submit change column createdtime createdtime int cascade; 最后，一定要注意恢复该限制：set hive.metastore.disallow.incompatible.col.type.changes=true; 六 参考资料 大数据时代的技术hive：hive介绍 CentOS6.5安装hive-2.1.0 Hive官方文档翻译——Hive Tutorial（上）（Hive 入门指导） Hive学习系列(一)什么是Hive及Hive的架构 Hive学习系列(二)Hive的查询流程详解 Hive几种数据导入方式 Hive的数据存储模式 Hive数据类型转换 Hive的几种内置服务 Hadoop Hive概念学习系列之hive里的扩展接口（CLI、Beeline、JDBC）","categories":[],"tags":[]},{"title":"","slug":"数据仓库学习（维度建模）","date":"2018-01-10T16:00:00.000Z","updated":"2018-11-28T06:32:43.879Z","comments":true,"path":"2018/01/11/数据仓库学习（维度建模）/","link":"","permalink":"http://guzhenping.com/2018/01/11/数据仓库学习（维度建模）/","excerpt":"","text":"数据仓库学习（维度建模） 一 雪花模型 二 星形模型 三 雪花 vs 星形可参考这篇：星型模型和雪花型模型比较 四 参考资料 漫谈数据仓库之维度建模","categories":[],"tags":[]},{"title":"","slug":"大数据开发学习（Zookeeper）","date":"2018-01-07T16:00:00.000Z","updated":"2018-11-28T06:32:43.878Z","comments":true,"path":"2018/01/08/大数据开发学习（Zookeeper）/","link":"","permalink":"http://guzhenping.com/2018/01/08/大数据开发学习（Zookeeper）/","excerpt":"","text":"大数据开发学习（ZooKeeper） 一 前言ZooKeeper有三种配置方式：单节点、伪集群和集群。本文以集群配置方式为例。 环境介绍： CentOS 6.5 64位 Java 1.8 ZooKeeper 3.4.9 二 安装下载：wget http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz如有版本需求，修改上述的版本号（3.4.9）即可 解压到当前目录：tar -zxvf zookeeper-3.4.9.tar.gz 配置过程，在网上有大量的教程，可以参照《在 CentOS7 上安装 Zookeeper-3.4.9 服务》。 提示，在很多配置过程中，不建议新手使用hostname来配置，尽管你很熟悉hosts文件，也不建议。请使用IP来完成相关的节点配置。 三 使用运行： zkServer.sh start-foreground zkServer.sh start 参数详解zookeeper的配置参数详解（zoo.cfg） 六 参考资料 zookeeper 学习（一） zookeeper工作原理、安装配置、工具命令简介","categories":[],"tags":[]},{"title":"","slug":"大数据开发学习（Fabric）","date":"2018-01-01T16:00:00.000Z","updated":"2018-11-28T06:32:43.797Z","comments":true,"path":"2018/01/02/大数据开发学习（Fabric）/","link":"","permalink":"http://guzhenping.com/2018/01/02/大数据开发学习（Fabric）/","excerpt":"","text":"大数据开发学习（Fabric） 一 前言本篇介绍Fabric的一些常用知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。 二 安装Fabric现在支持Python2和Python3，安装： pip install fabric pip install fabric3 输入fab命令： 三 使用在终端（Terminal）执行fab命令执行配置文件。默认的配置文件为：fabfile.py，也可以使用fab -f xxxx.py来指定配置文件。 除了指定配置文件外，执行需指定具体函数,如：fab hello or fab -f hello.py hello。 举例说明：在当前路径下新建2个py文件：fabfile.py和hello.py，输入相同的内容： 12def hello(): print(&quot;Hello fab!&quot;) 在当前路径下执行：fab hello and fab -f hello.py hello,输出结果： 其他使用，参见《Fabric批量远程执行操作》 六 参考资料 Fabric批量远程执行操作","categories":[],"tags":[]},{"title":"","slug":"大数据开发学习（Sqoop）","date":"2018-01-01T16:00:00.000Z","updated":"2018-11-28T06:32:43.797Z","comments":true,"path":"2018/01/02/大数据开发学习（Sqoop）/","link":"","permalink":"http://guzhenping.com/2018/01/02/大数据开发学习（Sqoop）/","excerpt":"","text":"大数据开发学习（Sqoop） 一 前言二 安装三 使用六 参考资料 Sqoop导入关系数据库到Hive","categories":[],"tags":[]},{"title":"Python学习（迭代器&生成器）","slug":"Python学习（迭代器&生成器）","date":"2017-05-22T09:04:48.000Z","updated":"2018-11-30T02:42:52.947Z","comments":true,"path":"2017/05/22/Python学习（迭代器&生成器）/","link":"","permalink":"http://guzhenping.com/2017/05/22/Python学习（迭代器&生成器）/","excerpt":"前言一直对Python的生成器、yield相关的东西比较晕。也终于到了这一天，算算总账，一口气理解这些东西。 这篇文章的背景是对yield/生成器/协程知识的总结，用于自我提升。请静下心来读，走马观花就不会有自己的思考。 懂分享的人，一定会快乐。笔者的内心期待着大家一起进步。所以，一起来分享自己的理解，留下你的评论吧。","text":"前言一直对Python的生成器、yield相关的东西比较晕。也终于到了这一天，算算总账，一口气理解这些东西。 这篇文章的背景是对yield/生成器/协程知识的总结，用于自我提升。请静下心来读，走马观花就不会有自己的思考。 懂分享的人，一定会快乐。笔者的内心期待着大家一起进步。所以，一起来分享自己的理解，留下你的评论吧。 行文介绍因为这是一整块知识，请不要拆散理解（好处是更易融会贯通）。所以，从头到尾的必备知识： 可迭代对象（iterable） 迭代器（iterator） 迭代（iterator） yield表达式 生成器（generators） 本文会分别介绍上面的知识，最后介绍协程。剧透一下，协程和生成器很相似，所以学习协程，必备生成器相关的知识。 啥是iterable？Python中任意的对象，只要它定义了可以返回一个迭代器的iter方法，或者定义了可以支持下标索引的getitem方法，那么它就是一个可迭代对象。 简单说，可迭代对象就是能提供迭代器的任意对象。 啥是iterator？任意对象，只要定义了__iter__方法和next(Python2) 或者next（Python3）方法，它就是一个迭代器。__iter__()返回迭代器对象本身；next()或者__next__()返回容器的下一个元素，在结尾时引发StopIteration异常退出。 对于可迭代对象，可以使用内建函数iter()来获取它的迭代器对象。 123456789101112131415# python3中&gt;&gt;&gt; test = [1,2,3] &gt;&gt;&gt; item = iter(test) # 获取迭代器对象&gt;&gt;&gt; print(item) # 打印该对象的类型、地址&lt;list_iterator object at 0x10231ab38&gt;&gt;&gt;&gt; item.__next__() # 获取第一个元素1&gt;&gt;&gt; item.__next__() # 获取第二个元素2&gt;&gt;&gt; item.__next__() # 获取第三个元素3&gt;&gt;&gt; item.__next__() # 获取StopIterationTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration 可以对string和dict做上面的事情，看看效果。扩展一点：在for循环中，for语法会自动调用迭代器的next()或next()，并能在遇到StopIteration时正常退出循环。 作为强化，给出一个自定义iterator的实例： 12345678910111213141516171819202122232425262728293031323334353637383940class MyIteratorFab(): &quot;&quot;&quot;Python3实现生成斐波那契数Fibonacci 输出最后一个数据不大于max &quot;&quot;&quot; def __init__(self, max): self.max = max self.a = 0 self.b = 1 def __iter__(self): return self def __next__(self): &quot;&quot;&quot; 返回容器中下一个元素,没有(符合标准的)元素后, 抛出StopIteration python2中请将__next__替换成next &quot;&quot;&quot; if self.b &lt;= self.max: r = self.b # 用于记录倒数第二个b的值,该值才是小于self.max的 self.a, self.b = self.b, self.a + self.b return r # 不可返回self.b, 该值会比self.max大一点点 else: raise StopIteration() # 这条代码很重要,删除这不能正常退出for循环# 测试test = MyIteratorFab(100)for item in test: print(item) ############################################## 附一段生成fibonacci数的函数，同上述自定义迭代器是# 同一功能.#############################################def fib_func2(max): &quot;&quot;&quot;一个普通的的生产fibonacci数的函数 输出最后一个数据不大于max &quot;&quot;&quot; a, b = 0, 1 while b &lt;= max: print(b) a, b = b, a+b 最后，在《Python迭代器和生成器》一文中提到过：对于一个可迭代对象，如果它本身又是一个迭代器对象，那么没办法支持多次迭代。感兴趣可戳过去阅读。 文中给出该问题的解法是，对迭代器对象类再包一个可迭代对象，实现多次迭代。拿上述MyIteratorFab()自定义迭代器举例： 12345678910111213141516171819202122232425# 问题：fab = MyIteratorFab(10)print([item for item in fab]) # output: [1, 1, 2, 3, 5, 8]print([item for item in fab]) # output: []# 解决方法：class BetterFab(): &quot;&quot;&quot;实现生成斐波那契数Fibonacci 输出最后一个数据不大于max 可多次迭代 &quot;&quot;&quot; def __init__(self, max): self.max = max def __iter__(self): return MyIteratorFab(self.max) # 测试fab2 = BetterFab(10)print([item for item in fab2]) # output: [1, 1, 2, 3, 5, 8]print([item for item in fab2]) # output: [1, 1, 2, 3, 5, 8]# 暴力的解法，但不推荐：print([item for item in MyIteratorFab(10)]) # output: [1, 1, 2, 3, 5, 8]print([item for item in MyIteratorFab(10)]) # output: [1, 1, 2, 3, 5, 8] 到此，iterator就算是说完了。这是三个以itera*开头的概念中最核心的一个概念。当然，它也是生成器（generator）的基础。 啥是iteration？提醒一下，这是一个名词。用简单的话讲，它就是从某个地方（比如一个列表）取出一个元素的过程。当我们使用一个循环来遍历某个东西时，这个过程本身就叫迭代。 如何理解yield表达式？下面的内容摘自Python3官方文档翻译： 123&gt; yield_atom ::= &quot;(&quot; yield_expression &quot;)&quot;yield_expression ::= &quot;yield&quot; [expression_list | &quot;from&quot; expression]&gt; yield表达式仅在定义生成器函数时使用，因此只能用在函数定义的主体中。在函数体中使用yield表达式会使该函数成为生成器。 当生成器函数被调用时，它返回一个称为生成器的迭代器。然后，生成器控制生成器函数的执行。当生成器的一个方法被调用时，执行开始。此时，执行进行到第一个yield表达式，在那里它被再次挂起，将expression_list的值返回给生成器的调用者。挂起，我们的意思是保留所有局部状态，包括局部变量的当前绑定，指令指针，内部计算栈和任何异常处理的状态。当通过调用其中一个生成器的方法来恢复执行时，函数可以像yield表达式只是另一个外部调用一样继续进行。恢复后的yield表达式的值取决于恢复执行的方法。如果使用__next__()（通常通过for或next()内置函数），则结果为None。否则，如果使用send()，则结果将是传递到该方法的值。 摘自：http://python.usyiyi.cn/translate/python_352/reference/expressions.html#yieldexpr yield 是一个类似 return 的关键字，只是这个函数返回的是个生成器。 另外，官网也提到yield其实和Coroutine类似： 所有这些使生成器函数与协程非常相似；它们产生多次，它们具有多个入口点并且它们的执行可以被挂起。唯一的区别是生成器函数不能控制在它yield后继续执行的位置；控制总是转移到生成器的调用者。 啥是generators？下面的内容摘自Python3官方文档翻译： 12&gt; generator_expression ::= &quot;(&quot; expression comp_for &quot;)&quot;&gt; 生成器表达式产生一个新的生成器对象。它的语法与推导式的语法相同，除了它被括在括号中而不是括号或花括号中。 当生成器对象调用__next__()方法时，生成器表达式中使用的变量将被懒惰地计算（以与正常生成器相同的方式）。但是，最左边的for子句会立即被求值，所以它产生的错误可以在生成器表达式代码中的任何其它可能的错误之前发现。后续for子句无法立即计算，因为它们可能取决于之前的for循环。例如：(x*y for x in range(10) for y in bar(x))。 摘自：http://python.usyiyi.cn/translate/python_352/reference/expressions.html#yieldexpr 生成器也是一种迭代器，但是你只能对其迭代一次。这是因为它们并没有把所有的值存在内存中，而是在运行时生成值。你通过遍历来使用它们，要么用一个“for”循环，要么将它们传递给任意可以进行迭代的函数和结构。大多数时候生成器是以函数来实现的。然而，它们并不返回一个值，而是yield(暂且译作“生出”)一个值。 生成器最佳应用场景是：你不想同一时间将所有计算出来的大量结果集分配到内存当中，特别是结果集里还包含循环。 协程（Coroutine）常见于合作式多任务，迭代器，无限列表，管道。 协程是基于单个线程的，Python的线程是基于单核的并发实现。 并发对应计算机中充分利用单核（一个CPU）实现（看起来）多个任务同时执行。实现并发编程可以用多进程、多线程、异步、协程。 从上面这段引用可以看出：为啥很多人讨论协程与其他并发编程方式的异同优劣？但是今天我们不聊并发，所以关注该话题的童鞋请自己Google文章。 深挖Coroutine的本质: allowing multiple entry points for suspending and resuming execution at certain locations. 允许多个入口对程序进行挂起、继续执行等操作。 这和生成器很相似，但有区别： 生成器是数据的生产者 协程则是数据的消费者 协程会消费掉发送给它的值。 协程常用的方法：next()send()close() 结语有件事情必须要说明白： 我是宇宙中微不足道的一粒沙。即使作为沙粒，我想浸在湛蓝的海水中，也想沐浴在灿烂的阳光下，更想陪着孩童们构建沙滩城堡。自然清楚自己是一粒沙，但还是想要这样的生活。 所以，我愿意写这些微不足道的东西，来分享自己的价值。尽管，我也微不足道。 参考资料 Python 线程与协程 Python关键字yield的解释 Python yield 使用浅析 Python高级编程之生成器(Generator)与coroutine(一):Generator Python高级编程之生成器(Generator)与coroutine(二):coroutine介绍 Python高级编程之生成器(Generator)与coroutine(三):coroutine与pipeline(管道)和Dataflow(数据流_ Python高级编程之生成器(Generator)与coroutine(四):一个简单的多任务系统 Python迭代器和生成器","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://guzhenping.com/categories/编程语言/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://guzhenping.com/tags/Python/"}]},{"title":"","slug":"数据仓库学习（ETL）","date":"2017-05-10T16:00:00.000Z","updated":"2018-11-28T06:32:43.795Z","comments":true,"path":"2017/05/11/数据仓库学习（ETL）/","link":"","permalink":"http://guzhenping.com/2017/05/11/数据仓库学习（ETL）/","excerpt":"","text":"数据仓库学习（ETL） 一 前言在数据仓库组呆了1年，对ETL做些总结。ETL流在每个公司都多少有差异，一家之言，仅供参考。“不过，懂分享的人一定会快乐”。 比较有特色的是，我们会用tsv这种实体往各个数据库（关系/No sql）中导入。至于为啥这么干，打死你我也不会说。 二 数据源导出tsv大部分公司会考虑直接用sqoop来做，但是我们在用tsv。导出tsv的方式也是挺简单粗暴。一段sql，一个脚本，生成元数据文件的.tsv.bz2格式，rsync把数据送到该去的地方消费掉。 三 OLAPolap和ETL密不可分，推荐一个OLAP的案例：唯品会海量实时OLAP分析技术升级之路 六 参考资料 Sqoop导入关系数据库到Hive","categories":[],"tags":[]},{"title":"HA成功升级的总结","slug":"HA成功升级的总结","date":"2017-02-02T09:04:48.000Z","updated":"2018-11-30T02:38:07.484Z","comments":true,"path":"2017/02/02/HA成功升级的总结/","link":"","permalink":"http://guzhenping.com/2017/02/02/HA成功升级的总结/","excerpt":"前言从升级到现在，一共过了1个半月，到昨天（20170511）总算踏实了。踩的那些坑，真的教会了笔者咋做人。升了，笔者并不后悔。当然脸被打了这么多下，也高兴不起来。 升级过程的所有问题，发生在当事人无法注意到、无法理解的地方。然而，只要时间充足，精力充沛，严谨严格，肯定能克服。 必须申明：本文是基于个体实际经验所写，难免片面，可能不具备参考价值。其次，本文是对一个已经存在数年有大数量的集群所做的一些升级，情况较为特殊。","text":"前言从升级到现在，一共过了1个半月，到昨天（20170511）总算踏实了。踩的那些坑，真的教会了笔者咋做人。升了，笔者并不后悔。当然脸被打了这么多下，也高兴不起来。 升级过程的所有问题，发生在当事人无法注意到、无法理解的地方。然而，只要时间充足，精力充沛，严谨严格，肯定能克服。 必须申明：本文是基于个体实际经验所写，难免片面，可能不具备参考价值。其次，本文是对一个已经存在数年有大数量的集群所做的一些升级，情况较为特殊。 升级准备这里就不多说了，参见《Hadoop Namenode HA升级》。 总结一句，准备和成功概率成正比，有付出，才有收获。 问题1：异构的配置文件对于hadoop集群来说，很多人都觉得hadoop配置文件是一样的。错了，真的错了。hadoop集群支持异构模式。所以，配置可以不同。 在本次升级过程中，配置上，遇到的问题：个别机器的盘符不同于大部分机器，个别机器的datanode数据所在盘没有填写在&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;下。 对于linux盘符不太熟悉的观众请戳：《Linux硬盘盘符分配》 例如，有以下两种（及以上）配置： 1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/deploy/hadoopdata/datanode/,/mnt/sdb/hadoopdata/datanode/&lt;/value&gt;&lt;/property&gt; 1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/deploy/hadoopdata/datanode/,/mnt/sdc/hadoopdata/datanode/,/mnt/sdd/hadoopdata/datanode/&lt;/value&gt;&lt;/property&gt; 这种错误的配置，会导致namenode在启动过程中无法找到block，处于安全模式无法退出。对于集群停机不能超过5小时的公司/团体来说，只能强制立刻安全模式。此时，将会产生坏块。 对于hadoop集群来说，必须清除坏块。换句话说，因为配置的失误，会导致datanode丢数据。这种丢法，和没有挂上的那个/些盘有关，而且一丢就是丢一个盘，后果严重。 当然hadoop的块备份是大于等于2，如果只是一个盘，对于集群来说就相当于没有丢。反之，则是随机丢失块数据。 问题2：Hadoop堆内存先说结论：对于HA来说，两个namenode（active/standby）的堆内存应该要比hadoop集群metadata大至少1倍。这个结论并没有理论根据，但确实实践所得的最可靠数据。假设一个运行5年的集群有20G metadata数据，则namenode(a/s)需要40G以上。 为啥这么说？因为，namenode主备自动切换时(即主namenode异常，备namenode启动)，standby namenode需要将metadata读到进程堆内存中。堆内存不住，NameNode进程会报GC堆已满。GC堆相关问题请戳：《深入理解JVM—JVM内存模型》 理解主备热切，大致有两个要点，本文不多说。只点一下，第一，单节点Namenode启动过程发生了哪些事情；第二，HA集群namenode启动中发生了哪些事情。主要就是围绕FSImage和EditsLog展开。请戳下方链接： 《NameNode启动过程详细剖析 NameNode中几个关键的数据结构 FSImage》 《Hadoop-2.X HA模式下的FSImage和EditsLog合并过程》 对于拥有20G左右metadata的集群，standby需要10-20分钟内就可以启动(堆内存够大)。如果热切时，standby namenode的内存就16G,那么半个小时差不多只能走到25%左右。 堆内存大点很易理解，数据多嘛。而且，网上有很多hadoop在提交作业时报jvm堆内存不足的问题，可以参考。 笔者的小伙伴在解决standby namenode启动过慢这个问题时，通过修改安装目录的../etc/hadoop/hadoop-env.sh文件中的HADOOP_HEAPSIZE这个参数。一开始,该参数同集群metadata数据大小无异，第二次改成约1.5倍，第三次改成约2倍。 HADOOP_HEAPSIZE的值会成为本地（不是每台机器共用的，可以不同）的JVM的堆大小。本地的各个Java守护进程都会共享这个堆，此时进程NameNoe能满足快速启动的条件。虽然会有其他java进程也用，但是HA模式下的namenode没有过多的java进程（其实就是DFSZKFailoverController）。 问题3：NameNode RPC调用方式未升级前，连接hadoop集群通过&lt;name&gt;fs.defaultFS&lt;/name&gt;下的hdfs://IP：PORT来连。升级后，需要采用命名空间的方式： 1234&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mainhadoop&lt;/value&gt;&lt;/property&gt; 即：hdfs://命名空间。连接上命名空间后，zookeeper集群会自动分配程序连接到当前active的namenode。 对此，受影响较大的是一些执行脚本和已存在的hive表。执行脚本一般写死，只能一个一个修改。 对于hive表，需要进入到元数据所在数据库，修改数据Location指向。如果hive是以mysql作元数据存储，则需连上mysql，修改SDS和DBS两张表的数据。将“hdfs://ip:port/XXXXXXXXXX”改成新的hadoop命名空间。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"集群运维","slug":"集群运维","permalink":"http://guzhenping.com/tags/集群运维/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://guzhenping.com/tags/Hadoop/"}]},{"title":"Hadoop 2.7.2 HA升级方案","slug":"HA升级过程","date":"2017-01-02T09:04:48.000Z","updated":"2018-11-30T02:39:17.615Z","comments":true,"path":"2017/01/02/HA升级过程/","link":"","permalink":"http://guzhenping.com/2017/01/02/HA升级过程/","excerpt":"","text":"前言本次升级主要对Hadoop的core-site.xml和hdfs-site.xml文件进行修改，暂时不涉及其他配置。 这次升级过程，大致是三步：备份数据文件，修改配置文件，启动集群。如果升级中发现异常，启动回滚方案。 备份数据备份配置文件需要执行fabric脚本，在每台机器上进行备份。将core-site.xml和hdfs-site.xml分别cp为core-site.xml.ha.back和hdfs-site.xml.ha.back 以下是本次要修改的core-site.xml和hdfs-site.xml配置文件原内容。 core-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://sha2hdpn01:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/deploy/hadoopdata/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.deploy.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.deploy.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;10080&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;topology.script.file.name&lt;/name&gt; &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/topology.sh&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;net.topology.script.file.name&lt;/name&gt; &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/topology.sh&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;sha2hdpn02:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;sha2hdpn01:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/deploy/hadoopdata/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/deploy/hadoopdata/datanode/,/mnt/sdb/hadoopdata/datanode/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; &lt;value&gt;/home/deploy/hadoopdata/checkpoint&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;/name&gt; &lt;value&gt;/home/deploy/hadoopdata/checkpoint&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;600&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt; &lt;value&gt;107374182400&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.fsdataset.volume.choosing.policy&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt; &lt;value&gt;10485760&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/exclude&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt; &lt;value&gt;52428800&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.max.transfer.threads&lt;/name&gt; &lt;value&gt;8192&lt;/value&gt; &lt;description&gt; Specifies the maximum number of threads to use for transferring data in and out of the DN. default is 4096,###Modified### &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.socket.write.timeout&lt;/name&gt; &lt;value&gt;480000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.socket-timeout&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt; &lt;value&gt;8192&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt; &lt;value&gt;80&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 备份namenode数据需要关闭集群后，在sha2hdpn01上备份namenode的元数据,位置：/home/deploy/hadoopdata,大小：25G。 备份secondary namenode的数据需要关闭集群后，在sha2hdpn02上备份snn的元数据，位置：/home/deploy/hadoopdata, 大小：774G。 修改配置文件修改core-stie.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hacluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/deploy/hadoopdata/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;sha2hb01:2181,sha2hb02:2181,sha2hb03:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.deploy.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.deploy.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;10080&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;topology.script.file.name&lt;/name&gt; &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/topology.sh&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;net.topology.script.file.name&lt;/name&gt; &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/topology.sh&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xml先删除secondary namenode配置，在添加和ha配置相关的内容。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/deploy/hadoopdata/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/deploy/hadoopdata/datanode/,/mnt/sdb/hadoopdata/datanode/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;hacluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.hacluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.hacluster.nn1&lt;/name&gt; &lt;value&gt;sha2hdpn01:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.hacluster.nn2&lt;/name&gt; &lt;value&gt;sha2hdpn02:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.hacluster.nn1&lt;/name&gt; &lt;value&gt;sha2hdpn01:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.hacluster.nn2&lt;/name&gt; &lt;value&gt;sha2hdpn02:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://sha2hdpw46:8485; sha2hdpw47:8485;sha2hdpw48:8485/hacluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/deploy/hadoopdata/journaldata&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.hacluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/deploy/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt; &lt;value&gt;107374182400&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.fsdataset.volume.choosing.policy&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt; &lt;value&gt;10485760&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/usr/local/hadoop-default/etc/hadoop/exclude&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt; &lt;value&gt;52428800&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.max.transfer.threads&lt;/name&gt; &lt;value&gt;8192&lt;/value&gt; &lt;description&gt; Specifies the maximum number of threads to use for transferring data in and out of the DN. default is 4096,###Modified### &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.socket.write.timeout&lt;/name&gt; &lt;value&gt;480000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.socket-timeout&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt; &lt;value&gt;8192&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt; &lt;value&gt;80&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动集群先启动zookeeper集群，并确定状态； 再启动journalnode集群，用于nn间数据同步（注意文件夹存储的位置和权限）； 在主节点上启动namenode； 在副节点上，同步namenode的元数据：hdfs namenode -bootstandby（metadata为7.3G, 注意磁盘空间及时长）； 在副节点上启动namenode; 在任意节点上，启动dfs: sh start-dfs.sh; 启动其他的。 回滚方案此处升级，主要是对core-site.xml和hdfs-site.xml文件进行修改，所以回滚方案的主要逻辑就是恢复这两份文件。 利用fabric写脚本，先每台机器上备份（cp命令），如需回滚，先关闭集群，再把该文件替换回来（mv命令）即可。 附脚本rollback_hadoop.py： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from fabric.api import run,sudo,roles,env,cd,executeenv.roledefs = &#123; 'all_node': ['sha2hdpn01','sha2hdpn02','sha2hdpw01','sha2hdpw02','sha2hdpw03','sha2hdpw04','sha2hdpw05','sha2hdpw06','sha2hdpw07','sha2hdpw08','sha2hdpw09','sha2hdpw10','sha2hdpw11','sha2hdpw12','sha2hdpw13','sha2hdpw14','sha2hdpw15','sha2hdpw16','sha2hdpw17','sha2hdpw18','sha2hdpw19','sha2hdpw20','sha2hdpw21','sha2hdpw22','sha2hdpw23','sha2hdpw24','sha2hdpw25','sha2hdpw26','sha2hdpw27','sha2hdpw28','sha2hdpw29','sha2hdpw30','sha2hdpw31','sha2hdpw32','sha2hdpw33','sha2hdpw34','sha2hdpw35','sha2hdpw36','sha2hdpw37','sha2hdpw38','sha2hdpw39','sha2hdpw40','sha2hdpw41','sha2hdpw42','sha2hdpw43','sha2hdpw44','sha2hdpw45','sha2hdpw46','sha2hdpw47','sha2hdpw48'], 'namenode': ['sha2hdpn01'], 'test_node': ['sha2hdpw48']&#125;env.user = 'deploy'env.password = 'XXXXXX' # yourself# env.shell = '/bin/sh -c'@roles('all_node')def showfile(): # run('ll /usr/local/hadoop-default/etc/hadoop')i with cd('/usr/local/hadoop-default/etc/hadoop'): run(\"ls core-site.xml hdfs-site.xml\")@roles('namenode')def start_hadoop(): print('start hadoop cluster...') # run('sh /usr/local/hadoop-default/sbin/start-all.sh') print('done...')@roles('all_node')#@roles('test_node')def backup(): print('start backup...') with cd('/usr/local/hadoop-default/etc/hadoop'): run('cp core-site.xml core-site.xml.ha.back') run('cp hdfs-site.xml hdfs-site.xml.ha.back') print('done...')@roles('all_node')#@roles('test_node')def rollback(): print('start roolback...') with cd('/usr/local/hadoop-default/etc/hadoop'): run('mv core-site.xml.ha.back core-site.xml') run('mv hdfs-site.xml.ha.back hdfs-site.xml') print('done...')def deploy(): execute(showfile)def run_backup(): execute(backup)def run_rollback(): execute(rollback) execute(start_hadoop) 测试方案重跑airflow中金融/流量的报表任务，用于提交job，查看是否能够完整跑完。这些任务中有使用了hdfs及yarn的操作，如果成功，说明hadoop ha集群可以用。 结语烧香拜佛，希望成功！","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"集群运维","slug":"集群运维","permalink":"http://guzhenping.com/tags/集群运维/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://guzhenping.com/tags/Hadoop/"}]},{"title":"Hadoop学习指南(HA配置)","slug":"Hadoop学习指南（HA配置）","date":"2017-01-02T09:04:48.000Z","updated":"2018-11-30T02:50:33.953Z","comments":true,"path":"2017/01/02/Hadoop学习指南（HA配置）/","link":"","permalink":"http://guzhenping.com/2017/01/02/Hadoop学习指南（HA配置）/","excerpt":"前言本篇介绍Hadoop的一些常用知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。 HA模式，主要是将namenode及resourcemanager都变成主备两个。这里先不讨论resourcemanager，主要针对namenode。 将namenode变成可主备自动切换的，主要是通过zookeeper集群对namenode的健康状态进行监控，然后选举一个健康的namenode做active(主)的，另一个成为standby(备)。因此，保证zookeeper集群的配置是正确且不易挂掉，是HA的基石。同时，注意HA升级过程中的相关进程的启动步骤即可完成。","text":"前言本篇介绍Hadoop的一些常用知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。 HA模式，主要是将namenode及resourcemanager都变成主备两个。这里先不讨论resourcemanager，主要针对namenode。 将namenode变成可主备自动切换的，主要是通过zookeeper集群对namenode的健康状态进行监控，然后选举一个健康的namenode做active(主)的，另一个成为standby(备)。因此，保证zookeeper集群的配置是正确且不易挂掉，是HA的基石。同时，注意HA升级过程中的相关进程的启动步骤即可完成。 官网有两种配置：NFS和QJM，两者区别参见:《HDFS v2 HA方案对比》 本文以NFS为例进行讨论。 首先声明环境： CentOS 6.5 64位 Hadoop 2.7.1 Java 1.8 所有操作均基于拥有第一代hadoop集群的环境之上。同时满足Centos 6.5系统内网络互通、机器免密码登陆、防火墙关闭。重要的是/etc/hosts文件如下： 12345192.168.20.2 hadoop1192.168.20.3 hadoop2192.168.20.4 hadoop3192.168.20.5 hadoop4192.168.20.6 hadoop5 另外，温馨提示：在hadoop的各种文件配置中，最好不要出现空格。例如： 1234&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop2:2181, hadoop3:2181, hadoop4:2181&lt;/value&gt;&lt;/property&gt; hadoop将无法找到hadoop3和hadoop4这种IP。如果手贱多敲，那么将会浪费很多时间。 进程功能介绍zk:维护共享锁保证只有一个active的namenode journalnode：在两个nn间同步元数据 配置zookeeper集群该集群最少需要3台机器（用于选举）。下面详述配置过程。 假设1： 3台机器是这样： IP 标识 192.168.20.3 hadoop2 192.168.20.4 hadoop3 192.168.20.5 hadoop4 假设2： 下载完的zookeeper源码包位于：/home/deploy/zookeeper-3.4.9。 假设3：java环境为1.8版本，位于：/home/deploy/jdk1.8.0_111。 第一步，配置zoo.cfg:vi /home/deploy/zookeeper-3.4.9/conf/zoo.cfg 修改配置: 12345678910111213141516171819202122232425# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/home/deploy/zookeeper-3.4.9/data# logs dirdataLogDir=/home/deploy/zookeeper-3.4.9/logs# the port at which the clients will connectclientPort=2181server.1=hadoop3:2888:3888server.2=hadoop4:2888:3888server.3=hadoop2:2888:3888 第二步，配置myid文件 在zoo.cfg中dataDir路径：dataDir=/home/deploy/zookeeper-3.4.9/data下，新建myid文件。vi myid在hadoop3的机器上，该文件内容为1，hadoop4机器上，该文件内容为2，hadoop5机器上，该文件内容为3。内容应保持同zoo.cfg中的server.x的x值相同 第三步，开/关zookeeper集群： 开：sh /home/deploy/zookeeper-3.4.9/bin/zkServer.sh start 关：sh /home/deploy/zookeeper-3.4.9/bin/zkServer.sh stop 请在将三台机器全部开启后，查看状态：zkServer.sh status。 配置core-site.xml在原文件上进行添加: 123456789101112&lt;!-- 指定hdfs的nameservice为h01，需与dfs.nameservices一致 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop2:2181,hadoop3:2181,hadoop4:2181&lt;/value&gt;&lt;/property&gt; 配置hdfs-site.xml这里有两步，第一步是删除关于secondary namenode的配置，第二步是添加HA的配置。 删： 12345678910111213141516&lt;!-- 以下3个 property 的配置，是非HA模式下的，即一个集群只有一个namenode，在这里不可使用 --&gt; &lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;h01.vm.com:50070&lt;/value&gt; &lt;description&gt;Secondary get fsimage and edits via dfs.http.address&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.secondary.http.address&lt;/name&gt; &lt;value&gt;h02.vm.com:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; &lt;value&gt;/home/vagrant/VMBigData/hadoop/data/namesecondary&lt;/value&gt;&lt;/property&gt; 添： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;!-- 命名空间的逻辑名称 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt;&lt;/property&gt;&lt;!-- 命名空间中所有NameNode的唯一标示。该标识指示DataNode集群中有哪些NameNode --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop1:9091&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop2:9091&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop1:9092&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop2:9092&lt;/value&gt;&lt;/property&gt;&lt;!-- JournalNode URLs，ActiveNameNode 会将 Edit Log 写入这些 JournalNode 所配置的本地目录即 dfs.journalnode.edits.dir --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop2:8485;hadoop3:8485;hadoop4:8485/mycluster&lt;/value&gt;&lt;/property&gt;&lt;!-- JournalNode 用于存放 editlog 和其他状态信息的目录 --&gt;&lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/deploy/hadoop-2.7.1/journaldata&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;!-- 一种关于 NameNode 的隔离机制(fencing) --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/deploy/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt;&lt;/property&gt; 修改slaves文件主要是把作为namenode节点的ip从该文件中删除，视各自机器环境而定。 启动HA集群启动顺序，非常讲究。。。 重要的一点：不容许使用 hdfs namenode -format的命令，此命令会删除原集群的所有数据。 第一步，先关所有的集群进程。 第二步，打开zookeeper集群：sh /home/deploy/zookeeper-3.4.9/bin/zkServer.sh start 第三步，打开journalnode进程，该进程在两个nn间同步元数据。在hadoop2\\3\\4上都执行：sh hadoop-daemon.sh start journalnode 第四步，在原namenode节点上执行：sh hadoop-daemon.sh start namenode。此操作会将namenode状态变成active。 第五步，在备（standby）节点执行同步namenode数据的命令：hdfs namenode -bootstrapStandby。切记不要使用scp的方式同步元数据，会导致文件权限问题。 第六步，启动备namendoe：sh hadoop-daemon.sh start namenode 第七步，初始化zkfc。在主备两台上任意一台执行：hdfs zkfc -formatZK 第七步，启动zk(DFSZKFailoverController)，该进程维护共享锁保证只有一个active的namenode。分别在两台作为NN的节点上执行：sh hadoop-daemon.sh start zkfc 第八步，启动hdfs集群（就是打开所有的datanode进程）：sh start-dfs.sh 第九步，打开没有任何变化的yarn：sh start-yarn.sh。上面说了暂时不讨论resourcemanger的升级。 至此，hadoop集群双NN的升级就完成了。 效果展示第一步， 打开两个NN的监控页： 第二步：kill hadoop1的namenode进程，查看hadoop2中namenode的状态由standby变为active：重复几次，主备仍能自切。 参考资料 官方文档: HDFS High Availability 官方文档: ResourceManager High Availability Hadoop NameNode 高可用 (High Availability) 实现解析 Hadoop的HA机制(Zookeeper集群+Hadoop集群)配置记录 hadoop HA高可用集群模式搭建指南 Journal Storage Directory not formatted","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"集群运维","slug":"集群运维","permalink":"http://guzhenping.com/tags/集群运维/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://guzhenping.com/tags/Hadoop/"}]},{"title":"Hadoop学习指南（集群运维篇）","slug":"Hadoop学习指南（集群运维篇）","date":"2016-12-09T09:04:48.000Z","updated":"2018-11-30T02:34:31.633Z","comments":true,"path":"2016/12/09/Hadoop学习指南（集群运维篇）/","link":"","permalink":"http://guzhenping.com/2016/12/09/Hadoop学习指南（集群运维篇）/","excerpt":"前言本篇介绍Hadoop的一些常用知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。","text":"前言本篇介绍Hadoop的一些常用知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。 常用HDFS命令 hadoop fs -ls URI hadoop fs -du -h URI hadoop fs -cat URI [文件较大，hadoop fs -cat xxxx | head] hadoop fs -put URI hadoop fs -get URI hadoop fs -rmr URI hadoop fs -stat %b,%o,%n,%r,%y URI (%b：文件大小, %o：Block 大小, %n：文件名, %r：副本个数, %y 或%Y：最后一次修改日期和时间) hadoop fs -tail [-f] URI hdfs dfsadmin -report hadoop fs -appendToFile URI1[,URI2,…] URI（hadoop fs -appendToFile helloCopy1.txt helloCopy2.txt /user/tmp/hello.txt） hadoop fsck / -files -blocks 关键的clusterID这是hadoop集群的id号，只有拥有相同id的各个节点才能加入的这个集群中来。 大致是在：hdfs namenode -format命令之后生成这个id。 很多情况下，你的集群如果可能拥有不同的id号哦。就比如： 启动datanode是遇到： “DataNode: Initialization failed for Block pool” 此时应当查看：cat /home/deploy/hadoop-2.7.1/hdfs/name/current/VERSION cat /home/deploy/hadoop-2.7.1/hdfs/data/current/VERSION 一个是datanode数据的文件夹，一个是namenode数据的文件夹。 1234567#Fri Feb 17 15:48:44 CST 2017namespaceID=74707331clusterID=CID-e3e7c80e-3099-461d-9fa9-404f4910def5cTime=0storageType=NAME_NODEblockpoolID=BP-1729560578-192.168.20.2-1487317724421layoutVersion=-63 1234567#Thu Dec 22 09:57:13 CST 2016storageID=DS-63997596-6d60-46e8-a08c-94426208f9d9clusterID=CID-076b4e8a-9ed9-47e9-b6e0-4c16440c33e8cTime=0datanodeUuid=3ae0642e-2d72-4b45-ba63-de5e6ca1c7c7storageType=DATA_NODElayoutVersion=-56 添加删除节点Hadoop添加删除节点 重启丢失节点子节点DataNode丢失sbin/hadoop-daemon.sh start datanode 子节点NodeManager丢失sbin/yarn-daemon.sh start nodemanager 主节点丢失sbin/start-all.sh or sbin/hadoop-daemon.sh start namenode sbin/hadoop-daemon.sh start secondarynamenode sbin/yarn-daemon.sh start resourcemanager 配置文件出错管理hadoop集群，会经常遇到配置文件的相关问题。这里举一个例子，比如yarn的nodemanager起不来的问题。 yarn的相关配置文件有两个：yarn-site.xml和yarn-env.sh 在yarn-site.xml文件： 1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt;&lt;/property&gt; 在yarn-env.sh文件： 1JAVA_HEAP_MAX=-Xmx1024m 应该保证yarn-site.xml中的memory-mb数值比yarn-env.sh中JAVA_HEAP_MAX的数值小。yarn-site.xml的配置是要求nodemanager启动的最少内存，低于该值无法启动。实际启动时，使用yarn-env.sh中的配置。修改比如：JAVA_HEAP_MAX=-Xmx2048m no xxx to stophadoop会经常有这个问题，大概就是没有找到该进程的PID文件，所以报错。 具体参见连接：解决关闭Hadoop时no namenode to stop异常 每次启动hadoop（./start-all.sh）时，PID文件被生成，存储进程号。关闭时，PID文件被删除。 在hadoop2.7.1版本中，关于HADOOP_PID_DIR(文件路径：../etc/hadoop/hadoop-env.sh)的描述是这样的： 123456# The directory where pid files are stored. /tmp by default.# NOTE: this should be set to a directory that can only be written to by# the user that will run the hadoop daemons. Otherwise there is the# potential for a symlink attack.export HADOOP_PID_DIR=$&#123;HADOOP_PID_DIR&#125;export HADOOP_SECURE_DN_PID_DIR=$&#123;HADOOP_PID_DIR&#125; 最好将PID文件放在只写目录中。 关于mapred-site.xml配置参见blog:《如何给运行在Yarn的MapReduce作业配置内存》 参考资料-如何给运行在Yarn的MapReduce作业配置内存 hadoop HDFS常用文件操作命令 HDFS 文件操作命令","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"集群运维","slug":"集群运维","permalink":"http://guzhenping.com/tags/集群运维/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://guzhenping.com/tags/Hadoop/"}]},{"title":"集群搭建指南（上卷）","slug":"集群搭建指南（上卷）","date":"2016-11-16T09:04:48.000Z","updated":"2018-11-30T02:36:17.890Z","comments":true,"path":"2016/11/16/集群搭建指南（上卷）/","link":"","permalink":"http://guzhenping.com/2016/11/16/集群搭建指南（上卷）/","excerpt":"前言阅读本文，需要具备Linux、计算机网络的基础知识。所以在文中出现的相关基础知识，均以链接的形式给出，务必理解该链接的内容后，继续阅读本指南。 集群搭建的环境多种多样，本文采用VitualBox安装5台虚拟机构建集群。具体环境： CentOS 6.5 64位 VirtualBox 5.1.10 Mac版 本机macOS Sierra 10.12.1 i7 8G内存","text":"前言阅读本文，需要具备Linux、计算机网络的基础知识。所以在文中出现的相关基础知识，均以链接的形式给出，务必理解该链接的内容后，继续阅读本指南。 集群搭建的环境多种多样，本文采用VitualBox安装5台虚拟机构建集群。具体环境： CentOS 6.5 64位 VirtualBox 5.1.10 Mac版 本机macOS Sierra 10.12.1 i7 8G内存 单台装机先下载CentOS的iso格式纯净镜像，可以下载LiveDVD和minimal两个版本。本文采用LiveDVD版做演示。 打开virtualbox： 点击”新建”按钮： 选择版本，并命名： 点击”继续”按钮,改内存大小： 点击”继续”按钮,改虚拟硬盘： 点击“创建”按钮，改硬盘类型：关于磁盘类型，参考：https://zhidao.baidu.com/question/1302436594642278379.html 点击”继续”按钮，改存储方式： 继续，改文件位置和大小： 点击”创建”,主界面显示该机器： 选中该机器，点击主界面”设置”按钮： 点击”系统”，修改”启动顺序”： 点击”网络”,修改网卡1： 为修改网卡2，先关闭该对话框，打开vitualbox管理界面： 点击偏好设置： 点击”网络”： 选择”仅主机（Host-Only）网络”： 点击添加按钮（右边绿色按钮），新增vboxnet1： 点击OK保存，再次选中该虚机（hadoop6）,打开主界面的”设置”按钮，点击”网络”，点击”网卡2”，勾选”启用网络连接”，选择连接方式： 选vboxnet1,则其余所有节点均需保持一致。点击OK保存。此时点击主界面的”启动”按钮： 选择Centos的iso文件所在位置，点击”启动”。在弹出黑色界面时（有9秒时间），按一次任意的某个键，进入Boot界面。选择第一种安装方式：verify and boot： 按下回车键，进入centos系统： 双击系统桌面的”install to hard drive”: 进入安装界面： 一路点击next按钮,点击yes即可。时区选亚洲上海,root账户的密码要牢记。然后，进入等待界面： 点击”close”,关闭安装界面： 关闭该虚拟机，调整系统启动顺序。选中该虚机，点击”设置”，点击”系统”,调整”启动顺序”，将”硬盘”放在第一位，将”光驱”放在最后一位： 点击ok保存，再次启动该虚机，进入设置页面： 一路forward即可，创建用户名和密码自己定义： 为集群选择同一时间，需要勾选synchronize date and time over the network.防止某个节点挂了以后和其他节点的系统时间不一致： 一路forward，点击finish,进入登陆界面： 注：如果采用minimal安装方法，装机方式仅有两处与上述不同： 选第一种安装方式 选skip，跳过检测 其余过程，同上述LiveDVD版安装过程。 集群搭建重复上述步骤，安装5台虚拟机。此时需要配置集群的网络：配置网卡，修改hostname,添加host解析，添加普通用户。 配置网卡通过上述步骤的装机方法，每台机器中都将有两个网卡eth0和eth1。eth0作为虚机网络的公网网口，eth1作为虚拟机网络的内网网口。 配置网卡，需要在目录：/etc/sysconfig/network-scripts下新建以ifcfg-ethX（X是一个数字，从0开始，一般到3结束。）文件。所以，关于eth0的配置在：/etc/sysconfig/network-scripts/ifcfg-eth0; 关于eth1的配置在：/etc/sysconfig/network-scripts/ifcfg-eth1。 如果有不明白的地方，可以参考下文—-CentOS网络配置详解。 配置eth0在System-&gt;Preferences-&gt;Network Connections，进行配置。 先编辑Auto eth0: 连接名修改为eth0，勾选所有用户可用，设置IPv4,选择DHCP即可： 点击Apply，输入root账户验证即可。 如果采用minimal版本的CentOS安装，文件配置方法，需要在/etc/sysconfig/network-scripts/ifcfg-eth0中修改： ONBOOT=yes BOOTPROTO=dhcp 其他参数均由系统自动生成： 其余几台节点的eth0和上述相同。这样所有节点的公网IP是相同的。只要宿主机可以上网， 那么所有节点均可上网。 配置eth1eth1采用host-only模式，在Manual方法下填写ip。网段确定在192.168.xx.xx中。对于5个节点的集群，主机名为hadoop1-hadoop5，则ip分别是： 192.168.20.2 hadoop1 192.168.20.3 hadoop2 192.168.20.4 hadoop3 192.168.20.5 hadoop4 192.168.20.6 hadoop5 先编辑Auto eth1, 设置IPv4，选择Manual,添加ip,子网掩码，网关: 点击Apply,验证root账户，即可成功。这样192.168.20.2就分配给这台机器了（下面说把这台机器变成hadoop2）。 如果采用minimal版本的CentOS安装，文件配置方法，需要在/etc/sysconfig/network-scripts/ifcfg-eth1中修改： ONBOOT=yes BOOTPROTO=none IPADDR=192.168.20.2 NETMASK=255.255.255.0 其他参数(注意不要配置gateway)均由系统自动生成： 其余几台机器同上述配置过程，只需要更改IP(IPADDR参数)即可，子网掩码和网关（minimal不用配）均相同。 修改hostname一般是localhost开头，但是不容易标识机器。改成可标识的。有5台机器，那么这5台机器可以对应hadoop1-5。 修改文件的目录在: /etc/sysconfig/network 将HOSTNAME这个参数改掉即可。对于5个节点，分别在每台机器上修改为hadoop1-5。 添加host解析每台虚机都有ip，但是ip难记，加个host解析，方便使用。 配置文件在:/etc/hosts 先把127.0.0.1改成你已经修改的hostname的值，比如在hadoop1(192.168.20.2)这台机器上，需要改成： 127.0.0.1 hadoop1 ::1 hadoop1 这里，每个节点是不同的。 所有的节点均需添加内容： 192.168.20.2 hadoop1 192.168.20.3 hadoop2 192.168.20.4 hadoop3 192.168.20.5 hadoop4 192.168.20.6 hadoop5 保存即可。 添加普通用户添加普通账号，比如：deploy,分配给使用者。配置/etc/sudoers文件，使得该账户也可以进行sudo操作。 此处较为简单，参考下文—-linux的账户。 装机的思考为什么需要两个网卡？如果只使用默认的 NAT，会发现一旦宿主机断开公网，自己的几台虚机之间也会无法通。最简单的解决方案是双网卡，如下图所示： 为什么修改两次系统启动顺序？第一次装机是系统是来自iso文件的（光驱启动）。每次都从该文件启动，则无法对系统进行定制修改。将其装在硬盘上（自己的虚拟硬盘），每次从硬盘启动即可对系统进行定制修改。 所以，需要改变启动顺序。装到硬盘后，从硬盘启动。 linux的账户root账户权限太大，必须给开发者一个使用账户。 相关链接：http://linuxme.blog.51cto.com/1850814/347086/ CentOS网络配置详解包含网络配置的很多东西：http://blog.chinaunix.net/uid-26495963-id-3230810.html","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"集群运维","slug":"集群运维","permalink":"http://guzhenping.com/tags/集群运维/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://guzhenping.com/tags/Hadoop/"}]},{"title":"集群搭建指南（下卷）","slug":"集群搭建指南（中卷）","date":"2016-11-09T09:04:48.000Z","updated":"2018-11-30T02:35:30.864Z","comments":true,"path":"2016/11/09/集群搭建指南（中卷）/","link":"","permalink":"http://guzhenping.com/2016/11/09/集群搭建指南（中卷）/","excerpt":"前言本文的搭建基于上卷的配置，环境不再一一赘述。网络配置好的5台节点均可相互ping通，对于节点hadoop1(192.168.20.2)可以ping同其余四台hadoop2(192.168.20.3)、hadoop3(192.168.20.4)、hadoop4(192.168.20.5)、hadoop5(192.168.20.6)。其余节点同理。 本文继续进行集群的搭建—-SSH配置。禁止root账户进行ssh登陆，只允许指定用户进行ssh登陆。","text":"前言本文的搭建基于上卷的配置，环境不再一一赘述。网络配置好的5台节点均可相互ping通，对于节点hadoop1(192.168.20.2)可以ping同其余四台hadoop2(192.168.20.3)、hadoop3(192.168.20.4)、hadoop4(192.168.20.5)、hadoop5(192.168.20.6)。其余节点同理。 本文继续进行集群的搭建—-SSH配置。禁止root账户进行ssh登陆，只允许指定用户进行ssh登陆。 定义用户添加用户已经在上卷中提及,仅供参考。这里假设所有节点均有一个账户:deploy，密码:hadoop。 同时，已在/etc/sudoers文件提高了deploy的权限。 SSH配置修改ssh配置文件配置文件是：/etc/ssh/sshd_config。找到相关变量，修改为： PermitRootLogin no RSAAuthentication yes PubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys PasswordAuthentication no 以上配置作用分别是：禁止root用户远程ssh，开启RSA验证，开启公钥验证，设置验证密钥文件在~/.ssh/authorized_keys中，禁止密码登录。 保存，重启ssh服务生效。重启命令：restart```1234567891011#### 生成公钥私钥以deploy登录，进入~目录，输入```ssh-keygen -t rsa```命令，一路回车生成id_rsa（私钥）和id_rsa.pub(公钥)。生成的文件在目录：~/.ssh/下。在该目录下新建authorized_keys文件，将id_rsa.pub的内容拷贝的该文件中，后续有ssh请求均会来此文件验证内容。同时，给相关文件赋予权限：```chmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys 此时，即可利用私钥id_rsa用deploy账户登陆该机器。 拷贝各节点的公钥在各个节点重复上述过程，修改ssh的配置文件，在deploy账户下生成密钥，赋予文件权限。 为了让集群的节点能相互ssh登陆，需要讲个节点的公钥全部放置到deploy账户下的~/.ssh/authorized_keys（/home/deploy/.ssh/authorized_keys）文件中。每个节点的authorized_keys应该是相同的。 配置总结SSH是会验证不同用户账户下的~/.ssh/authorized_keys的公钥。在Linux系统中，~是指当前账户所在的目录。因为Linux支持多账户，所以，用不同的账户登录，~就代表不同路径。以deploy账户登录，则~代表：/home/deploy/；以deploy2账户登录，~就代表：/home/deploy2/。 如果指定某账户能ssh登录，则必须在登录该账户的情况下，在~目录下生成公钥和私钥。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"集群运维","slug":"集群运维","permalink":"http://guzhenping.com/tags/集群运维/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://guzhenping.com/tags/Hadoop/"}]},{"title":"集群搭建指南（下卷）","slug":"集群搭建指南（下卷）","date":"2016-11-02T09:04:48.000Z","updated":"2018-11-30T02:33:15.385Z","comments":true,"path":"2016/11/02/集群搭建指南（下卷）/","link":"","permalink":"http://guzhenping.com/2016/11/02/集群搭建指南（下卷）/","excerpt":"前言什么是Hadoop? Apache Hadoop is a framework for running applications on large cluster built of commodity hardware. The Hadoop framework transparently provides applications both reliability and data motion. Hadoop implements a computational paradigm named Map/Reduce, where the application is divided into many small fragments of work, each of which may be executed or re-executed on any node in the cluster. In addition, it provides a distributed file system (HDFS) that stores data on the compute nodes, providing very high aggregate bandwidth across the cluster. Both MapReduce and the Hadoop Distributed File System are designed so that node failures are automatically handled by the framework. 本文是用于搭建hadoop集群的学习性文章，主要对hadoop的一些基础知识进行解释，中间穿插着hadoop的安装配置过程。 环境问题： 系统：Centos 6.5 64位 minamal版本 hadoop版本：2.7.1 java版本：1.8 可能不具备通用性，仅供参考。","text":"前言什么是Hadoop? Apache Hadoop is a framework for running applications on large cluster built of commodity hardware. The Hadoop framework transparently provides applications both reliability and data motion. Hadoop implements a computational paradigm named Map/Reduce, where the application is divided into many small fragments of work, each of which may be executed or re-executed on any node in the cluster. In addition, it provides a distributed file system (HDFS) that stores data on the compute nodes, providing very high aggregate bandwidth across the cluster. Both MapReduce and the Hadoop Distributed File System are designed so that node failures are automatically handled by the framework. 本文是用于搭建hadoop集群的学习性文章，主要对hadoop的一些基础知识进行解释，中间穿插着hadoop的安装配置过程。 环境问题： 系统：Centos 6.5 64位 minamal版本 hadoop版本：2.7.1 java版本：1.8 可能不具备通用性，仅供参考。 安装概览安装完系统，调通集群内的网络（ssh）,配置好java环境，再进行hadoop的安装。 安装hadoop时，集中精力配置namenode节点这一台机器，修改相关的配置文件，然后使用同步文件（scp）到其他机器上去，hadoop集群搭建即可完成。 值得一提，集群间必须网络通畅，否则无法完成分布式部署。有关问题参见—-集群搭建指南（中卷）。 另外，在生成环境中，hadoop的安装均在普通账户下（比如：deploy）完成。所以，新手请注意将java及hadoop安装在开发账户下（是deploy账户的话，位置一般是：/home/deploy/jdk1.8.0_111, /home/deploy/hadoop-2.7.1）。再通过修改账户目录下的 ~/.bash_profile文件修改环境变量。 安装步骤对于5个节点的集群（hadoop1-hadoop5），配置节点hadoop1即可。hadoop集群有很多特性，这里仅设置低配版的hadoop,不添加太多配置属性值，以减轻学习压力。 This document does not cover advanced topics such as Security or High Availability. 尽管只添加部分属性值，也需要配7个文件： core-site.xml hdfs-site.xml yarn-site.xml mapred-site.xml hadoop-env.sh yarn-env.sh slaves 7个文件的解释，参见:hadoop基础知识-&gt;1. 配置文件说明。 配置前，假设是: 在deploy账户下进行配置 hadoop文件在：/home/deploy/hadoop-2.7.1/ java文件在：/home/deploy/jdk1.8.0_111 在/home/deploy/hadoop-2.7.1/目录下，新建文件夹：tmp, hdfs, hdfs/data, hdfs/name 完成后，文件结构是这样： 接下来准备配置，如果在配置过程中发现没有该文件，可以自己新建。如果文件已有内容，可以全部删除/注释，然后添加下述配置。另外，本节给出的多有示例配置的端口都可自己定义，不必追求一致。 core-site.xml命令：vi /home/deploy/hadoop-2.7.1/etc/hadoop/core-site.xml 123456789101112&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9091&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/deploy/hadoop-2.7.1/tmp&lt;/value&gt; &lt;description&gt;Abasefor oteh temporary directories.&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml命令：vi /home/deploy/hadoop-2.7.1/etc/hadoop/hdfs-site.xml 123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;!--配置NameNode--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/deploy/hadoop-2.7.1/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;!--配置DataNode--&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/deploy/hadoop-2.7.1/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;!--配置Secondary Namenode --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop1:9095&lt;/value&gt; &lt;/property&gt; &lt;!--修改默认的HDFS配置--&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml命令：vi /home/deploy/hadoop-2.7.1/etc/hadoop/yarn-site.xml 12345678910111213141516171819202122232425&lt;configuration&gt; &lt;!--下述配置NodeManger--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!--下述配置ResourceManager --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;hadoop1:9099&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml命令：vi /home/deploy/hadoop-2.7.1/etc/hadoop/mapred-site.xml 1234567&lt;configuration&gt; &lt;!--配置MaoReduce应用--&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hadoop-env.sh命令：vi /home/deploy/hadoop-2.7.1/etc/hadoop/hadoop-env.sh 找到export JAVA_HOME的地方，将原来的内容注释，换成：export JAVA_HOME=/home/deploy/jdk1.8.0_111 yarn-env.sh命令：vi /home/deploy/hadoop-2.7.1/etc/hadoop/yarn-env.sh 找到export JAVA_HOME的地方，将原来的内容注释，换成：export JAVA_HOME=/home/deploy/jdk1.8.0_111 slaves命令：vi /home/deploy/hadoop-2.7.1/etc/hadoop/slaves 添加你所有datanode节点的hostname,假设有5个节点，名称是hadoop1-5(hadoop1作为namenode),则添加内容： 1234hadoop2hadoop3hadoop4hadoop5 启动hadoop集群配置完所有的文件，在该机器（假设是hadoop1）上使用scp命令到其他4台机器上。命令： 1234scp -r /home/deploy/hadoop-2.7.1/* deploy@hadoop2:/home/deploy/hadoop-2.7.1/scp -r /home/deploy/hadoop-2.7.1/* deploy@hadoop3:/home/deploy/hadoop-2.7.1/scp -r /home/deploy/hadoop-2.7.1/* deploy@hadoop4:/home/deploy/hadoop-2.7.1/scp -r /home/deploy/hadoop-2.7.1/* deploy@hadoop5:/home/deploy/hadoop-2.7.1/ 这样集群的配置文件就都一样了。然后,在hadoop1上执行命令，将集群格式化成一个分布式文件系统，执行：/home/deploy/hadoop-2.7.1/bin/hdfs namenode -format （注：该命令只能执行一次，多次执行的话，需清空前一次执行产生的hdfs/data/、hdfs/name/和tmp/文件夹下生成的文件。但是这样，老集群的数据就会丢失。相关问题已网上的解答为准。） 接下来启动集群：bash /home/deploy/hadoop-2.7.1/sbin/start-all.sh 查看相关的UI Web界面： 按这里的配置，Namenode程序的可视化界面在hadoop1:50070/(使用的默认端口）,如图： Secondery Namenode程序的可视化界面在hadoop1:9095/（使用的自定义端口）, 如图： ResourceManager程序的可视化界面在hadoop1:9099/（使用的自定义端口），如图： 最后，关闭集群：bash /home/deploy/hadoop-2.7.1/sbin/stop-all.sh hadoop基础知识1. 配置文件说明简单的配置无安全性的hadoop集群，需要修改： core-site.xml hdfs-site.xml yarn-site.xml mapred-site.xml hadoop-env.sh yarn-env.sh slaves 这7个文件均在同一目录：hadoop安装包名/etc/hadoop/。 To configure the Hadoop cluster you will need to configure the environment in which the Hadoop daemons execute as well as the configuration parameters for the Hadoop daemons. Hadoop由HDFS、Yarn、MapReduce等组成，hadoop的后台程序，其实就是他们的后台程序。 HDFS后台程序有NameNode,SecondaryNode和DataNode。YARN后台程序有ResourceManager,NodeManager和WebAppProxy。如果MapReduce框架被启用了，MapReduce Job History Server将会运行。（安装完hadoop后，使用jps命令可以看到这些后台程序的名称。） 1.1 配置hadoop后台程序的环境管理员可以使用hadoop-env.sh，也可选择性使用mapred-env.sh和yarn-env.sh脚本定制haddoop后台程序的环境。 至少，必须指定JAVA_HOME，以确保正确定义每一个远程节点。 可以配置的选择，在下图： 1.2 配置hadoop后台程序这部分处理在给定的配置文件中需要被指定的重要参数。 etc/hadoop/core-site.xml etc/hadoop/hdfs-site.xml 配置NameNode: 配置DataNode: etc/hadoop/yarn-site.xml 配置ResourceManager和NodeManager： 配置ResourceManager: 配置NodeManager: 配置History Server: 配置NodeManagers的健康监控： etc/hadoop/mapred-site.xml 配置MapReduce应用： 配置MapReduce JobHistory Server: slaves List all slave hostnames or IP addresses in your etc/hadoop/slaves file, one per line. Helper scripts (described below) will use the etc/hadoop/slaves file to run commands on many hosts at once. It is not used for any of the Java-based Hadoop configuration. In order to use this functionality, ssh trusts (via either passphraseless ssh or some other means, such as Kerberos) must be established for the accounts used to run Hadoop. 2. Hadoop Map/Reduce框架2.1 Programming model and execution frameworkMap/Reduce是一种编程范式，用于将大型分布式计算表达为对键/值对数据集的分布式操作序列。Hadoop Map/Reduce框架利用一组机器，并在集群中的所有节点上执行用户定义的Map/Reduce作业。一个Map/Reduce计算包含两个阶段：map阶段和reduce阶段。 在map阶段，框架将输入数据集拆分为大量片段，并将每个片段分配给map任务。框架还在其操作的节点集群上分布许多map任务。每个map任务从其分配的片段消耗键/值对，并产生一组中间键/值对。 对于每个输入的键/值对（K,V），map任务将调用户自定义的map函数。该函数将输入变换为不同的键/值对(K’,V’)。 map阶段过后，Map/Reduce框架通过键对中间数据集进行排序，并产一组（K‘,V’*）元组，使得与特定键相关联的所有值一起出现。它还将元组的集合（这组（K’,V’）元组）分割成等于reduce任务的数目的多个片段。 在reduce阶段，每个reduce任务消耗分配给他的（K’,V’*）元组的片段。对于每个这样的元组，它调用用户自定义的reduce函数，其将元组变换为输出值键/值对(K,V)。框架又一次在节点集群上分配许多reduce任务，并处理将合适的中间数据片段运送到每个reduce任务的事情。 关于hadoop的任务（tasks），有三个特点: 每个阶段的任务以容错方式（a fault-tolerant manner）执行，如果节点（多个节点）在计算中间失败，则分配给它（它们）的任务在其余节点之间重新分布 任务较多（有许多map和reduce）时，能够实现良好的负载平衡 允许失败的任务以最小的运行时间开销重新运行 2.2 ArchitectureHadoop Map/Reduce框架是主(master)/从(slave)架构。它有一个主服务器或jobtracker和几个从服务器或tasktrackers。jobtracker是用户和框架之间的交互点。用户向jobtracker提交map/reduce作业，将作业（job/jobs）放在待处理作业的队列中，并按先到先服务（first-come/first-served）的方式执行。jobtracker管理map和reduce任务到tasktracker的分配。tasktracker根据jobtracker的指令执行任务，并处理map和reduce阶段之间的数据移动。 2.3 Map/Reduce框架小结网上有很多以上原理的过程图，在hadoop官网上没有找到，但言语上描述的相当清楚。如有需求，自行网上找图。 3. Hadoop DFS3.1 介绍Hadoop的分布式文件系统旨在：能够在跨大型集群中的机器上可靠地存储非常大的文件。它的\b灵感来自Google文件系统。Hadoop DFS将每个文件存储为一个块序列，文件中除最后一个块之外的所有块都具有相同的大小。属于文件的块将被复制以实现容错。块大小（block size）和复制因子(replication factor)可以按文件配置。HDFS中的文件是“一次写入”，并且在任何时间都有一个写入程序。 3.2 Architecture像Hadoop Map/Reduce一样，HDFS遵循主/从架构。HDFS的安装包含一个Namenode,即一个主服务器，用于管理文件系统命名空间并控制客户端对文件的访问。此外，还有一些Datanodes,管理存储附加到他们运行的节点。Namenode进行文件系统命名空间操作，例如：通过RPC接口打开、关闭、重命名等文件和目录。它还确定块（block）到Datanodes的映射。Datanodes负责从文件系统客户端提供读取和写入请求，它们还根据Namenode的指令执行块创建、删除和复制。 参考资料 Hadoop参数汇总 Hadoop官网文档–cluster setup instructions in latest 2.x stable release docs hadoop中NameNode、DataNode、Secondary、NameNode、JobTra","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"集群运维","slug":"集群运维","permalink":"http://guzhenping.com/tags/集群运维/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://guzhenping.com/tags/Hadoop/"}]},{"title":"Hadoop学习指南(HDFS篇)","slug":"Hadoop学习指南（HDFS篇）","date":"2016-11-02T09:04:48.000Z","updated":"2018-11-30T02:41:15.542Z","comments":true,"path":"2016/11/02/Hadoop学习指南（HDFS篇）/","link":"","permalink":"http://guzhenping.com/2016/11/02/Hadoop学习指南（HDFS篇）/","excerpt":"","text":"# 前言本篇介绍Hadoop的一些常用知识。要说和网上其他manual的区别，那就是这是笔者写的一套成体系的文档，不是随心所欲而作。 Hadoop DFS介绍 Hadoop的分布式文件系统旨在：能够在跨大型集群中的机器上可靠地存储非常大的文件。它的\b灵感来自Google文件系统。Hadoop DFS将每个文件存储为一个块序列，文件中除最后一个块之外的所有块都具有相同的大小。属于文件的块将被复制以实现容错。块大小（block size）和复制因子(replication factor)可以按文件配置。HDFS中的文件是“一次写入”，并且在任何时间都有一个写入程序。 Architecture 像Hadoop Map/Reduce一样，HDFS遵循主/从架构。HDFS的安装包含一个Namenode,即一个主服务器，用于管理文件系统命名空间并控制客户端对文件的访问。此外，还有一些Datanodes,管理存储附加到他们运行的节点。Namenode进行文件系统命名空间操作，例如：通过RPC接口打开、关闭、重命名等文件和目录。它还确定块（block）到Datanodes的映射。Datanodes负责从文件系统客户端提供读取和写入请求，它们还根据Namenode的指令执行块创建、删除和复制。 翻译自hadoop官网wiki Hadoop常用命令 hadoop fs -ls URI hadoop fs -du -h URI hadoop fs -cat URI [文件较大，hadoop fs -cat xxxx | head] hadoop fs -put URI hadoop fs -get URI hadoop fs -rmr URI hadoop fs -stat %b,%o,%n,%r,%y URI (%b：文件大小, %o：Block 大小, %n：文件名, %r：副本个数, %y 或%Y：最后一次修改日期和时间) hadoop fs -tail [-f] URI hdfs dfsadmin -report hadoop fs -appendToFile URI1[,URI2,…] URI（hadoop fs -appendToFile helloCopy1.txt helloCopy2.txt /user/tmp/hello.txt） hadoop fsck / -files -blocks 列出文件系统中各个文件由哪些块构成。 请将URI替换成自己想要查看的文件路径或文件夹路径。 参考资料 hadoop HDFS常用文件操作命令 HDFS 文件操作命令","categories":[{"name":"大数据","slug":"大数据","permalink":"http://guzhenping.com/categories/大数据/"}],"tags":[{"name":"集群运维","slug":"集群运维","permalink":"http://guzhenping.com/tags/集群运维/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://guzhenping.com/tags/Hadoop/"}]}]}